\documentclass[12pt, english]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage{pdfpages}
\usepackage[toc,page]{appendix}
\usepackage{hyperref}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{float} % Allows putting an [H] in \begin{figure} to specify the exact location of the figure
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{afterpage}
\usepackage{cancel}
\usepackage[square,sort,comma,numbers]{natbib}
\setlength{\parindent}{0pt} % set indent of the first line of the paragraph
\setlength{\parskip}{0.1cm} % set space between paragraphs
\usepackage[left=1.1in,right=1.1in,top=1.5in,bottom=1.5in]{geometry}

\begin{document}

\tableofcontents

\newpage

\section{[TODO] Math fundamentals]}

\section{[TODO] Supervised learning fundamentals]}

\section{[TODO] Neural networks}

\subsection{[TODO] LSTM}

Link: https://arxiv.org/pdf/1503.04069.pdf, also http://colah.github.io/posts/2015-08-Understanding-LSTMs/

LSTM \cite{hochreiter_long_1997} \cite{greff_lstm_2017} ...

\section{[TODO] Hyper-parameter tuning}

Some approaches:

"Classic"

\begin{itemize}
  \item Grid search, consisting in an exhaustive search of the hyper-parameter space.
  \item Random search, consisting in a randomly-guided search over the hyper-parameter space, where samples are commonly chosen to be bounded over a hyper-sphere over the current maximum \cite{bergstra_random_2012}.
\end{itemize}

\begin{itemize}
  \item SMAC \cite{hutter_sequential_2011}.
  \item Tree-structured Parzen Estimators (TPE) \cite{bergstra_algorithms_2011}.
  \item Covariance Matrix Adaptation Evolution Strategy (CMA-ES) \cite{loshchilov_cma-es_2016}
\end{itemize}

Bayesian Optimization \cite{snoek_practical_2012} \cite{shahriari_taking_2016}

Recent work:

\begin{itemize}
  \item http://papers.nips.cc/paper/7472-neural-architecture-search-with-bayesian-optimisation-and-optimal-transport.pdf
  \item http://metalearning.ml/2019/papers/metalearn2019-white.pdf
  \item https://arxiv.org/pdf/1903.09434.pdf
  \item http://proceedings.mlr.press/v97/nayebi19a/nayebi19a.pdf
\end{itemize}


Neural architecture search


\section{Reinforcement learning}

RL in 3 Equations (from Nando de Freitas talk at Khipu 2019)

Most RL algorithms can be divided into three families: policy gradient algorithms (Eqn. \ref{eqn_policy_gradient_trajectory}), $Q$-value policy gradient algorithms (Eqn. \ref{eqn_policy_gradient_value}) and $Q$-value algorithms (Eqn. \ref{eqn_action_value_function}).

\begin{eqnarray}
  \nabla_\theta J(\pi_\theta) &=& \mathbb{E}_\tau [(\sum_{t=0}^T R_t \nabla_\theta \log \pi_\theta (a_t \mid s_t))] \label{eqn_policy_gradient_trajectory} \\
  \nabla_\theta J(\pi_\theta) &=& \mathbb{E}_{d_{\pi(s)} \pi(a \mid s)} [Q_\pi(s,a) \nabla \log \pi_\theta (a \mid s)] \label{eqn_policy_gradient_value}\\
  Q^*(s,a) &=& \mathbb{E}_{s'} [r(s,a,s') + \gamma max_{a'} Q^*(s',a') \mid s,a] \label{eqn_action_value_function}
\end{eqnarray}
%
where $J(\pi_\theta)$ is the expected return of a policy $\pi$ parametrized with $\theta$, $\tau$ is a trajectory with length $T$, and $d_{\pi(s)}$ is the probability distribution of the states.

\subsection{Key concepts}

\subsubsection{Policy gradient}

Given $\pi_\theta$, a stochastic policy parametrized with $\theta$. Our objective is to maximize the expected return
\begin{equation}
  \label{eqn_expected_return}
  J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]
\end{equation}
%
where $R(\tau)$ is the sum of discounted rewards over the sampled trajectory $\tau = (s_0, a_0, \dots, s_{t+1})$.

The aim is to optimize the policy by gradient ascent, such that

\begin{equation}
  \theta_{t+1} = \theta_t + \alpha J(\pi_\theta)\Bigr|_{\theta_t}
\end{equation}

How to find a numberically computable expression? The probability of a trajectory $\tau$ whose policy is based on $\pi$ with parameters $\theta$, is given by

\begin{equation}
  \label{eqn_trajectory_probability}
  P(\tau \mid \theta) = \rho_0 (s_0) \prod_{t=0}^T P(s_{t+1} \mid s_t, a_t) \pi_\theta (a_t \mid s_t)
\end{equation}
%
where $\rho_0 (s_0)$ is the probability distribution of the initial state.

Considering that $\nabla \log (x) = \frac{1}{x}$, therefore

\begin{eqnarray*}
  \nabla_\theta \log (P(\tau \mid \theta)) &=& \frac{1}{P(\tau \mid \theta)} \nabla_\theta P(\tau \mid \theta) \\ \nonumber
  P(\tau \mid \theta) \nabla_\theta \log (P(\tau \mid \theta)) &=& \nabla_\theta P(\tau \mid \theta)
\end{eqnarray*}

On the other hand, log of a trajectory is given by

\begin{eqnarray}
  \log P(\tau \mid \theta) &=& \log \rho_0 (s_0) + \sum_{t=1}^T [\log P(s_{t+1} \mid s_t, a_t) + \log \pi_\theta (a_t \mid s_t)] \\
  \text{applying gradient} \nonumber \\
  \nabla_\theta \log P(\tau \mid \theta) &=& \cancel{\nabla_\theta \log \rho_0 (s_0)} + \sum_{t=1}^T[ \cancel{\nabla_\theta \log P(s_{t+1} \mid s_t, a_t)} + \nabla_\theta \log \pi_\theta (a_t \mid s_t)] \nonumber \\
  \nabla_\theta \log P(\tau \mid \theta) &=& \sum_{t=1}^T[\nabla_\theta \log \pi_\theta (a_t \mid s_t)] \label{eqn_grad_log_trajectory}
\end{eqnarray}

Considering Eqn. \ref{eqn_expected_return} and Eqn. \ref{eqn_grad_log_trajectory} and that $\nabla_\theta R(\tau)$ is also $0$ as the return does not depend on $\theta$, expanding the expectation it is derived that

\begin{equation}
  \label{eqn_policy_gradient}
  \nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [\sum_{t=1}^T[\nabla_\theta \log \pi_\theta (a_t \mid s_t)] R(\tau)]
\end{equation}
%
This is called the \textbf{policy gradient theorem}. As it is an expectation, it can be estimated by a sample mean. Therefore, if we have a set of trajectories $D = {\tau_1, \dots, \tau_N}$, a gradient ascent step can be done estimating the policy gradient with

\begin{equation}
  \label{eqn_policy_gradient_estimated}
  \hat{g} = \frac{1}{|D|} \sum_{\tau \in D} \sum_{t=0}^T[\nabla_\theta \log \pi_\theta (a_t \mid s_t)] R(\tau)
\end{equation}

A common form of Equation \ref{eqn_policy_gradient} and Equation \ref{eqn_policy_gradient_estimated} involves the estimated advantage $\hat{A}_t$ instead of the return $R(\tau)$.

Vanilla policy gradient is the simplest algorithm that makes use of this theorem. It collects trajectories $D = {\tau_1, \dots, \tau_N}$ by running policy $\pi_i = \pi(\theta_i)$. Then, it uses the parameters $\phi$ of the value function $V_\phi$ to compute the advantage function $\hat{A}_t$ to estimate \ref{eqn_policy_gradient_estimated}, and then performs two updates:

\begin{enumerate}
  \item Update of the policy:
  $$\theta_{k+1} = \theta_k + \alpha_k \hat{g}_k$$
  \item Update of the value function, by regression on the mean squared error:
  $$\phi_{k+1} = \arg\min_\phi \frac{1}{|D_k|T} \sum_{\tau \in D_k} \sum_{t=0}^T (V_\phi(s_t) - \hat{R_t})^2$$
  where $\hat{R_t}$ are the discounted rewards.
\end{enumerate}

Section based on \url{https://spinningup.openai.com/en/latest/user/introduction.html}

\subsubsection{Importance sampling}

Prediction problem: exploration policy $b$ (considered as fixed/given, e.g. $\epsilon$-greedy policy), target policy $\pi$ (e.g. optimal policy, considered as fixed/given, a common case is the deterministic greedy policy). We have episodes of experience obtained by following policy $b$. How to estimate $v_\pi$ or $q_\pi$ having episodes that followed exploration policy $b$.

Assumption of coverage: $\pi(a \mid s) > 0$ implies $b(a \mid s) > 0$. Implication: $b$ must be stochastic in state where it is not identical to $\pi$.

Importance sampling (IS) is a technique used for estimating expected values under one distribution given samples from another. In this context, IS is applied to off-policy learning by weighting returns according to the relative probability of their trajectories. Therefore, IS ratio over a trajectory is given by

\begin{equation}
  IS = \prod_{i=1}^t \frac{\pi(a_i \mid s_i)}{b(a_i \mid s_i)}
\end{equation}

Now, $V_b (s) = \mathbb{E} [R_t \mid s_t = s]$, where $R_t$ is the sum of discounted rewards. Therefore, using the IS ratio,

$V_\pi (s) = \mathbb{E} [\prod_{i=1}^t \frac{\pi(a_i \mid s_i)}{b(a_i \mid s_i)} R_t \mid s_t = s]$

\subsubsection{Reparametrization trick}

\url{https://arxiv.org/pdf/1312.6114.pdf}

Let $Z \sim q_\phi(Z \mid X)$ be a conditional distribution. The reparametrization trick consists in expressing the random varible $Z$ as a deterministic variable $Z = g_\phi(\varepsilon, X)$, being $\varepsilon$ an auxiliary variable with an independent marginal $P(\varepsilon)$.


\subsection{"Main" deep RL algorithms}

\subsubsection{DQN}
\label{DQN}

Family: Q-value

\underline{TL;DR}


\underline{Problem addressed}
\underline{Proposed approach}



\subsubsection{TRPO}
\label{TRPO}

Family: policy gradient

Link: \url{https://arxiv.org/abs/1502.05477}

\underline{TL;DR}

TRPO \cite{schulman_trust_2015-1} is an on-policy algorithm that updates policies $\pi_\theta$ by gathering pairs of $(s,a)$ through experience, and updating iteratively the policy by finding the $\theta$ that solves an approximation of an optimization problem given by $\pi_{i+1} = \arg\max_\pi L_{\theta_i} = \mathbb{E} \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_old (a_t \mid s_t)}} A_t$ and constrained to $D_{KL}^{\max}(\theta_i, \theta) \leq \delta$. Such constraint is called the \textit{trust region} constraint, because it allows the policy to change with a safe learning step that iteratively adapts over the course of the learning.

\underline{Problem addressed}

\begin{enumerate}
  \item Given the advantage $\eta(\tilde{\pi})$ of a policy $\tilde{\pi}$ over a policy $\pi$.
  \item It was shown in Kakade and Langford (2002) that if $\pi_\theta(a \mid s)$ is a differentiable function of the parameter vector $\theta$, then an approximation of the advantage $L_\theta(\tilde{\theta})$ (which used visitation frequency $\rho_\theta$ instead of $\rho_{\tilde{\theta}}$) will match $\eta$ to first order, with a sufficiently small step $\alpha$.
\end{enumerate}

Problem: how to determine the sufficiently small step that improves the approximation $L_{\theta_{old}(\theta)}$, is also applicable to stochastic policies?


\underline{Proposed approach}

\begin{enumerate}
  \item Replace step-size $\alpha$ with a distance measure between $\pi$ and $\tilde{\pi}$.
  \item An algorithm similar to policy improvement is used to iteratively 1) calculate the advantages of the policy $\theta_i$ for every $s,a$ and 2) solves the constrained optimization problem $\pi_{i+1} = \arg\max_\pi [L_{\theta_i} - C D_{KL}^{\max}(\theta_i, \theta)]$.
  \item Using the penalty coefficient $C$ will involve very small step sizes.
  \item Therefore, an additional \textit{trust region constraint} is added to relax the optimization problem, that now seeks to maximize $L_{\theta}$, where the $KL$ divergence is subject to be lesser than a certain $\delta$.
  \item Now, calculating the $KL$ divergence as it is, is infeasible due that it is bounded by every point in the state space.
  Instead, it is calculated the average $KL$ divergence using importance sampling, replacing the expectations for sample averages, $Q$-values for empirical estimate, and approximating through Monte Carlo.
  \item Two schemes proposed for this estimation:

  \begin{enumerate}
    \item Single path estimation, where states $s$ are sampled from $\rho$ (the visitation frequency) and then a policy $\pi_{\theta_{old}}$ is simulated for a number of timesteps to generate a trajectory, making $b(a \mid s) = \pi_{\theta_{old}}$.
    Also, $Q_{\theta_{old}}$ is computed at each state-action pair taking the discounted sum of rewards along the trajectory.

    \item Vine, where states $s$ are sampled from $\rho$ (the visitation frequency) and then a policy $\pi_{\theta_{old}}$ is simulated for a number of timesteps to generate a trajectory, just like in single path estimation.
    Then, a subset of $N$ states of the trajectories are chosen (rollout set), and each of them are used to sample $K$ actions according to $b(a_{k \in K} \mid s_{n \in N})$. According to the authors, $b(s \mid a) = \pi_{\theta_i (s \mid a)}$ performs well in continuous problems, while the uniform distribution performs well on discrete tasks such as Atari.

    $\hat{Q}_{\theta_i}(s_n, a_{n,k})$ is estimated by performing a rollout (i.e. a short trajectory) starting from state $s_n$ and action $a_{n,k}$.

    Upsides against single path: better advantage estimation, less variance of the $Q$ value.
    Downsides: need more simulations.
  \end{enumerate}

\end{enumerate}

Summarized algorithm:

\begin{enumerate}
  \item Use single path or vine to gather $(s,a)$ pairs, estimating its $Q$-values with Monte Carlo and using the importance sampling ratio.
  \item Averaging over samples, construct the estimated objective function and constraint.
  \item Solve this optimization problem with constraint to update the policy vector $\theta$. In the paper, authors use conjugate gradient algorithm followed by a line search.
\end{enumerate}


\subsubsection{[TODO] GAE}
\label{GAE}

Family: Policy gradient

Link: https://arxiv.org/pdf/1506.02438.pdf

\underline{TL;DR}


\underline{Proposed approach}

Generalized Advantage Estimation (GAE) proposes TRPO for the value function, which .

\begin{itemize}
  \item Using value functions to .
\end{itemize}


\subsubsection{DDPG}
\label{DDPG}

Family: Q policy gradient

Link: \url{https://arxiv.org/pdf/1509.02971.pdf}

\underline{TL;DR}
DDPG \cite{lillicrap_continuous_2015} is an off-policy actor critic algorithm that trains simultaneously a deterministic actor function $a$ and a critic function $Q$ with an experience memory. As in continuous tasks is expensive to find $\max_a Q(s,a)$, the maximum of $Q$ is instead approximated by $Q(s, a(s \mid \theta_a))$.

\underline{Problem addressed}

How to use $Q$-learning algorithms when the action space is continuous?

\underline{Proposed approach}

DQN modified for actor critic. It learns a deterministic actor function $a(s \mid \theta_a)$ and a critic function $Q(s,a \mid \theta_c)$. As finding $\max_a Q(s,a)$ in continuous action spaces is computationally expensive, the max is instead approximated by $\max_a Q(s,a \mid \theta_c) \approx Q(s, a(s \mid \theta_a))$.

Critic is updated with a mean squared error loss as in DQN, and actor policy is updated using the sampled policy gradient and applying the chain rule to find the $\theta_a$ that maximizes

\begin{equation}
  \nabla_{\theta_a} J (\pi_{\theta_a}) \approx \frac{1}{|N|} \sum_{s \in N} \nabla_{\theta_a} Q(s,a(s \mid \theta_a) \mid \theta_c) \nabla_{\theta_a} a(s \mid \theta_a)
\end{equation}
%
where $N$ is the batch $\{(s_i, a_i, r_i, s_{i+1})\}^{|N|}_{i=1}$ and $|N|$ its size.

As it is off-policy, DDPG uses a replay memory as in DQN to iteratively update its actor and critic networks by

\begin{eqnarray}
  \theta'_a &=& \rho \theta'_a + (1-\rho) \nabla J (\pi_{\theta_a}) \\
  \theta'_c &=& \rho \theta'_c + (1-\rho) \theta_c
\end{eqnarray}
%
where $\rho << 1$.
On the other hand, actions are selected by $Q(s, a(s \mid \theta_a)) + \varepsilon$, where $\varepsilon$ is a random exploration noise.


\subsubsection{A3C}
\label{A3C}

Family: policy gradient, distributed

Link: \url{https://arxiv.org/pdf/1602.01783.pdf}

\underline{TL;DR}

Asynchronous Advantage Actor Critic (A3C) \cite{mnih_asynchronous_2016} is an asyncronous policy-based model free algorithm that consists on multiple agents running on environment in parallel, accumulating gradients and updating it after several timesteps.

\underline{Problem addressed}

Provide a different paradigm for deep RL algorithms that does not rely on experience replay for stabilizing learning and reducing training time and memory requirements.

\underline{Proposed approach}

\begin{itemize}
  \item Instead of using experience replay, multiple agents are run in parallel on multiple instances of the environment. Each agent runs in its own thread and interacts with its own copy of the environment, and they apply the gradients after accumulating it over multiple timesteps, instead of using minibatches. In addition, agents may use different exploration policies in each thread so as to improve robustness.
  \item As an actor critic architecture, it concurrently maintains a policy $\pi(a \mid s, \theta)$ and an estimation of the value function $V(s \mid \phi)$. Both are updated after a certain amount of timesteps $t$, and it is given by the estimate $\nabla_{\theta^-} \log \pi (a \mid s, \theta) A(s,a \mid \theta, \theta_v)$, being $A$ an estimate of the advantage function. A target network $\theta^-$ is shared among all agents.
  \item Their synchronous, determinisic variant is called A2C.
\end{itemize}


\subsubsection{[TODO] GAIL}
\label{GAIL}

Link: \url{https://arxiv.org/pdf/1606.03476.pdf}

\underline{TL;DR}

GAIL \cite{ho_generative_2016} uses expert trajectories to learn a cost function over them and then learn an action policy...



\underline{Problem addressed}

Learning a policy from example expert behavior in an efficient way. Other approaches include

\begin{itemize}
  \item Behavioral cloning, which learns a policy as a supervised learning problem, over state-action pairs over expert trajectores. Downside: compounding error caused by covariance shift.
  \item Inverse Reinforcement Learning (IRL), which consists in learning a cost function over expert trajectories in which the expert is uniquely optimal. Downside: expensive to run.
\end{itemize}

\underline{Proposed approach}

Generative Adversarial Imitation Learning (GAIL) ...

\textbf{TODO}


\subsubsection{[TODO] ACER}
\label{ACER}

Link: \url{https://arxiv.org/abs/1611.01224}


\subsubsection{PPO}
\label{PPO}

Link: \url{https://arxiv.org/abs/1707.06347}

\underline{TL;DR}

PPO \cite{schulman_proximal_2017} is an on-policy algorithm that updates its policy $\pi_{\theta_{k+1}}$ by taking gradient ascent over an objetive function that limits (by clipping or applying a penalization based on the KL divergence) how far the new policy can go.

\underline{Problem addressed}

How to take the biggest possible improvement step on a policy update without it being too much that the learning may collapse the performance?

TRPO implements a penalty that is necessary, but as it is, optimizing its objective may involve excessively large update steps on the policy, prone to unstabilities.

\underline{Proposed approach}

Being $\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)}$ the probability ratio between the old policy and new policy, being $\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)} = 1$ when it is the same policy.

The approach of Proximal Policy Optimization (PPO) is to penalize changes of the policy that moves the coefficient $\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)}$ away from $1$. For that, the objective function proposed is given by

\begin{equation}
  \label{eqn_clipped_objective}
  L_{\text{CLIP}} (\theta) = \hat{\mathbb{E}} [\min (\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)} \hat{A}_t(s,a), \text{clip} (\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)},1-\epsilon, 1+\epsilon) \hat{A}_t(s,a))]
\end{equation}

Put in other terms, if $\hat{A}_t(s,a) > 0$, this means that action $a$ is improving the policy. Therfore, the objective will be given by

\begin{equation*}
  L_{\text{CLIP}} (\theta) = \min (\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)}, 1+\epsilon) \hat{A}_t(s,a)
\end{equation*}

Otherwise, if $\hat{A}_t(s,a) < 0$, action $a$ is detrimental for the policy, and the objective will be given by

\begin{equation*}
  L_{\text{CLIP}} (\theta) = \max (\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)}, 1-\epsilon) \hat{A}_t(s,a)
\end{equation*}

In Equation \ref{eqn_clipped_objective}, the aim is 1) to prevent the surrogate (approximated) objective funtion to fall in ranges of $r_t(\theta)$ outside the interval $[1-\epsilon, 1+\epsilon]$ so as to limit the change in the policy, and 2) to take the minimum of the clipped and unclipped objective so the final objective is a lower (pessimistic) bound.

As an alternative, a penalty that uses $KL$ divergence such as in TRPO and adapts its cost parameter $\beta$ according to the magnitude of the divergence may be used.

\subsubsection{HER}

Link: \url{https://arxiv.org/pdf/1707.01495.pdf}

\underline{TL;DR}

Family: Q-value, Q Policy-gradient

Hindsight Experience Replay (HER) is an algorithm that stores and later replay transitions of experience in memory with the original goal used in the current episode, and a subset of other goals. HER also works applied to only one goal, and it can be used with any off-policy algorithm.

\subsubsection{[TODO] ACKTR}

Link: \url{https://arxiv.org/abs/1708.05144}

\subsubsection{TD3}
\label{TD3}

Family: Q Policy-gradient

Link: \url{https://arxiv.org/pdf/1802.09477.pdf}

\underline{TL;DR}

Twin Delayed DDPG (TD3) \cite{fujimoto_addressing_2018} is an improvement of DDPG that uses two $Q$ functions and a delayed update of the actor policy in order to avoid overestimation of the $Q$ values.

\underline{Problem addressed}

How to avoid overestimation on the $Q$ function in DDPG?

\underline{Proposed approach}

Twin Delayed DDPG boosts DDPG by:

\begin{itemize}
  \item Using two $Q$ functions $Q_{\phi_1}$ and $Q_{\phi_2}$ instead of one, and using the smaller estimation of the two in order to compute the target $y$. The same target is used to update both.
  \item Using a random noise for exploration, but clipping it in order to ensure it falls in valid action ranges.
  \item Learning the policy by gradient ascent such as in DDPG, but only after a delay and by maximizing $J(\theta_a)$ regarding the value $Q_{\phi_1}$, (i.e. not using $Q_{\phi_2}$).
\end{itemize}


\subsubsection{SAC}
\label{SAC}

Family: Q Policy-gradient

Link: \url{https://arxiv.org/pdf/1801.01290.pdf}

\underline{TL;DR}

SAC \cite{haarnoja_soft_2018} is an off-policy actor critic algorithm based in maximum entropy reinforcement learning with an stochastic actor (i.e. an actor that learns from sampling actions based on the current policy instead of using past actions from memory) with the aim of maximizing both the reward and entropy (i.e. succeed at the task while acting as randomly as possible).

\underline{Problem addressed}

Using an off-policy algorithm (to avoid sampling inefficiency), how to achieve stability and convergence in learning?

\underline{Prior work}

Link: \url{https://arxiv.org/pdf/1702.08165.pdf}

This work is based in soft Q-learning \cite{haarnoja_reinforcement_2017}, which consists in learning a policy with its reward augmented such that the optimal policy is expressed via a Boltzmann distribution, and aims to maximize its entropy at each visited state in order to improve exploration and allow to better transfer skills between tasks. The maximum entropy policy is given by

\begin{equation}
  \pi^*_{max\_entropy} = \arg\max_\pi \sum_t \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} [r(s_t, a_t) + \alpha H (\pi(. \mid s_t))]
\end{equation}

For that, they want to approximate "soft" versions of the Q and V functions, proposing soft Q and V as a generalization of the conventional "hard" Q and V. Soft Q and V are defined by

\begin{eqnarray}
  Q_{soft}^* (s_t, a_t) &=& r_t + \mathbb{E}_{(s_{t+1}) \sim \rho_\pi} [\sum_{l+1}^\infty \gamma^l (r_{t+1} + \alpha H(\pi_{max\_entropy}^* (. \mid s_{t+l})))] \\
  V^*_{\text{soft}} &=& \alpha \log \int_A \exp \{\frac{1}{\alpha} Q_{soft}^* (s_t, a')\} da'
\end{eqnarray}

By Theorem 1, it is connected soft Q and V with the maximum entropy optimal policy by

\begin{equation}
  \pi^*_{maximum\_entropy}(a_t \mid s_t) = \exp\{\frac{1}{\alpha} (Q_{soft}^* (s_t, a_t) - V_{soft}^* (s_t))\}
\end{equation}

They propose an algorithm with an actor-critic setting based in the soft Q function and made tractable by an approximation and Stein variational gradient descent (SVGD).

\underline{Proposed approach}

Proposed approach consisting in:

\begin{itemize}
  \item An off policy formulation to reuse previous experience.
  \item An actor-critic architecture that learns 1) $Q_{\phi_1}$ and $Q_{\phi_2}$ functions as in TD3, 2) a policy $\pi_\theta$ and 3) a value function $V_\psi$. In this setting, the architecture employs a stochastic actor that sample actions from the learning policy in order to update the policy.
  \item Using entropy regularized reinfocement learning in order to give the agent an extra reward proportional to the entropy of the policy at each time step, given by $\alpha H(\pi(a \mid s_t)) = \mathbb{E}[-log(\pi(a \mid s_t))]$, being $\alpha$ an hyper-parameter that controls such bonus and therefore the stochasticity of the optimal policy.
\end{itemize}


\subsubsection{Ape-X}
\label{Ape-X}

Family: Q-value / Q policy gradient, distributed
Link: \url{https://arxiv.org/pdf/1803.00933.pdf}

\underline{TL;DR}

Ape-X \cite{} is an off-policy distributed algorithm that consists in a set of actors that gather experience and calculate their initial priorities, and a learner that updates the priorities and samples experiences and updates its priorities. This method can be used with both Q-value and Q-policy gradient algorithms.

\underline{Proposed approach}

Extend experience replay to the distributed setting

Deep RL algorithm decomposed into two parts:

\begin{enumerate}
  \item Acting: multiple actors that interacts with an environment, evaluates a policy implemented as a deep NN, and stores experience in a shared global replay memory. Actors concurrently run on CPUs in order to generate data.
  \item Learning: a single learner that samples batches of data from the memory to update its policy parameters. A single learner runs on GPU to sample the most useful experiences from the replay memory.
\end{enumerate}

As the method is off-policy, it can use algorithms such as DQN and its improvements such as dueling networks or double deep Q-networks, or algorithms such as DDPG.

\subsubsection{R2D2}
\label{R2D2}

Family: Q-value, distributed
Link: \url{https://openreview.net/pdf?id=r1lyTjAqYX}

\underline{TL;DR}

R2D2 is built over Ape-X and improves it by using an LSTM layer after the convolutional stack, and an n-step double Q-learning, among other improvements.

\underline{Proposed approach}

Recurrent Replay distributed DQN (R2D2) builds over Ape-X, and applies several modifications:

\begin{itemize}
  \item It stores fixed length ($m=80$) sequences of transition tuples $(s,a,r)$, overlapping each of the adjacent sequences by 40 time-steps, and not crossing episode boundaries.
  \item Using an LSTM layer after a convolutional stack.
  \item Using n-step double Q-learning, with $n=5$
  \item Using an invertible value function instead of clipping rewards.
  \item Replay prioritization uses a mixture of max and mean absolute n-step TD errors $\delta_i$ over the sequence $\nu \max_i \delta_i + (1âˆ’\nu)^{\bar{\delta}}$, using $\nu=\bar{\delta}=0.9$.
  \item Using a higher discount factor of $\gamma=0.997$ and disabled loss of life determining episode end in the Atari benchmark.
\end{itemize}

Improved by: R2D3 \ref{R2D3}

\subsubsection{R2D3}
\label{R2D3}

Family: Q-value, distributed
Link: \url{https://arxiv.org/pdf/1909.01387.pdf}

\underline{TL;DR}

R2D3 \cite{paine_making_2019} is an off-policy algorithm that improves R2D2 by including a buffer of expert demonstrations alongside a replay buffer, computing the policy by sampling a mixture of expert demonstrations and experience generated by the agent.

\underline{Problem addressed}

Tackle the problem of learning from demonstrations in hard exploration sparse-rewarded tasks with partially observable environments and highly variable initial conditions.

\underline{Proposed approach}

Recurrent Replay Distributed DQN from Demonstrations (R2D3) builds upon R2D2 including a demo replay buffer with expert demostrations of the task to be solved. The sampling proportion of this buffer vs the experience replay buffer is determined by a demo ratio hyper-parameter $\rho$.

\subsubsection{TODOs}

https://arxiv.org/abs/1707.01495
https://arxiv.org/pdf/1806.06923.pdf

Policy-gradient

\begin{itemize}
  \item ACER \url{https://arxiv.org/pdf/1611.01224.pdf}
  \item IMPALA \url{https://arxiv.org/pdf/1802.01561.pdf}
  \item GAE \url{https://arxiv.org/pdf/1506.02438.pdf}
\end{itemize}

Q Policy-gradient

\begin{itemize}
  \item
\end{itemize}

Q-value algorithms
\begin{itemize}
  \item DQN
  \item Dueling DQN
  \item Double DQN
  \item PER
  \item Rainbow
\end{itemize}

Other

\begin{itemize}
  \item
\end{itemize}




\subsection{[TODO] Milestones in RL}


2019: \begin{itemize}
  \item AlphaStar \cite{}
  \item Capture the Flag \cite{}
  \item Dota 2 \cite{}
  \item Multiagent \cite{}
\end{itemize}


\subsection{[TODO] Benchmarks}

\begin{itemize}
  \item OpenAI Gym
  \item Atari
  \item DeepMind Control Suite
  \item DeepMind BSuite
  \item DeepMind Lab
\end{itemize}

\subsection{[TODO] Hyper-parameter tuning in RL}



\subsection{Meta RL and Transfer Learning}

\subsubsection{[TODO] $RL^2$}

Link: \url{}

\subsubsection{Pretraining}

\underline{Self-supervision of policies}

In \cite{shelhamer_loss_2017} \href{https://arxiv.org/abs/1612.07307}{[link]}, it is proposed to perform self-supervision of policies by resorting to

\begin{enumerate}
  \item A joint optimization of reinforcement learning and self-supervision, where an agent network is trained, sharing the inputs, encoder (eg convolutional neural network) and transitions with architectures that train by self-supervision. Each of them trains a task-specific decoder that is discarded once the pre-training is complete, where the shared representation is transfer back to the initial agent network.
  \item Self-supervisory intrinsic rewards.
\end{enumerate}

\underline{Pretraining in base environment and transfer to others}

In the experiments in \cite{haarnoja_reinforcement_2017} \href{https://arxiv.org/pdf/1702.08165.pdf}{[link]}, a simulated robot agent is pretrained with their maximum entropy algorithm with a reward function that gives rewards when the robot moves quickly in an open environment (giving a reward equal to the speed of the center of mass), and then it is finetuned to similar tasks involving the same robot in different hallway environments.

Interesting statement: "near-optimal policies are often near-deterministic, which makes them poor initializers for new tasks."


\subsubsection{MAML RL}

Link: \url{https://arxiv.org/pdf/1703.03400.pdf}

The goal in few-shot meta-learning in RL is to enable an agent to quickly acquire a policy for a goal or a new test task (e.g. a new environment) using a small amount of experience in the test task.

As in RL the expected reward is normally not differentiable due to unknown dynamics, in MAML \cite{finn_model-agnostic_2017}, it is proposed a meta-learning algorithm with the following main loop:

\begin{enumerate}
  \item Sample a batch of tasks over a distribution of different tasks $T = \{T_1, \dots T_n\}$.
  \item For each sampled task $T_i$, it samples $K$ trajectories following the policy parametrized with the global parameters $\theta$, in order to compute the gradient $\nabla_\theta J(\pi_\theta)$.
  \item Apply gradient ascent to update a new set of weights $\theta'$ with a step size $\alpha$, used to sample $H$ new trajectories. Save parameters $\theta'$ until all the tasks of the batch are sampled.
  \item For each sampled task in the batch, update global weights $\theta$ with gradient ascent over $\theta$ and using each respective policy parametrized by $\theta'$.
\end{enumerate}


\subsubsection{REPTL}


\subsubsection{Fine-tuning}


\subsubsection{Universal Meta-Learning for Reinforcement Learning}

Link: https://arxiv.org/pdf/1806.04640.pdf


\subsection{[TODO] Some Frameworks}

Tensorflow

\begin{itemize}
  \item \href{}{Dopamine}
  \item \href{}{Tensorflow Agents}
  \item \href{https://stable-baselines.readthedocs.io/en/master/}{Stable baselines}
\end{itemize}

Pytorch

\begin{itemize}
  \item {https://github.com/kengz/SLM-Lab}{SLM-Lab}
\end{itemize}


\subsection{[TODO] Other algorithms and interesting papers}

\subsubsection{HAC}

Paper: \url{https://arxiv.org/pdf/1712.00948.pdf}
Blog post: \url{http://bigai.cs.brown.edu/2019/09/03/hac.html}

Hierarchical Arctor Critic (HAC) \cite{levy_learning_2019} is an algorithm that jointly learns a hierarchy of policies, by breaking a complex task into a set of subtasks consisting in short sequences of actions.

Authors propose a framework consisting of

\begin{enumerate}
  \item A hierarchical policy architecture that yields transitions and reward functions at all levels, treating the policies generated by the below levels as optimal.
  \item A method for learning multiple levels of policies independently and in parallel.
\end{enumerate}


\subsubsection{TODO}


\textbf{TODO}

https://openreview.net/pdf?id=r1etN1rtPB


\subsection{Good references}

\begin{itemize}
  \item \href{https://spinningup.openai.com/en/latest/spinningup/spinningup.html}{Open AI - Spinning Up as a Deep RL Researcher}
  \item \href{http://incompleteideas.net/book/the-book-2nd.html}{R. Sutton and A. Barto - Reinforcement Learning: An Introduction (2nd Edition)} and \href{https://github.com/ShangtongZhang/reinforcement-learning-an-introduction}{Python implementation of the exercises}.
  \item \href{https://www.alexirpan.com/2018/02/14/rl-hard.html}{A. Irpan - Deep Reinforcement Learning Doesn't Work Yet}
  \item \href{https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html}{L. Weng: A (Long) Peek into Reinforcement Learning}
  \item \href{http://arxiv.org/abs/1701.07274}{Y. Li - Deep Reinforcement Learning: An Overview}
\end{itemize}





\bibliographystyle{plainnat}
\bibliography{bibliography.bib}

\end{document}
