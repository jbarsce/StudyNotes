\documentclass[12pt, english]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage{pdfpages}
\usepackage[toc,page]{appendix}
\usepackage{hyperref}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{float} % Allows putting an [H] in \begin{figure} to specify the exact location of the figure
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{afterpage}
\usepackage[square,sort,comma,numbers]{natbib}

\begin{document}

\section{Reinforcement learning}

RL in 3 Equations (from Nando de Freitas talk at Khipu)

\begin{eqnarray}
  \nabla_\theta \rho_\pi (\theta) &=& \mathbb{E}_\tau [(\sum_{t=0}^T R_t \nabla_\theta \log \pi_\theta (a_t \mid s_t))] \\
  \nabla_\theta \rho_\pi (\theta) &=& \mathbb{E}_{d_\pi}(s) \pi(a \mid s) [Q_\pi(s,a) \nabla \log \pi_\theta (a \mid s)] \\
  Q^*(s,a) &=& \mathbb{E}_{s'} [r(s,a,s') + \gamma max_{a'} Q^*(s',a') \mid s,a]
\end{eqnarray}

\subsection{Key concepts}

\subsubsection{Importance sampling}

Prediction problem: exploration policy $b$ (considered as fixed/given, e.g. $\epsilon$-greedy policy), target policy $\pi$ (e.g. optimal policy, considered as fixed/given, a common case is the deterministic greedy policy). We have episodes of experience obtained by following policy $b$. How to estimate $v_\pi$ or $q_\pi$ having episodes that followed exploration policy $b$.

Assumption of coverage: $\pi(a \mid s) > 0$ implies $b(a \mid s) > 0$. Implication: $b$ must be stochastic in state where it is not identical to $\pi$.

Importance sampling (IS) is a technique used for estimating expected values under one distribution given samples from another. In this context, IS is applied to off-policy learning by weighting returns according to the relative probability of their trajectories. Therefore, IS ratio over a trajectory is given by

\begin{equation}
  IS = \prod_{i=1}^t \frac{\pi(a_i \mid s_i)}{b(a_i \mid s_i)}
\end{equation}

Now, $V_b (s) = \mathbb{E} [R_t \mid s_t = s]$, where $R_t$ is the sum of discounted rewards. Therefore, using the IS ratio,

$V_\pi (s) = \mathbb{E} [\prod_{i=1}^t \frac{\pi(a_i \mid s_i)}{b(a_i \mid s_i)} R_t \mid s_t = s]$

\subsection{Algorithms}

\subsubsection{TRPO \cite{schulman_trust_2015}}

Link: \url{https://arxiv.org/abs/1502.05477}

\underline{Problem addressed}:

\begin{enumerate}
  \item Given the advantage $\eta(\tilde{\pi})$ of a policy $\tilde{\pi}$ over a policy $\pi$.
  \item It was shown in Kakade and Langford (2002) that if $\pi_\theta(a \mid s)$ is a differentiable function of the parameter vector $\theta$, then an approximation of the advantage $L_\theta(\tilde{\theta})$ (which used visitation frequency $\rho_\theta$ instead of $\rho_{\tilde{\theta}}$) will match $\eta$ to first order, with a sufficiently small step $\alpha$.
\end{enumerate}

Problem: how to determine the sufficiently small step that improves the approximation $L_{\theta_{old}(\theta)}$, is also applicable to stochastic policies?


\underline{Proposed approach}

\begin{enumerate}
  \item Replace $\alpha$ with a distance measure between $\pi$ and $\tilde{pi}$.
  \item An algorithm similar to policy improvement is used to iteratively 1) calculate the advantages of the policy $\theta_i$ for every $s,a$ and 2) solves the constrained optimization problem $\pi_{i+1} = \arg\max_\pi [L_{\theta_i} - C D_{KL}^{\max}(\theta_i, \theta)]$.
  \item Using the penalty coefficient $C$ will involve very small step sizes.
  \item Therefore, an additional \textit{trust region constraint} is added to relax the optimization problem, that now seeks to maximize $L_{\theta}$, where the $KL$ divergence is subject to be lesser than a certain $\delta$.
  \item Now, calculating the $KL$ divergence as it is is infeasible due that it is bounded by every point in the state space.
\end{enumerate}

\subsubsection{PPO}

\subsection{Interesting papers}


https://openreview.net/pdf?id=r1etN1rtPB



\bibliographystyle{plainnat}
\bibliography{bibliography.bib}

\end{document}
