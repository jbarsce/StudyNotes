
@article{acerbi_practical_2017,
  title = {Practical {{Bayesian Optimization}} for {{Model Fitting}} with {{Bayesian Adaptive Direct Search}}},
  author = {Acerbi, Luigi and Ma, Wei Ji},
  year = {2017},
  month = may,
  abstract = {Computational models in fields such as computational neuroscience are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning. Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel hybrid BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers, on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, including `vanilla' BO, showing great promise for advanced BO techniques, and BADS in particular, as a general model-fitting tool.},
  archivePrefix = {arXiv},
  eprint = {1705.04405},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/acerbi_ma_2017_practical bayesian optimization for model fitting with bayesian adaptive direct.pdf;/home/juan/Zotero/storage/FS3XWRNZ/1705.html},
  journal = {arXiv:1705.04405 [q-bio, stat]},
  keywords = {Quantitative Biology - Neurons and Cognition,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  primaryClass = {q-bio, stat}
}

@inproceedings{ahmed_we_2016,
  title = {Do We Need "{{Harmless}}'' {{Bayesian Optimization}} and "{{First}}-{{Order}}'' {{Bayesian Optimization}}?},
  author = {Ahmed, M. O. and Shahriari, B. and Schmidt, M.},
  year = {2016},
  address = {{Barcelona, Spain}},
  abstract = {A recent empirical study highlighted the shocking result that,  for many hyper-parameter tuning problems, Bayesian optimization methods can be outperformed by random guessing run for twice as many iterations [1].  This is supported by theoretical  results  showing  the  optimality  of  random  search  under  certain  assumptions, but disagrees with other theoretical and empirical results showing that Bayesian optimization can lead to large gains in some situations.  In light of this fact, we propose two research directions that we believe the community should pursue.  First, we should focus on developing ``harmless'' Bayesian optimization methods that do no worse than random, and we propose a very simple ``harmless'' algorithm. Second, we should focus on developing first-order Bayesian optimization algorithms that use gradient information to improve performance for situations where Bayesian optimization already beats random.  We empirically show the advantage of both of these ideas in simple simulations. We also propose a simple strategy for reducing the memory and computational requirements of existing first-order Bayesian optimization methods by using directional derivatives instead of full gradients, which can be obtained from analytic functions even when gradient code is not available.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/ahmed et al_2016_do we need harmless'' bayesian optimization and first-order'' bayesian.pdf;/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/ahmed et al_2016_do we need harmless'' bayesian optimization and first-order'' bayesian2.pdf;/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/ahmed et al_2016_do we need harmless'' bayesian optimization and first-order'' bayesian3.pdf}
}

@inproceedings{akiba_optuna_2019,
  ids = {akiba\_optuna\_2019-1},
  title = {Optuna: {{A Next}}-Generation {{Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}  - {{KDD}} '19},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  pages = {2623--2631},
  publisher = {{ACM Press}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1145/3292500.3330701},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to lightweight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/ optuna/).},
  arxiv = {[object Object]},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/akiba et al_2019_optuna.pdf;/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/akiba et al_2019_optuna2.pdf;/home/juan/Zotero/storage/EMZGLNGL/1907.html},
  isbn = {978-1-4503-6201-6},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en}
}

@techreport{al-shedivat_continuous_2017,
  title = {Continuous {{Adaptation}} via {{Meta}}-{{Learning}} in {{Nonstationary}} and {{Competitive Environments}}},
  author = {{Al-Shedivat}, Maruan and Bansal, Trapit and Burda, Yuri and Sutskever, Ilya and Mordatch, Igor and Abbeel, Pieter},
  year = {2017},
  month = oct,
  abstract = {Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.},
  archivePrefix = {arXiv},
  eprint = {1710.03641},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/al-shedivat et al_2017_continuous adaptation via meta-learning in nonstationary and competitive.pdf;/home/juan/Zotero/storage/UBUHT2P7/1710.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning}
}

@book{alpaydin_introduction_2015,
  title = {Introduction {{To Machine Learning 3Rd Edition}}},
  author = {Alpaydin},
  year = {2015},
  edition = {3rd edition},
  publisher = {{Phi}},
  abstract = {Paperback International Edition ... Same contents as in the US edition at Low Cost !!},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/alpaydin_2015_introduction to machine learning 3rd edition.pdf},
  isbn = {978-81-203-5078-6},
  language = {English}
}

@article{andrychowicz_learning_2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and {de Freitas}, Nando},
  year = {2016},
  month = jun,
  abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  archivePrefix = {arXiv},
  eprint = {1606.04474},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/andrychowicz et al_2016_learning to learn by gradient descent by gradient descent.pdf;/home/juan/Zotero/storage/SENKEZ6D/1606.html},
  journal = {arXiv:1606.04474 [cs]},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{arulkumaran_brief_2017,
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = nov,
  volume = {34},
  pages = {26--38},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2743240},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  archivePrefix = {arXiv},
  eprint = {1708.05866},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/arulkumaran et al_2017_a brief survey of deep reinforcement learning.pdf;/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/arulkumaran et al_2017_a brief survey of deep reinforcement learning2.pdf;/home/juan/Zotero/storage/4KCMX3EB/1708.html;/home/juan/Zotero/storage/WN4NAHM6/1708.html},
  journal = {IEEE Signal Processing Magazine},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Machine Learning,Statistics - Machine Learning},
  number = {6}
}

@article{aslanides_universal_2017,
  title = {Universal {{Reinforcement Learning Algorithms}}: {{Survey}} and {{Experiments}}},
  shorttitle = {Universal {{Reinforcement Learning Algorithms}}},
  author = {Aslanides, John and Leike, Jan and Hutter, Marcus},
  year = {2017},
  pages = {1403--1410},
  abstract = {Electronic proceedings of IJCAI 2017},
  file = {/home/juan/Zotero/storage/J7SKIV6U/194.html}
}

@article{auer_finite-time_2002,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicol{\`o} and Fischer, Paul},
  year = {2002},
  month = may,
  volume = {47},
  pages = {235--256},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1013689704352},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/auer et al_2002_finite-time analysis of the multiarmed bandit problem.pdf;/home/juan/Zotero/storage/G85KCMLR/A1013689704352.html},
  journal = {Machine Learning},
  language = {en},
  number = {2-3}
}

@article{azizzadenesheli_maybe_nodate,
  title = {Maybe a {{Few Considerations}} in {{Reinforcement Learning Research}}?},
  author = {Azizzadenesheli, Kamyar},
  pages = {6},
  abstract = {Recent advances in computation power accessible to machine learning researchers has sparked the flurry of research interest in large scale Reinforcement Learning (RL). However, this interest is facing RL unique characteristics; Any theoretical study as well as empirical investigation of any innovative RL algorithm requires an extensive amount of experts' time and imposes intolerably massive computation cost. These challenges are even more vital in RL due to the significantly limited dedicated resources, such as human-time and computation resources, to RL research. These hardnesses and enormous costs impel an immediate consideration in both guiding our research directions and allocating our resources to advance RL in practice. We devote this paper to discuss a set of essential issues that are necessary to be addressed in order to provide greater supervisions in conducting future RL studies.},
  file = {/home/juan/Zotero/storage/A6Y4JDQA/Azizzadenesheli - Maybe a Few Considerations in Reinforcement Learni.pdf},
  language = {en}
}

@article{baker_emergent_2019,
  title = {Emergent {{Tool Use From Multi}}-{{Agent Autocurricula}}},
  author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
  year = {2019},
  month = sep,
  abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
  archivePrefix = {arXiv},
  eprint = {1909.07528},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/baker et al_2019_emergent tool use from multi-agent autocurricula.pdf;/home/juan/Zotero/storage/8BKVXN33/1909.html},
  journal = {arXiv:1909.07528 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{baptista_bayesian_2018,
  title = {Bayesian {{Optimization}} of {{Combinatorial Structures}}},
  author = {Baptista, Ricardo and Poloczek, Matthias},
  year = {2018},
  month = jun,
  abstract = {The optimization of expensive-to-evaluate blackbox functions over combinatorial structures is an ubiquitous task in machine learning, engineering and the natural sciences. The combinatorial explosion of the search space and costly evaluations pose challenges for current techniques in discrete optimization and machine learning, and critically require new algorithmic ideas. This article proposes, to the best of our knowledge, the first algorithm to overcome these challenges, based on an adaptive, scalable model that identifies useful combinatorial structure even when data is scarce. Our acquisition function pioneers the use of semidefinite programming to achieve efficiency and scalability. Experimental evaluations demonstrate that this algorithm consistently outperforms other methods from combinatorial and Bayesian optimization.},
  archivePrefix = {arXiv},
  eprint = {1806.08838},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/baptista_poloczek_2018_bayesian optimization of combinatorial structures.pdf},
  journal = {arXiv:1806.08838 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@inproceedings{barsce_cognitive_2013,
  title = {A {{Cognitive Approach}} to {{Real}}-Time {{Rescheduling Using SOAR}}-{{RL}}},
  author = {Barsce, Juan Cruz and Palombarini, Jorge A. and Mart{\'i}nez, Ernesto C.},
  year = {2013},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/barsce et al_2013_a cognitive approach to real-time rescheduling using soar-rl.pdf;C\:\\Users\\JuanCruz\\Documents\\Dropbox\\Intercambio Grupo de Investigacion\\Paper CACIC\\Barsce Palombarini Martínez - A Cognitive Approach to Real-time Rescheduling Using SOAR-RL (Camera Ready).pdf}
}

@article{barsce_hierarchical_2019,
  title = {A {{Hierarchical Two}}-Tier {{Approach}} to {{Hyper}}-Parameter {{Optimization}} in {{Reinforcement Learning}}},
  author = {Barsce, Juan Cruz and Palombarini, Jorge A. and Mart{\'i}nez, Ernesto},
  year = {2019},
  month = sep,
  abstract = {Optimization of hyper-parameters in reinforcement learning (RL) algorithms is a key task, because they determine how the agent will learn its policy by interacting with its environment, and thus what data is gathered. In this work, an approach that uses Bayesian optimization to perform a two-step optimization is proposed: first, categorical RL structure hyper-parameters are taken as binary variables and optimized with an acquisition function tailored for such variables. Then, at a lower level of abstraction, solution-level hyper-parameters are optimized by resorting to the expected improvement acquisition function, while using the best categorical hyper-parameters found in the optimization at the upper-level of abstraction. This two-tier approach is validated in a simulated control task. Results obtained are promising and open the way for more user-independent applications of reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1909.08332},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/barsce et al_2019_a hierarchical two-tier approach to hyper-parameter optimization in.pdf;/home/juan/Zotero/storage/NCDEPFZT/1909.html},
  journal = {arXiv:1909.08332 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{barsce_palombarini_martinez_reparacion_2013,
  title = {Reparaci{\'o}n {{Autom{\'a}tica}} de {{Planes}} de {{Producci{\'o}n Usando SOAR}}-{{RL}}},
  author = {{Barsce, Palombarini, Mart{\'i}nez}},
  year = {2013},
  file = {C\:\\Users\\JuanCruz\\Documents\\Dropbox\\Intercambio Grupo de Investigacion\\CNEISI 2013\\Paper\\Barsce Palombarini Martínez - Reparación Automática de Planes de Producción Usando SOAR-RL (Final).pdf}
}

@inproceedings{barsce_towards_2017,
  title = {Towards Autonomous Reinforcement Learning: {{Automatic}} Setting of Hyper-Parameters Using {{Bayesian}} Optimization},
  shorttitle = {Towards Autonomous Reinforcement Learning},
  booktitle = {2017 {{XLIII Latin American Computer Conference}} ({{CLEI}})},
  author = {Barsce, J. C. and Palombarini, J. A. and Mart{\'i}nez, E. C.},
  year = {2017},
  month = sep,
  pages = {1--9},
  doi = {10.1109/CLEI.2017.8226439},
  abstract = {With the increase of machine learning usage by industries and scientific communities in a variety of tasks such as text mining, image recognition and self-driving cars, automatic setting of hyper-parameter in learning algorithms is a key factor for obtaining good performances regardless of user expertise in the inner workings of the techniques and methodologies. In particular, for a reinforcement learning task, the efficiency of an agent learning a policy in an uncertain environment has a strong dependency on how hyper-parameters in the algorithm are set. In this work, an autonomous framework that employs Bayesian optimization and Gaussian process regression to optimize the hyper-parameters of a reinforcement learning algorithm is proposed. A gridworld example is discussed in order to show how hyper-parameter configurations of a learning algorithm (SARSA) are iteratively improved based on two performance functions.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/barsce et al_2017_towards autonomous reinforcement learning.pdf;/home/juan/Zotero/storage/4E9E7RD2/8226439.html},
  keywords = {autonomous reinforcement learning,Bayes methods,Bayesian optimization,Electronic mail,Gaussian process regression,Gaussian processes,hyper-parameter configurations,hyperparameter optimization,learning (artificial intelligence),Learning (artificial intelligence),Machine learning algorithms,machine learning usage,Optimization,regression analysis,reinforcement learning algorithm,reinforcement learning task,SARSA}
}

@article{barsce_towards_2018,
  title = {Towards {{Autonomous Reinforcement Learning}}: {{Automatic Setting}} of {{Hyper}}-Parameters Using {{Bayesian Optimization}}},
  shorttitle = {Towards {{Autonomous Reinforcement Learning}}},
  author = {Barsce, Juan Cruz and Palombarini, Jorge A. and Mart{\'i}nez, Ernesto C.},
  year = {2018},
  month = aug,
  volume = {21},
  pages = {1:1-1:22},
  doi = {10.19153/cleiej.21.2.1},
  abstract = {With the increase of machine learning usage by industries and scientific communities in a variety of tasks such as text mining, image recognition and self-driving cars, automatic setting of hyper-parameter in learning algorithms is a key factor for achieving satisfactory performance regardless of user expertise in the inner workings of the techniques and methodologies. In particular, for a reinforcement learning algorithm, the efficiency of an agent learning a control policy in an uncertain environment is heavily dependent on the hyper-parameters used to balance exploration with exploitation. In this work, an autonomous learning framework that integrates Bayesian optimization with Gaussian process regression to optimize the hyper-parameters of a reinforcement learning algorithm, is proposed. Also, a bandits-based approach to achieve a balance between computational costs and decreasing uncertainty about the Q-values, is presented. A gridworld example is used to highlight how hyper-parameter configurations of a learning algorithm (SARSA) are iteratively improved based on two performance functions.},
  archivePrefix = {arXiv},
  eprint = {1805.04748},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/barsce et al_2018_towards autonomous reinforcement learning.pdf;/home/juan/Zotero/storage/Q6UPG3AZ/1805.html},
  journal = {CLEI Electronic Journal},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning},
  language = {English},
  number = {2},
  series = {Special Issue Devoted to Extended Papers from {{CLEI}} 2017 plus One Regular Paper}
}

@article{barto_recent_2003,
  title = {Recent {{Advances}} in {{Hierarchical Reinforcement Learning}}},
  author = {Barto, Andrew G. and Mahadevan, Sridhar},
  year = {2003},
  month = oct,
  volume = {13},
  pages = {341--379},
  issn = {0924-6703, 1573-7594},
  doi = {10.1023/A:1025696116075},
  abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
  file = {/home/juan/Zotero/storage/H6QFVAPK/10.html},
  journal = {Discrete Event Dynamic Systems},
  language = {en},
  number = {4}
}

@incollection{barto_reinforcement_2004,
  title = {Reinforcement {{Learning}} and {{Its Relationship}} to {{Supervised Learning}}},
  booktitle = {Handbook of {{Learning}} and {{Approximate Dynamic Programming}}},
  author = {Barto, Andrew G. and Dietterich, Thomas G. and Si, Jennie and Barto, Andy and Powell, Warren and Wunsch, Donald},
  year = {2004},
  pages = {45--63},
  publisher = {{John Wiley \& Sons, Inc.}},
  doi = {10.1002/9780470544785.ch2},
  copyright = {Copyright \textcopyright{} 2004 the Institute of Electrical and Electronics Engineers, Inc. All rights reserved.},
  file = {/home/juan/Zotero/storage/ZJQAGUCK/summary.html},
  isbn = {978-0-470-54478-5},
  keywords = {approximate dynamic programming,machine learning,problem conversion,software systems,supervised learning},
  language = {en}
}

@incollection{bengio_practical_2012,
  title = {Practical {{Recommendations}} for {{Gradient}}-{{Based Training}} of {{Deep Architectures}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {Bengio, Yoshua},
  year = {2012},
  pages = {437--478},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_26},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  file = {/home/juan/Zotero/storage/NUQE4JWI/10.html},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{bergstra_algorithms_2011,
  title = {Algorithms for {{Hyper}}-{{Parameter Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Bergstra, James S. and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  editor = {{Shawe-Taylor}, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {2546--2554},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/bergstra et al_2011_algorithms for hyper-parameter optimization.pdf}
}

@article{bergstra_hyperopt_2013,
  title = {Hyperopt: {{A Python Library}} for {{Optimizing}} the {{Hyperparameters}} of {{Machine Learning Algorithms}}},
  author = {Bergstra, James and Yamins, Dan and Cox, David D},
  year = {2013},
  pages = {8},
  abstract = {Sequential model-based optimization (also known as Bayesian optimization) is one of the most efficient methods (per function evaluation) of function minimization. This efficiency makes it appropriate for optimizing the hyperparameters of machine learning algorithms that are slow to train. The Hyperopt library provides algorithms and parallelization infrastructure for performing hyperparameter optimization (model selection) in Python. This paper presents an introductory tutorial on the usage of the Hyperopt library, including the description of search spaces, minimization (in serial and parallel), and the analysis of the results collected in the course of minimization. The paper closes with some discussion of ongoing and future work.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/bergstra et al_2013_hyperopt.pdf},
  language = {en}
}

@article{bergstra_random_2012,
  title = {Random {{Search}} for {{Hyper}}-{{Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  year = {2012},
  volume = {13},
  pages = {281--305},
  issn = {ISSN 1533-7928},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/bergstra_bengio_2012_random search for hyper-parameter optimization.pdf;/home/juan/Zotero/storage/T8SX5CME/bergstra12a.html},
  journal = {Journal of Machine Learning Research},
  number = {Feb}
}

@book{bishop_pattern_2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher},
  year = {2006},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted. Coming soon: *For students, worked solutions to a subset of exercises available on a public web site (for exercises marked "www" in the text) *For instructors, worked solutions to remaining exercises from the Springer web site *Lecture slides to accompany each chapter *Data sets available for download},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/bishop_2006_pattern recognition and machine learning.pdf;/home/juan/Zotero/storage/X9HJLHLR/9780387310732.html},
  isbn = {978-0-387-31073-2},
  language = {en},
  series = {Information {{Science}} and {{Statistics}}}
}

@article{botvinick_reinforcement_2019,
  title = {Reinforcement {{Learning}}, {{Fast}} and {{Slow}}},
  author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and {Kurth-Nelson}, Zeb and Blundell, Charles and Hassabis, Demis},
  year = {2019},
  month = apr,
  issn = {13646613},
  doi = {10.1016/j.tics.2019.02.006},
  file = {/home/juan/Zotero/storage/2YE9BSSM/Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf;/home/juan/Zotero/storage/RBWS7JCZ/Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en}
}

@book{boyd_convex_2004,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  year = {2004},
  month = mar,
  edition = {1 edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  abstract = {Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics.},
  isbn = {978-0-521-83378-3},
  language = {English}
}

@article{brochu_tutorial_2010,
  title = {A {{Tutorial}} on {{Bayesian Optimization}} of {{Expensive Cost Functions}}, with {{Application}} to {{Active User Modeling}} and {{Hierarchical Reinforcement Learning}}},
  author = {Brochu, Eric and Cora, Vlad M. and {de Freitas}, Nando},
  year = {2010},
  month = dec,
  abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
  archivePrefix = {arXiv},
  eprint = {1012.2599},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/brochu et al_2010_a tutorial on bayesian optimization of expensive cost functions, with.pdf;/home/juan/Zotero/storage/EHN8XBJ6/1012.html},
  journal = {arXiv:1012.2599 [cs]},
  keywords = {Computer Science - Learning,G.1.6,G.3,I.2.6},
  primaryClass = {cs}
}

@article{brockman_openai_2016,
  title = {{{OpenAI Gym}}},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  year = {2016},
  eprint = {arXiv:1606.01540}
}

@article{brown_superhuman_2019,
  ids = {brown\_superhuman\_2019},
  title = {Superhuman {{AI}} for Multiplayer Poker},
  author = {Brown, Noam and Sandholm, Tuomas},
  year = {2019},
  month = jul,
  pages = {eaay2400},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aay2400},
  abstract = {In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker, the most popular form of poker played by humans.},
  copyright = {Copyright \textcopyright{} 2019, American Association for the Advancement of Science},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/brown_sandholm_2019_superhuman ai for multiplayer poker.pdf;/home/juan/Zotero/storage/2CPTBQBY/science.html;/home/juan/Zotero/storage/JLC3CRQ2/science.html},
  journal = {Science},
  language = {en},
  pmid = {31296650}
}

@article{browne_survey_2012,
  title = {A {{Survey}} of {{Monte Carlo Tree Search Methods}}},
  author = {Browne, C. B. and Powley, E. and Whitehouse, D. and Lucas, S. M. and Cowling, P. I. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
  year = {2012},
  month = mar,
  volume = {4},
  pages = {1--43},
  issn = {1943-068X},
  doi = {10.1109/TCIAIG.2012.2186810},
  abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
  file = {/home/juan/Zotero/storage/6RFX93NB/6145622.html},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  keywords = {Artificial intelligence,Artificial intelligence (AI),bandit-based methods,computer Go,Computers,Decision theory,game search,game theory,Game theory,Games,key game,Markov processes,MCTS research,Monte Carlo methods,Monte Carlo tree search (MCTS),Monte carlo tree search methods,nongame domains,random sampling generality,tree searching,upper confidence bounds (UCB),upper confidence bounds for trees (UCT)},
  number = {1}
}

@article{burda_large-scale_2018,
  title = {Large-{{Scale Study}} of {{Curiosity}}-{{Driven Learning}}},
  author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  year = {2018},
  month = aug,
  abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
  archivePrefix = {arXiv},
  eprint = {1808.04355},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/burda et al_2018_large-scale study of curiosity-driven learning.pdf;/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/burda et al_2018_large-scale study of curiosity-driven learning2.pdf;/home/juan/Zotero/storage/IAREHIUQ/Burda et al_2018_Large-Scale Study of Curiosity-Driven Learning.pdf;/home/juan/Zotero/storage/239WT7DV/1808.html;/home/juan/Zotero/storage/932C6HE9/1808.html;/home/juan/Zotero/storage/LF4J4QZG/1808.html},
  journal = {arXiv:1808.04355 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{carvalho_horseshoe_2010,
  title = {The Horseshoe Estimator for Sparse Signals},
  author = {Carvalho, C. M. and Polson, N. G. and Scott, J. G.},
  year = {2010},
  month = jun,
  volume = {97},
  pages = {465--480},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asq017},
  abstract = {This paper proposes a new approach to sparse-signal detection called the horseshoe estimator. We show that the horseshoe is a close cousin of the lasso in that it arises from the same class of multivariate scale mixtures of normals, but that it is almost universally superior to the double-exponential prior at handling sparsity. A theoretical framework is proposed for understanding why the horseshoe is a better default ``sparsity'' estimator than those that arise from powered-exponential priors. Comprehensive numerical evidence is presented to show that the difference in performance can often be large. Most importantly, we show that the horseshoe estimator corresponds quite closely to the answers one would get if one pursued a full Bayesian model-averaging approach using a ``two-groups'' model: a point mass at zero for noise, and a continuous density for signals. Surprisingly, this correspondence holds both for the estimator itself and for the classification rule induced by a simple threshold applied to the estimator. We show how the resulting thresholded horseshoe can also be viewed as a novel Bayes multiple-testing procedure.},
  file = {/home/juan/Zotero/storage/73WGLAL7/Carvalho et al. - 2010 - The horseshoe estimator for sparse signals.pdf},
  journal = {Biometrika},
  language = {en},
  number = {2}
}

@article{chae_autonomous_2017,
  title = {Autonomous {{Braking System}} via {{Deep Reinforcement Learning}}},
  author = {Chae, Hyunmin and Kang, Chang Mook and Kim, ByeoungDo and Kim, Jaekyum and Chung, Chung Choo and Choi, Jun Won},
  year = {2017},
  month = feb,
  abstract = {In this paper, we propose a new autonomous braking system based on deep reinforcement learning. The proposed autonomous braking system automatically decides whether to apply the brake at each time step when confronting the risk of collision using the information on the obstacle obtained by the sensors. The problem of designing brake control is formulated as searching for the optimal policy in Markov decision process (MDP) model where the state is given by the relative position of the obstacle and the vehicle's speed, and the action space is defined as whether brake is stepped or not. The policy used for brake control is learned through computer simulations using the deep reinforcement learning method called deep Q-network (DQN). In order to derive desirable braking policy, we propose the reward function which balances the damage imposed to the obstacle in case of accident and the reward achieved when the vehicle runs out of risk as soon as possible. DQN is trained for the scenario where a vehicle is encountered with a pedestrian crossing the urban road. Experiments show that the control agent exhibits desirable control behavior and avoids collision without any mistake in various uncertain environments.},
  archivePrefix = {arXiv},
  eprint = {1702.02302},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/chae et al_2017_autonomous braking system via deep reinforcement learning.pdf;/home/juan/Zotero/storage/NZ4TTI94/1702.html},
  journal = {arXiv:1702.02302 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@phdthesis{chaslot_monte-carlo_2010,
  title = {Monte-{{Carlo Tree Search}}},
  author = {Chaslot, Guillaume},
  year = {2010}
}

@article{chen_bayesian_2018,
  title = {Bayesian {{Optimization}} in {{AlphaGo}}},
  author = {Chen, Yutian and Huang, Aja and Wang, Ziyu and Antonoglou, Ioannis and Schrittwieser, Julian and Silver, David and {de Freitas}, Nando},
  year = {2018},
  month = dec,
  abstract = {During the development of AlphaGo, its many hyper-parameters were tuned with Bayesian optimization multiple times. This automatic tuning process resulted in substantial improvements in playing strength. For example, prior to the match with Lee Sedol, we tuned the latest AlphaGo agent and this improved its win-rate from 50\% to 66.5\% in self-play games. This tuned version was deployed in the final match. Of course, since we tuned AlphaGo many times during its development cycle, the compounded contribution was even higher than this percentage. It is our hope that this brief case study will be of interest to Go fans, and also provide Bayesian optimization practitioners with some insights and inspiration.},
  archivePrefix = {arXiv},
  eprint = {1812.06855},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/chen et al_2018_bayesian optimization in alphago.pdf;/home/juan/Zotero/storage/HPUM8YGJ/1812.html},
  journal = {arXiv:1812.06855 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{claesen_easy_2014,
  title = {Easy {{Hyperparameter Search Using Optunity}}},
  author = {Claesen, Marc and Simm, Jaak and Popovic, Dusan and Moreau, Yves and De Moor, Bart},
  year = {2014},
  month = dec,
  abstract = {Optunity is a free software package dedicated to hyperparameter optimization. It contains various types of solvers, ranging from undirected methods to direct search, particle swarm and evolutionary optimization. The design focuses on ease of use, flexibility, code clarity and interoperability with existing software in all machine learning environments. Optunity is written in Python and contains interfaces to environments such as R and MATLAB. Optunity uses a BSD license and is freely available online at http://www.optunity.net.},
  archivePrefix = {arXiv},
  eprint = {1412.1114},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/claesen et al_2014_easy hyperparameter search using optunity.pdf;/home/juan/Zotero/storage/859K3TEG/1412.html},
  journal = {arXiv:1412.1114 [cs]},
  keywords = {Computer Science - Learning,G.4,I.2.5,I.2.6,I.5.2},
  primaryClass = {cs}
}

@article{clark_moe_2014,
  title = {{{MOE}}: {{A}} Global, Black Box Optimization Engine for Real World Metric Optimization},
  author = {Clark, Scott and Liu, Eric and Frazier, Peter and Wang, JiaLei and Oktay, Deniz and Vesdapunt, Norases},
  year = {2014}
}

@article{cobbe_quantifying_2018,
  title = {Quantifying {{Generalization}} in {{Reinforcement Learning}}},
  author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  year = {2018},
  month = dec,
  abstract = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
  archivePrefix = {arXiv},
  eprint = {1812.02341},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/cobbe et al_2018_quantifying generalization in reinforcement learning.pdf;/home/juan/Zotero/storage/H28ST55V/Cobbe et al. - 2018 - Quantifying Generalization in Reinforcement Learni.pdf},
  journal = {arXiv:1812.02341 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{cortes_adanet_2016,
  title = {{{AdaNet}}: {{Adaptive Structural Learning}} of {{Artificial Neural Networks}}},
  shorttitle = {{{AdaNet}}},
  author = {Cortes, Corinna and Gonzalvo, Xavi and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
  year = {2016},
  month = jul,
  abstract = {We present new algorithms for adaptively learning artificial neural networks. Our algorithms (AdaNet) adaptively learn both the structure of the network and its weights. They are based on a solid theoretical analysis, including data-dependent generalization guarantees that we prove and discuss in detail. We report the results of large-scale experiments with one of our algorithms on several binary classification tasks extracted from the CIFAR-10 dataset. The results demonstrate that our algorithm can automatically learn network structures with very competitive performance accuracies when compared with those achieved for neural networks found by standard approaches.},
  archivePrefix = {arXiv},
  eprint = {1607.01097},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/cortes et al_2016_adanet.pdf;/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/cortes et al_2016_adanet2.pdf;/home/juan/Zotero/storage/9CHVY774/1607.html;/home/juan/Zotero/storage/9SN3G7AR/1607.html},
  journal = {arXiv:1607.01097 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{cully_robots_2015,
  title = {Robots That Can Adapt like Animals},
  author = {Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
  year = {2015},
  month = may,
  volume = {521},
  pages = {503--507},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14422},
  abstract = {As robots leave the controlled environments of factories to autonomously function in more complex, natural environments, they will have to respond to the inevitable fact that they will become damaged. However, while animals can quickly adapt to a wide variety of injuries, current robots cannot "think outside the box" to find a compensatory behavior when damaged: they are limited to their pre-specified self-sensing abilities, can diagnose only anticipated failure modes, and require a pre-programmed contingency plan for every type of potential damage, an impracticality for complex robots. Here we introduce an intelligent trial and error algorithm that allows robots to adapt to damage in less than two minutes, without requiring self-diagnosis or pre-specified contingency plans. Before deployment, a robot exploits a novel algorithm to create a detailed map of the space of high-performing behaviors: This map represents the robot's intuitions about what behaviors it can perform and their value. If the robot is damaged, it uses these intuitions to guide a trial-and-error learning algorithm that conducts intelligent experiments to rapidly discover a compensatory behavior that works in spite of the damage. Experiments reveal successful adaptations for a legged robot injured in five different ways, including damaged, broken, and missing legs, and for a robotic arm with joints broken in 14 different ways. This new technique will enable more robust, effective, autonomous robots, and suggests principles that animals may use to adapt to injury.},
  archivePrefix = {arXiv},
  eprint = {1407.3501},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/cully et al_2015_robots that can adapt like animals.pdf;/home/juan/Zotero/storage/5573KP8U/1407.html},
  journal = {Nature},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Neurons and Cognition},
  number = {7553}
}

@inproceedings{dabney_adaptive_2012,
  title = {Adaptive {{Step}}-{{Size}} for {{Online Temporal Difference Learning}}},
  booktitle = {Twenty-{{Sixth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Dabney, William and Barto, Andrew G.},
  year = {2012},
  abstract = {The step-size, often denoted as {$\alpha$}, is a key parameter for most incremental learning algorithms. Its importance is especially pronounced when performing online temporal difference (TD) learning with function approximation. Several methods have been developed to adapt the step-size online. These range from straightforward back-off strategies to adaptive algorithms based on gradient descent. We derive an adaptive upper bound on the step-size parameter to guarantee that online TD learning with linear function approximation will not diverge. We then empirically evaluate algorithms using this upper bound as a heuristic for adapting the stepsize parameter online. We compare performance with related work including HL({$\lambda$}) and Autostep. Our results show that this adaptive upper bound heuristic out-performs all existing methods without requiring any meta-parameters. This effectively eliminates the need to tune the learning rate of temporal difference learning with linear function approximation.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/dabney_barto_2012_adaptive step-size for online temporal difference learning.pdf;/home/juan/Zotero/storage/MMY7LZIQ/summary.html}
}

@article{de_asis_multi-step_2017,
  title = {Multi-Step {{Reinforcement Learning}}: {{A Unifying Algorithm}}},
  shorttitle = {Multi-Step {{Reinforcement Learning}}},
  author = {De Asis, Kristopher and {Hernandez-Garcia}, J. Fernando and Holland, G. Zacharias and Sutton, Richard S.},
  year = {2017},
  month = mar,
  abstract = {Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD({$\lambda$}) elegantly unifies one-step TD prediction with Monte Carlo methods through the use of eligibility traces and the trace-decay parameter {$\lambda$}. Currently, there are a multitude of algorithms that can be used to perform TD control, including Sarsa, Q-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be extended across multiple time steps to achieve better performance. Each of these algorithms is seemingly distinct, and no one dominates the others for all problems. In this paper, we study a new multi-step action-value algorithm called Q({$\sigma$}) that unifies and generalizes these existing algorithms, while subsuming them as special cases. A new parameter, {$\sigma$}, is introduced to allow the degree of sampling performed by the algorithm at each step during its backup to be continuously varied, with Sarsa existing at one extreme (full sampling), and Expected Sarsa existing at the other (pure expectation). Q({$\sigma$}) is generally applicable to both on- and off-policy learning, but in this work we focus on experiments in the on-policy case. Our results show that an intermediate value of {$\sigma$}, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater performance.},
  archivePrefix = {arXiv},
  eprint = {1703.01327},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/ZB4V755D/De Asis et al. - 2017 - Multi-step Reinforcement Learning A Unifying Algo.pdf},
  journal = {arXiv:1703.01327 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@book{deisenroth_efficient_2010,
  title = {Efficient {{Reinforcement Learning Using Gaussian Processes}}},
  author = {Deisenroth, Marc Peter},
  year = {2010},
  publisher = {{KIT Scientific Publishing}},
  isbn = {978-3-86644-569-7},
  language = {en}
}

@article{deisenroth_pilco_nodate,
  title = {{{PILCO}}: {{A Model}}-{{Based}} and {{Data}}-{{Efficient Approach}} to {{Policy Search}}},
  author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
  pages = {8},
  abstract = {In this paper, we introduce pilco, a practical, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-ofthe-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.},
  file = {/home/juan/Zotero/storage/J5FZDXMZ/Deisenroth and Rasmussen - PILCO A Model-Based and Data-Efficient Approach t.pdf},
  language = {en}
}

@article{duan_rl2_2016,
  title = {{{RL2}}: {{Fast Reinforcement Learning}} via {{Slow Reinforcement Learning}}},
  shorttitle = {{{RL}}\$\^2\$},
  author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  year = {2016},
  month = nov,
  abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\$\^2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\$\^2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\$\^2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\$\^2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  archivePrefix = {arXiv},
  eprint = {1611.02779},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/duan et al_2016_rl2.pdf;/home/juan/Zotero/storage/3G3CGEDP/1611.html},
  journal = {arXiv:1611.02779 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{dulac-arnold_challenges_nodate,
  title = {Challenges of {{Real}}-{{World Reinforcement Learning}}},
  author = {{Dulac-Arnold}, Gabriel and Mankowitz, Daniel and Hester, Todd},
  pages = {14},
  abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.},
  file = {/home/juan/Zotero/storage/3RGG4EV7/Dulac-Arnold et al. - Challenges of Real-World Reinforcement Learning.pdf},
  language = {en}
}

@article{dulac-arnold_deep_2015,
  title = {Deep {{Reinforcement Learning}} in {{Large Discrete Action Spaces}}},
  author = {{Dulac-Arnold}, Gabriel and Evans, Richard and {van Hasselt}, Hado and Sunehag, Peter and Lillicrap, Timothy and Hunt, Jonathan and Mann, Timothy and Weber, Theophane and Degris, Thomas and Coppin, Ben},
  year = {2015},
  month = dec,
  abstract = {Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions.},
  archivePrefix = {arXiv},
  eprint = {1512.07679},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/dulac-arnold et al_2015_deep reinforcement learning in large discrete action spaces.pdf;/home/juan/Zotero/storage/BF79A7ER/1512.html},
  journal = {arXiv:1512.07679 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{dzeroski_relational_2001,
  title = {Relational {{Reinforcement Learning}}},
  author = {D{\v z}eroski, Sa{\v s}o and Raedt, Luc De and Driessens, Kurt},
  year = {2001},
  month = apr,
  volume = {43},
  pages = {7--52},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1007694015589},
  abstract = {Relational reinforcement learning is presented, a learning technique that combines reinforcement learning with relational learning or inductive logic programming. Due to the use of a more expressive representation language to represent states, actions and Q-functions, relational reinforcement learning can be potentially applied to a new range of learning tasks. One such task that we investigate is planning in the blocks world, where it is assumed that the effects of the actions are unknown to the agent and the agent has to learn a policy. Within this simple domain we show that relational reinforcement learning solves some existing problems with reinforcement learning. In particular, relational reinforcement learning allows us to employ structural representations, to abstract from specific goals pursued and to exploit the results of previous learning phases when addressing new (more complex) situations.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/džeroski et al_2001_relational reinforcement learning.pdf;/home/juan/Zotero/storage/A8HCVWQF/10.html},
  journal = {Machine Learning},
  language = {en},
  number = {1-2}
}

@article{ebden_gaussian_2008,
  title = {Gaussian {{Processes}}: {{A Quick Introduction}}},
  shorttitle = {Gaussian {{Processes}}},
  author = {Ebden, Mark},
  year = {2008},
  abstract = {A gentle introduction to Gaussian processes (GPs). The three parts of the document consider GPs for regression, classification, and dimensionality reduction.},
  archivePrefix = {arXiv},
  eprint = {1505.02965},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/ebden_2008_gaussian processes.pdf;/home/juan/Zotero/storage/IQX9Q2P8/1505.html},
  journal = {arXiv:1505.02965 [math, stat]},
  keywords = {Mathematics - Statistics Theory},
  primaryClass = {math, stat}
}

@article{ecoffet_go-explore_2019,
  title = {Go-{{Explore}}: A {{New Approach}} for {{Hard}}-{{Exploration Problems}}},
  shorttitle = {Go-{{Explore}}},
  author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2019},
  month = jan,
  abstract = {A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of "superhuman" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).},
  archivePrefix = {arXiv},
  eprint = {1901.10995},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/ecoffet et al_2019_go-explore.pdf;/home/juan/Zotero/storage/9852WNPD/1901.html},
  journal = {arXiv:1901.10995 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ehrhart_coronary_1975,
  title = {Coronary Vascular and Myocardial Responses to Carotid Body Stimulation in the Dog},
  author = {Ehrhart, I. C. and Parker, P. E. and Weidner, W. J. and Dabney, J. M. and Scott, J. B. and Haddy, F. J.},
  year = {1975},
  month = sep,
  volume = {229},
  pages = {754--760},
  issn = {0002-9513},
  doi = {10.1152/ajplegacy.1975.229.3.754},
  abstract = {Coronary vascular and myocardial responses to selective hypoxic and/or hypercapnic carotid chemoreceptor stimulation were investigated in constantly ventilated, pentobarbital or urethan-chloralose anesthetized dogs. Bilaterally isolated carotid chemoreceptors were perfused with autologous blood of varying O2 and CO2 tensions via an extracorporeal lung circuit. Systemic gas tensions were unchanged. Effects of carotid chemoreceptor stimulation on coronary vascular resistance, left ventricular dP/dt, and strain-gauge arch output were studied at natural coronary blood flow with the chest closed and during constant-flow perfusion of the left common coronary artery with the chest open. Carotid chemoreceptor stimulation slightly increased left ventricular dP/dt and slightly decreased the strain-gauge arch output, while markedly increasing systemic pressure. Coronary blood flow increased; however, coronary vascular resistance wa.as not affected. These studies show that local carotid body stimulation increases coronary blood flow but has little effect on the myocardium. The increase in coronary blood flow results mainly from an increase in systemic arterial pressure. Thus these data provide little evidence for increased sympathetic activity of the heart during local stimulation of the carotid chemoreceptors with hypoxic and hypercapnic blood.},
  journal = {The American Journal of Physiology},
  keywords = {Animals,Blood Pressure,Carbon Dioxide,Carotid Body,Carotid Sinus,Chemoreceptor Cells,Coronary Circulation,Dogs,Female,Heart,Heart Rate,Hydrogen-Ion Concentration,Male,Oxygen,Perfusion,Pressure,Vagotomy,Vascular Resistance,Ventricular Function},
  language = {eng},
  number = {3},
  pmid = {2017}
}

@article{elshawi_automated_2019,
  title = {Automated {{Machine Learning}}: {{State}}-of-{{The}}-{{Art}} and {{Open Challenges}}},
  shorttitle = {Automated {{Machine Learning}}},
  author = {Elshawi, Radwa and Maher, Mohamed and Sakr, Sherif},
  year = {2019},
  month = jun,
  abstract = {With the continuous and vast increase in the amount of data in our digital world, it has been acknowledged that the number of knowledgeable data scientists can not scale to address these challenges. Thus, there was a crucial need for automating the process of building good machine learning models. In the last few years, several techniques and frameworks have been introduced to tackle the challenge of automating the process of Combined Algorithm Selection and Hyper-parameter tuning (CASH) in the machine learning domain. The main aim of these techniques is to reduce the role of the human in the loop and fill the gap for non-expert machine learning users by playing the role of the domain expert. In this paper, we present a comprehensive survey for the state-of-the-art efforts in tackling the CASH problem. In addition, we highlight the research work of automating the other steps of the full complex machine learning pipeline (AutoML) from data understanding till model deployment. Furthermore, we provide comprehensive coverage for the various tools and frameworks that have been introduced in this domain. Finally, we discuss some of the research directions and open challenges that need to be addressed in order to achieve the vision and goals of the AutoML process.},
  archivePrefix = {arXiv},
  eprint = {1906.02287},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/elshawi et al_2019_automated machine learning.pdf;/home/juan/Zotero/storage/PBRGGFEG/1906.html},
  journal = {arXiv:1906.02287 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{elsken_neural_nodate,
  title = {Neural {{Architecture Search}}: {{A Survey}}},
  author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  pages = {21},
  abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
  file = {/home/juan/Zotero/storage/UPEWBH7W/Elsken et al. - Neural Architecture Search A Survey.pdf},
  language = {en}
}

@article{engel_bayes_nodate,
  title = {Bayes {{Meets Bellman}}: {{The Gaussian Process Approach}} to {{Temporal Difference Learning}}},
  author = {Engel, Yaakov and Mannor, Shie and Meir, Ron},
  pages = {8},
  abstract = {Wepresent a novel Bayesian approach to the problemof value function estimation in continuous state spaces. Wedefine a probabilistic generative modelfor the value function by imposing a Gaussian prior over value functions and assuming a Gaussian noise model. Due to the Gaussian nature of the random processes involved, the posterior distribution of the value function is also Gaussian and is therefore described entirely by its meanand covariance. Wederive exact expressions for the posterior process moments,and utilizing an efficient sequential sparsification method, we describe an on-line algorithm for learning them. Wedemonstrate the operation of the algorithm on a 2-dimensional continuous spatial navigation domain.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/engel et al_bayes meets bellman.pdf},
  language = {en}
}

@inproceedings{engel_reinforcement_2005,
  title = {Reinforcement Learning with {{Gaussian}} Processes},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning  - {{ICML}} '05},
  author = {Engel, Yaakov and Mannor, Shie and Meir, Ron},
  year = {2005},
  pages = {201--208},
  publisher = {{ACM Press}},
  address = {{Bonn, Germany}},
  doi = {10.1145/1102351.1102377},
  abstract = {Gaussian Process Temporal Difference (GPTD) learning offers a Bayesian solution to the policy evaluation problem of reinforcement learning. In this paper we extend the GPTD framework by addressing two pressing issues, which were not adequately treated in the original GPTD paper (Engel et al., 2003). The first is the issue of stochasticity in the state transitions, and the second is concerned with action selection and policy improvement. We present a new generative model for the value function, deduced from its relation with the discounted return. We derive a corresponding on-line algorithm for learning the posterior moments of the value Gaussian process. We also present a SARSA based extension of GPTD, termed GPSARSA, that allows the selection of actions and the gradual improvement of policies without requiring a world-model.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/engel et al_2005_reinforcement learning with gaussian processes.pdf},
  isbn = {978-1-59593-180-1},
  language = {en}
}

@inproceedings{engstrom_implementation_2019,
  ids = {engstrom\_implementation\_2019},
  title = {Implementation {{Matters}} in {{Deep RL}}: {{A Case Study}} on {{PPO}} and {{TRPO}}},
  shorttitle = {Implementation {{Matters}} in {{Deep RL}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  year = {2019},
  month = sep,
  abstract = {We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms, Proximal Policy Optimization and Trust Region Policy Optimization. We...},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/engstrom et al_2019_implementation matters in deep rl2.pdf;/home/juan/Zotero/storage/J4MLN2FW/Engstrom et al. - 2019 - Implementation Matters in Deep RL A Case Study on.pdf;/home/juan/Zotero/storage/RALHVIH2/forum.html}
}

@inproceedings{epshteyn_active_2008,
  title = {{Active reinforcement learning}},
  booktitle = {{Proceedings of the 25th International Conference on Machine Learning}},
  author = {Epshteyn, Arkady and Vogel, Adam and Dejong, Gerald},
  year = {2008},
  pages = {296--303},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/epshteyn et al_2008_active reinforcement learning.pdf;/home/juan/Zotero/storage/KFJFHM79/active-reinforcement-learning.html},
  language = {English (US)}
}

@article{espeholt_impala_2018,
  ids = {espeholt\_impala\_2018-1},
  title = {{{IMPALA}}: {{Scalable Distributed Deep}}-{{RL}} with {{Importance Weighted Actor}}-{{Learner Architectures}}},
  shorttitle = {{{IMPALA}}},
  author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  year = {2018},
  month = feb,
  abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
  archivePrefix = {arXiv},
  eprint = {1802.01561},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/espeholt et al_2018_impala2.pdf;/home/juan/Zotero/storage/PQYRH367/Espeholt et al. - 2018 - IMPALA Scalable Distributed Deep-RL with Importan.pdf;/home/juan/Zotero/storage/R4FZ4TZ7/1802.html},
  journal = {arXiv:1802.01561 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{fernandez_parameters_2018,
  title = {Parameters {{Tuning}} and {{Optimization}} for {{Reinforcement Learning Algorithms Using Evolutionary Computing}}},
  booktitle = {2018 {{International Conference}} on {{Information Systems}} and {{Computer Science}} ({{INCISCOS}})},
  author = {Fernandez, F. Carde{\~n}oso and Caarls, W.},
  year = {2018},
  month = nov,
  pages = {301--305},
  doi = {10.1109/INCISCOS.2018.00050},
  abstract = {Setting up the correct hyperparameters in reinforcement learning (RL) algorithms is an important part to achieve good performance in its execution and convergence. Manual adjustment for these hyperparameters is not a good practice because it consumes too much time and effort, therefore, it is advisable to use computational tools to optimize this tuning. Evolutionary computation (EC) techniques can be a good tool to tune and optimize the hyperparameters in the different algorithms. In this project we used the genetic algorithms (GA) approach to find the value of the hyperparameters that best fit the performance of the SARSA and Q-learning RL algorithms, addressing the underactuated pendulum swing-up task, maximizing the final rewards acquired and the agent's learning speed. We obtained good solutions with a fairly simple algorithm, but required multiple random restarts of the GA to escape local minima.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/fernandez_caarls_2018_parameters tuning and optimization for reinforcement learning algorithms using.pdf;/home/juan/Zotero/storage/VDSVZWU4/8564542.html},
  keywords = {agent learning,evolutionary computation techniques,evolutionary computing,genetic algorithms,Genetic algorithms,genetic algorithms approach,genetic algorithms; reinforcement learning; hyperparameters; tuning,learning (artificial intelligence),Mathematical model,multi-agent systems,Optimization,parameters optimization,parameters tuning,pendulums,reinforcement learning algorithms,RL,Task analysis,Tools,Tuning,underactuated pendulum swing-up task}
}

@incollection{feurer_hyperparameter_2019,
  title = {Hyperparameter {{Optimization}}},
  booktitle = {Automated {{Machine Learning}}: {{Methods}}, {{Systems}}, {{Challenges}}},
  author = {Feurer, Matthias and Hutter, Frank},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2019},
  pages = {3--33},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-05318-5_1},
  abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/feurer_hutter_2019_hyperparameter optimization.pdf},
  isbn = {978-3-030-05318-5},
  language = {en},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}}
}

@inproceedings{feurer_initializing_2015,
  title = {Initializing {{Bayesian Hyperparameter Optimization}} via {{Meta}}-Learning},
  booktitle = {Proceedings of the {{Twenty}}-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
  year = {2015},
  pages = {1128--1135},
  publisher = {{AAAI Press}},
  address = {{Austin, Texas}},
  abstract = {Model selection and hyperparameter optimization is crucial in applying machine learning to a novel dataset. Recently, a sub-community of machine learning has focused on solving this problem with Sequential Model-based Bayesian Optimization (SMBO), demonstrating substantial successes in many applications. However, for computationally expensive algorithms the overhead of hyperparameter optimization can still be prohibitive. In this paper we mimic a strategy human domain experts use: speed up optimization by starting from promising configurations that performed well on similar datasets. The resulting initialization technique integrates naturally into the generic SMBO framework and can be trivially applied to any SMBO method. To validate our approach, we perform extensive experiments with two established SMBO frameworks (Spearmint and SMAC) with complementary strengths; optimizing two machine learning frameworks on 57 datasets. Our initialization procedure yields mild improvements for low-dimensional hyperparameter optimization and substantially improves the state of the art for the more complex combined algorithm selection and hyperparameter optimization problem.},
  isbn = {978-0-262-51129-2},
  series = {{{AAAI}}'15}
}

@article{finn_generalizing_2016,
  title = {Generalizing {{Skills}} with {{Semi}}-{{Supervised Reinforcement Learning}}},
  author = {Finn, Chelsea and Yu, Tianhe and Fu, Justin and Abbeel, Pieter and Levine, Sergey},
  year = {2016},
  month = dec,
  abstract = {Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while deployed. However, this learning requires access to a reward function, which is often hard to measure in real-world domains, where the reward could depend on, for example, unknown positions of objects or the emotional state of the user. Conversely, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present or in a controlled setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect on its own? In this paper, we formalize this problem as semisupervised reinforcement learning, where the reward function can only be evaluated in a set of "labeled" MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of "unlabeled" MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent's own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.},
  archivePrefix = {arXiv},
  eprint = {1612.00429},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/finn et al_2016_generalizing skills with semi-supervised reinforcement learning.pdf;/home/juan/Zotero/storage/X8GCTX3A/1612.html},
  journal = {arXiv:1612.00429 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryClass = {cs}
}

@misc{finn_learning_nodate,
  title = {Learning to {{Learn}}},
  author = {Finn, Chelsea},
  abstract = {The BAIR Blog},
  file = {/home/juan/Zotero/storage/WDCGA4H5/learning-to-learn.html},
  howpublished = {http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/},
  journal = {The Berkeley Artificial Intelligence Research Blog}
}

@article{finn_model-agnostic_2017,
  title = {Model-{{Agnostic Meta}}-{{Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = mar,
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  archivePrefix = {arXiv},
  eprint = {1703.03400},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/finn et al_2017_model-agnostic meta-learning for fast adaptation of deep networks.pdf;/home/juan/Zotero/storage/JDRUK87B/1703.html},
  journal = {arXiv:1703.03400 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{firoiu_beating_2017,
  title = {Beating the {{World}}'s {{Best}} at {{Super Smash Bros}}. with {{Deep Reinforcement Learning}}},
  author = {Firoiu, Vlad and Whitney, William F. and Tenenbaum, Joshua B.},
  year = {2017},
  month = feb,
  abstract = {There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting.},
  archivePrefix = {arXiv},
  eprint = {1702.06230},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/firoiu et al_2017_beating the world's best at super smash bros.pdf;/home/juan/Zotero/storage/VTH5NMRI/1702.html},
  journal = {arXiv:1702.06230 [cs]},
  keywords = {Computer Science - Artificial Intelligence,I.2.6},
  primaryClass = {cs}
}

@book{forsyth_computer_2011,
  title = {Computer {{Vision}}: {{A Modern Approach}}},
  shorttitle = {Computer {{Vision}}},
  author = {Forsyth, David A. and Ponce, Jean},
  year = {2011},
  month = nov,
  edition = {2 edition},
  publisher = {{Pearson}},
  address = {{Upper Saddle River}},
  abstract = {Computer Vision: A Modern Approach, 2e, is appropriate for upper-division undergraduate- and graduate-level courses in computer vision found in departments of Computer Science, Computer Engineering and Electrical Engineering.     This textbook provides the most complete treatment of modern computer vision methods by two of the leading authorities in the field. This accessible presentation gives both a general view of the entire computer vision enterprise and also offers sufficient detail for students to be able to build useful applications. Students will learn techniques that have proven to be useful by first-hand experience and a wide range of mathematical methods},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/forsyth_ponce_2011_computer vision.pdf},
  isbn = {978-0-13-608592-8},
  language = {English}
}

@article{freund_decision-theoretic_1997,
  title = {A {{Decision}}-{{Theoretic Generalization}} of {{On}}-{{Line Learning}} and an {{Application}} to {{Boosting}}},
  author = {Freund, Yoav and Schapire, Robert E},
  year = {1997},
  month = aug,
  volume = {55},
  pages = {119--139},
  issn = {0022-0000},
  doi = {10.1006/jcss.1997.1504},
  abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone\textendash{}Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/freund_schapire_1997_a decision-theoretic generalization of on-line learning and an application to.pdf;/home/juan/Zotero/storage/B5WCINHA/S002200009791504X.html},
  journal = {Journal of Computer and System Sciences},
  number = {1}
}

@article{fujimoto_addressing_2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor}}-{{Critic Methods}}},
  author = {Fujimoto, Scott and {van Hoof}, Herke and Meger, David},
  year = {2018},
  month = oct,
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archivePrefix = {arXiv},
  eprint = {1802.09477},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/WAL9PPUD/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf},
  journal = {arXiv:1802.09477 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{gao_graphnas_2019,
  title = {{{GraphNAS}}: {{Graph Neural Architecture Search}} with {{Reinforcement Learning}}},
  shorttitle = {{{GraphNAS}}},
  author = {Gao, Yang and Yang, Hong and Zhang, Peng and Zhou, Chuan and Hu, Yue},
  year = {2019},
  month = aug,
  abstract = {Graph Neural Networks (GNNs) have been popularly used for analyzing non-Euclidean data such as social network data and biological data. Despite their success, the design of graph neural networks requires a lot of manual work and domain knowledge. In this paper, we propose a Graph Neural Architecture Search method (GraphNAS for short) that enables automatic search of the best graph neural architecture based on reinforcement learning. Specifically, GraphNAS first uses a recurrent network to generate variable-length strings that describe the architectures of graph neural networks, and then trains the recurrent network with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation data set. Extensive experimental results on node classification tasks in both transductive and inductive learning settings demonstrate that GraphNAS can achieve consistently better performance on the Cora, Citeseer, Pubmed citation network, and protein-protein interaction network. On node classification tasks, GraphNAS can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy.},
  archivePrefix = {arXiv},
  eprint = {1904.09981},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/gao et al_2019_graphnas.pdf;/home/juan/Zotero/storage/CGDTCX97/Gao et al. - 2019 - GraphNAS Graph Neural Architecture Search with Re.pdf;/home/juan/Zotero/storage/3CPVBTR3/1904.html},
  journal = {arXiv:1904.09981 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{garnelo_conditional_2018,
  title = {Conditional {{Neural Processes}}},
  author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J. and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J. and Eslami, S. M. Ali},
  year = {2018},
  month = jul,
  abstract = {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion.},
  archivePrefix = {arXiv},
  eprint = {1807.01613},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/garnelo et al_2018_conditional neural processes.pdf;/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/garnelo et al_2018_conditional neural processes2.pdf;/home/juan/Zotero/storage/4ZIEUWKY/1807.html;/home/juan/Zotero/storage/XL679TML/1807.html},
  journal = {arXiv:1807.01613 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{garnelo_neural_2018,
  title = {Neural {{Processes}}},
  author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  year = {2018},
  month = jul,
  abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  archivePrefix = {arXiv},
  eprint = {1807.01622},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/garnelo et al_2018_neural processes.pdf;/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/garnelo et al_2018_neural processes2.pdf;/home/juan/Zotero/storage/2SNR6MR9/1807.html;/home/juan/Zotero/storage/3B9XCK9U/1807.html},
  journal = {arXiv:1807.01622 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{garnelo_towards_2016,
  title = {Towards {{Deep Symbolic Reinforcement Learning}}},
  author = {Garnelo, Marta and Arulkumaran, Kai and Shanahan, Murray},
  year = {2016},
  month = sep,
  abstract = {Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.},
  archivePrefix = {arXiv},
  eprint = {1609.05518},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/garnelo et al_2016_towards deep symbolic reinforcement learning.pdf;/home/juan/Zotero/storage/3QUGS9GH/1609.html},
  journal = {arXiv:1609.05518 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning},
  primaryClass = {cs}
}

@article{ghavamzadeh_bayesian_2015,
  title = {Bayesian {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Bayesian {{Reinforcement Learning}}},
  author = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
  year = {2015},
  volume = {8},
  pages = {359--483},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000049},
  abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
  archivePrefix = {arXiv},
  eprint = {1609.04436},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/ghavamzadeh et al_2015_bayesian reinforcement learning.pdf;/home/juan/Zotero/storage/8MQ8862S/1609.html},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  number = {5-6}
}

@article{ginsbourger_multi-points_2008,
  title = {A Multi-Points Criterion for Deterministic Parallel Global Optimization Based on {{Gaussian}} Processes},
  author = {Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent},
  year = {2008}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/goodfellow et al_2016_deep learning.pdf},
  note = {http://www.deeplearningbook.org}
}

@book{gordon_theories_2004,
  ids = {gordon\_theories\_2004},
  title = {Theories of Visual Perception},
  author = {Gordon, Ian E.},
  year = {2004},
  edition = {3. ed},
  publisher = {{Psychology Press}},
  address = {{Hove}},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/gordon_2004_theories of visual perception.pdf;/home/juan/Zotero/storage/5QYY8LYU/Gordon - 2004 - Theories of visual perception.pdf},
  isbn = {978-1-84169-383-5 978-1-84169-384-2},
  language = {en},
  note = {OCLC: 249489157}
}

@article{gosavi_variance-penalized_2014,
  title = {Variance-Penalized {{Markov}} Decision Processes: Dynamic Programming and Reinforcement Learning Techniques},
  shorttitle = {Variance-Penalized {{Markov}} Decision Processes},
  author = {Gosavi, Abhijit},
  year = {2014},
  month = aug,
  volume = {43},
  pages = {649--669},
  issn = {0308-1079, 1563-5104},
  doi = {10.1080/03081079.2014.883387},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/gosavi_2014_variance-penalized markov decision processes.pdf},
  journal = {International Journal of General Systems},
  language = {en},
  number = {6}
}

@article{graves_framewise_2005,
  title = {Framewise Phoneme Classification with Bidirectional {{LSTM}} and Other Neural Network Architectures},
  author = {Graves, Alex and Schmidhuber, J{\"u}rgen},
  year = {2005},
  month = jul,
  volume = {18},
  pages = {602--610},
  issn = {08936080},
  doi = {10.1016/j.neunet.2005.06.042},
  abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and timewindowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.1 q 2005 Elsevier Ltd. All rights reserved.},
  file = {/home/juan/Zotero/storage/UFCREZ2Q/Graves and Schmidhuber - 2005 - Framewise phoneme classification with bidirectiona.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {5-6}
}

@article{greff_lstm_2017,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  shorttitle = {{{LSTM}}},
  author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutn{\'i}k, Jan and Steunebrink, Bas R. and Schmidhuber, J{\"u}rgen},
  year = {2017},
  month = oct,
  volume = {28},
  pages = {2222--2232},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2016.2582924},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({$\approx$} 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  archivePrefix = {arXiv},
  eprint = {1503.04069},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/greff et al_2017_lstm.pdf},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords = {68T10,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,H.5.5,I.2.6,I.2.7,I.5.1},
  language = {en},
  number = {10}
}

@article{guo_deep_2016,
  title = {Deep Learning for Visual Understanding: {{A}} Review},
  shorttitle = {Deep Learning for Visual Understanding},
  author = {Guo, Yanming and Liu, Yu and Oerlemans, Ard and Lao, Songyang and Wu, Song and Lew, Michael S.},
  year = {2016},
  month = apr,
  volume = {187},
  pages = {27--48},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2015.09.116},
  abstract = {Deep learning algorithms are a subset of the machine learning algorithms, which aim at discovering multiple levels of distributed representations. Recently, numerous deep learning algorithms have been proposed to solve traditional artificial intelligence problems. This work aims to review the state-of-the-art in deep learning algorithms in computer vision by highlighting the contributions and challenges from over 210 recent research papers. It first gives an overview of various deep learning approaches and their recent developments, and then briefly describes their applications in diverse vision tasks, such as image classification, object detection, image retrieval, semantic segmentation and human pose estimation. Finally, the paper summarizes the future trends and challenges in designing and training deep neural networks.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/guo et al_2016_deep learning for visual understanding.pdf;/home/juan/Zotero/storage/FRFQSKNV/S0925231215017634.html},
  journal = {Neurocomputing},
  keywords = {Applications,Challenges,Computer vision,Deep learning,Developments,Trends},
  number = {Supplement C},
  series = {Recent {{Developments}} on {{Deep Big Vision}}}
}

@article{haarnoja_soft_2018,
  title = {Soft {{Actor}}-{{Critic}}: {{Off}}-{{Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor}}-{{Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = aug,
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archivePrefix = {arXiv},
  eprint = {1801.01290},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/haarnoja et al_2018_soft actor-critic.pdf;/home/juan/Zotero/storage/M3GIR9PE/1801.html},
  journal = {arXiv:1801.01290 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{hansen_cma_2016,
  title = {The {{CMA Evolution Strategy}}: {{A Tutorial}}},
  shorttitle = {The {{CMA Evolution Strategy}}},
  author = {Hansen, Nikolaus},
  year = {2016},
  month = apr,
  abstract = {This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain.},
  archivePrefix = {arXiv},
  eprint = {1604.00772},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/hansen_2016_the cma evolution strategy.pdf;/home/juan/Zotero/storage/JEDXAY7E/1604.html},
  journal = {arXiv:1604.00772 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{hasselt_double_2010,
  title = {Double {{Q}}-Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}} 23},
  author = {Hasselt, Hado V.},
  editor = {Lafferty, J. D. and Williams, C. K. I. and {Shawe-Taylor}, J. and Zemel, R. S. and Culotta, A.},
  year = {2010},
  pages = {2613--2621},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/hasselt_2010_double q-learning.pdf;/home/juan/Zotero/storage/ZTC8PU9X/3964-double-q-learning.html}
}

@article{hazan_hyperparameter_2017,
  title = {Hyperparameter {{Optimization}}: {{A Spectral Approach}}},
  shorttitle = {Hyperparameter {{Optimization}}},
  author = {Hazan, Elad and Klivans, Adam and Yuan, Yang},
  year = {2017},
  month = jun,
  abstract = {We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions. We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm - an iterative application of compressed sensing techniques for orthogonal polynomials - requires only uniform sampling of the hyperparameters and is thus easily parallelizable. Experiments for training deep nets on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases matching what is attainable by hand-tuning. In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and even more so compared to Bayesian Optimization. We also outperform Random Search 5X. Additionally, our method comes with provable guarantees and yields the first quasi-polynomial time algorithm for learning decision trees under the uniform distribution with polynomial sample complexity, the first improvement in over two decades.},
  archivePrefix = {arXiv},
  eprint = {1706.00764},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/hazan et al_2017_hyperparameter optimization.pdf;/home/juan/Zotero/storage/CXJUW8FJ/1706.html},
  journal = {arXiv:1706.00764 [cs, math]},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control},
  primaryClass = {cs, math}
}

@article{he_automl_2019,
  title = {{{AutoML}}: {{A Survey}} of the {{State}}-of-the-{{Art}}},
  shorttitle = {{{AutoML}}},
  author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  year = {2019},
  month = aug,
  abstract = {Deep learning has penetrated all aspects of our lives and brought us great convenience. However, the process of building a high-quality deep learning system for a specific task is not only time-consuming but also requires lots of resources and relies on human expertise, which hinders the development of deep learning in both industry and academia. To alleviate this problem, a growing number of research projects focus on automated machine learning (AutoML). In this paper, we provide a comprehensive and up-to-date study on the state-of-the-art AutoML. First, we introduce the AutoML techniques in details according to the machine learning pipeline. Then we summarize existing Neural Architecture Search (NAS) research, which is one of the most popular topics in AutoML. We also compare the models generated by NAS algorithms with those human-designed models. Finally, we present several open problems for future research.},
  archivePrefix = {arXiv},
  eprint = {1908.00709},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/he et al_2019_automl.pdf;/home/juan/Zotero/storage/EM54934U/1908.html},
  journal = {arXiv:1908.00709 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{henderson_deep_2017,
  title = {Deep {{Reinforcement Learning}} That {{Matters}}},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  year = {2017},
  month = sep,
  abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  archivePrefix = {arXiv},
  eprint = {1709.06560},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/henderson et al_2017_deep reinforcement learning that matters.pdf;/home/juan/Zotero/storage/EVVZESBU/1709.html},
  journal = {arXiv:1709.06560 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{henderson_where_2018,
  title = {Where {{Did My Optimum Go}}?: {{An Empirical Analysis}} of {{Gradient Descent Optimization}} in {{Policy Gradient Methods}}},
  shorttitle = {Where {{Did My Optimum Go}}?},
  author = {Henderson, Peter and Romoff, Joshua and Pineau, Joelle},
  year = {2018},
  month = oct,
  abstract = {Recent analyses of certain gradient descent optimization methods have shown that performance can degrade in some settings \textendash{} such as with stochasticity or implicit momentum. In deep reinforcement learning (Deep RL), such optimization methods are often used for training neural networks via the temporal difference error or policy gradient. As an agent improves over time, the optimization target changes and thus the loss landscape (and local optima) change. Due to the failure modes of those methods, the ideal choice of optimizer for Deep RL remains unclear. As such, we provide an empirical analysis of the effects that a wide range of gradient descent optimizers and their hyperparameters have on policy gradient methods, a subset of Deep RL algorithms, for benchmark continuous control tasks. We find that adaptive optimizers have a narrow window of effective learning rates, diverging in other cases, and that the effectiveness of momentum varies depending on the properties of the environment. Our analysis suggests that there is significant interplay between the dynamics of the environment and Deep RL algorithm properties which aren't necessarily accounted for by traditional adaptive gradient methods. We provide suggestions for optimal settings of current methods and further lines of research based on our findings.},
  archivePrefix = {arXiv},
  eprint = {1810.02525},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/henderson et al_2018_where did my optimum go.pdf;/home/juan/Zotero/storage/FASC5SQX/Henderson et al. - 2018 - Where Did My Optimum Go An Empirical Analysis of.pdf},
  journal = {arXiv:1810.02525 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hinton_fast_2006,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, G. E. and Osindero, S. and Teh, Y. W.},
  year = {2006},
  month = jul,
  volume = {18},
  pages = {1527--1554},
  issn = {0899-7667},
  doi = {10.1162/neco.2006.18.7.1527},
  abstract = {We show how to use ``complementary priors'' to eliminate the explaining-away effects thatmake inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of thewake-sleep algorithm. After fine-tuning, a networkwith three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to displaywhat the associativememory has in mind.},
  file = {/home/juan/Zotero/storage/7MSNH3J9/6796673.html},
  journal = {Neural Computation},
  number = {7}
}

@incollection{hinton_recognize_2007,
  title = {To Recognize Shapes, First Learn to Generate Images},
  booktitle = {Progress in {{Brain Research}}},
  author = {Hinton, Geoffrey E.},
  editor = {Paul Cisek, Trevor Drew {and} John F. Kalaska},
  year = {2007},
  volume = {165},
  pages = {535--547},
  publisher = {{Elsevier}},
  doi = {10.1016/S0079-6123(06)65034-6},
  abstract = {The uniformity of the cortical architecture and the ability of functions to move to different areas of cortex following early damage strongly suggest that there is a single basic learning algorithm for extracting underlying structure from richly structured, high-dimensional sensory data. There have been many attempts to design such an algorithm, but until recently they all suffered from serious computational weaknesses. This chapter describes several of the proposed algorithms and shows how they can be combined to produce hybrid methods that work efficiently in networks with many layers and millions of adaptive connections.},
  file = {/home/juan/Zotero/storage/IPCRZNMD/S0079612306650346.html},
  keywords = {Boltzmann machines,contrastive divergence,feature discovery,generative models,learning algorithms,multilayer neural networks,shape recognition,unsupervised learning,wake–sleep algorithm},
  series = {Computational {{Neuroscience}}: {{Theoretical Insights}} into {{Brain Function}}}
}

@article{hochreiter_long_1997,
  title = {Long {{Short}}-{{Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  volume = {9},
  pages = {1735--1780},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1735},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/hochreiter_schmidhuber_1997_long short-term memory.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {8}
}

@article{horgan_distributed_2018,
  title = {Distributed {{Prioritized Experience Replay}}},
  author = {Horgan, Dan and Quan, John and Budden, David and {Barth-Maron}, Gabriel and Hessel, Matteo and {van Hasselt}, Hado and Silver, David},
  year = {2018},
  month = mar,
  abstract = {We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.},
  archivePrefix = {arXiv},
  eprint = {1803.00933},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/horgan et al_2018_distributed prioritized experience replay.pdf;/home/juan/Zotero/storage/CNNESEPV/1803.html},
  journal = {arXiv:1803.00933 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{horgan_distributed_2018-1,
  title = {Distributed {{Prioritized Experience Replay}}},
  author = {Horgan, Dan and Quan, John and Budden, David and {Barth-Maron}, Gabriel and Hessel, Matteo and {van Hasselt}, Hado and Silver, David},
  year = {2018},
  month = mar,
  abstract = {We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.},
  archivePrefix = {arXiv},
  eprint = {1803.00933},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/IVP7DLB4/Horgan et al. - 2018 - Distributed Prioritized Experience Replay.pdf},
  journal = {arXiv:1803.00933 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{huang_modified_2012,
  title = {Modified Genetic Algorithms for Solving Fuzzy Flow Shop Scheduling Problems and Their Implementation with {{CUDA}}},
  author = {Huang, Chieh-Sen and Huang, Yi-Chen and Lai, Peng-Jen},
  year = {2012},
  month = apr,
  volume = {39},
  pages = {4999--5005},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2011.10.013},
  abstract = {In this paper we propose an improved algorithm to search optimal solutions to the flow shop scheduling problems with fuzzy processing times and fuzzy due dates. A longest common substring method is proposed to combine with the random key method. Numerical simulation shows that longest common substring method combined with rearranging mating method improves the search efficiency of genetic algorithm in this problem. For application in large-sized problems, we also enhance this modified algorithm by CUDA based parallel computation. Numerical experiments show that the performances of the CUDA program on GPU compare favorably to the traditional programs on CPU. Based on the modified algorithm invoking with CUDA scheme, we can search satisfied solutions to the fuzzy flow shop scheduling problems with high performance.},
  file = {/home/juan/Zotero/storage/V7X8DI7X/S0957417411014989.html},
  journal = {Expert Systems with Applications},
  keywords = {CUDA,Flow shop scheduling problem,Fuzzy numbers,Genetic algorithm,Random key},
  number = {5}
}

@book{hutter_automated_2018,
  title = {Automated {{Machine Learning}}: {{Methods}}, {{Systems}}, {{Challenges}}},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2018},
  publisher = {{Springer}},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/hutter et al_2018_automated machine learning.pdf},
  note = {In press, available at http://automl.org/book.}
}

@article{hutter_beyond_2015,
  title = {Beyond {{Manual Tuning}} of {{Hyperparameters}}},
  author = {Hutter, Frank and L{\"u}cke, J{\"o}rg and {Schmidt-Thieme}, Lars},
  year = {2015},
  month = nov,
  volume = {29},
  pages = {329--337},
  issn = {0933-1875, 1610-1987},
  doi = {10.1007/s13218-015-0381-0},
  abstract = {The success of hand-crafted machine learning systems in many applications raises the question of making machine learning algorithms more autonomous, i.e., to reduce the requirement of expert input to a minimum. We discuss two strategies towards this goal: (1) automated optimization of hyperparameters (including mechanisms for feature selection, preprocessing, model selection, etc) and (2) the development of algorithms with reduced sets of hyperparameters. Since many research directions (e.g., deep learning), show a tendency towards increasingly complex algorithms with more and more hyperparamters, the demand for both of these strategies continuously increases. We review recent hyperparameter optimization methods and discuss data-driven approaches to avoid the introduction of hyperparameters using unsupervised learning. We end in discussing how these complementary strategies can work hand-in-hand, representing a very promising approach towards autonomous machine learning.},
  file = {/home/juan/Zotero/storage/8EMD7MPI/s13218-015-0381-0.html},
  journal = {KI - K{\"u}nstliche Intelligenz},
  language = {en},
  number = {4}
}

@article{hutter_paramils_2009,
  title = {{{ParamILS}}: {{An Automatic Algorithm Configuration Framework}}},
  shorttitle = {{{ParamILS}}},
  author = {Hutter, Frank and Hoos, Holger H and {Leyton-Brown}, Kevin and St{\"u}tzle, Thomas},
  year = {2009},
  volume = {36},
  pages = {267--306},
  issn = {1943-5037},
  doi = {10.1613/jair.2861},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/hutter et al_2009_paramils.pdf;/home/juan/Zotero/storage/3PHBQIR7/220543148_ParamILS_An_Automatic_Algorithm_Configuration_Framework.html},
  journal = {Journal of Artificial Intelligence Research (JAIR)}
}

@inproceedings{hutter_sequential_2011,
  title = {Sequential {{Model}}-{{Based Optimization}} for {{General Algorithm Configuration}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2011},
  month = jan,
  pages = {507--523},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-25566-3_40},
  abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/hutter et al_2011_sequential model-based optimization for general algorithm configuration.pdf;/home/juan/Zotero/storage/Y3GDG77F/978-3-642-25566-3_40.html},
  isbn = {978-3-642-25565-6 978-3-642-25566-3},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{hutter_universal_2005-1,
  title = {Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability},
  shorttitle = {Universal Artificial Intelligence},
  author = {Hutter, Marcus},
  year = {2005},
  publisher = {{Springer}},
  address = {{Berlin}},
  abstract = {This book presents sequential decision theory from a novel algorithmic information theory perspective. While the former is suited for active agents in known environment, the latter is suited for passive prediction in unknown environment. The book introduces these two different ideas and removes the limitations by unifying them to one parameter-free theory of an optimal reinforcement learning agent embedded in an unknown environment. Most AI problems can easily be formulated within this theory, reducing the conceptual problems to pure computational ones. Considered problem classes include sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations to other approaches. One intention of this book is to excite a broader AI audience about abstract algorithmic information theory concepts, and conversely to inform theorists about exciting applications to AI},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/hutter_2005_universal artificial intelligence.pdf;/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/hutter_2005_universal artificial intelligence2.pdf},
  isbn = {978-3-540-22139-5},
  language = {eng},
  note = {OCLC: 249769053},
  series = {Texts in Theoretical Computer Science}
}

@article{islam_reproducibility_2017,
  title = {Reproducibility of {{Benchmarked Deep Reinforcement Learning Tasks}} for {{Continuous Control}}},
  author = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
  year = {2017},
  month = aug,
  abstract = {Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.},
  archivePrefix = {arXiv},
  eprint = {1708.04133},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/islam et al_2017_reproducibility of benchmarked deep reinforcement learning tasks for continuous.pdf;/home/juan/Zotero/storage/2CT222K9/1708.html},
  journal = {arXiv:1708.04133 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{jaderberg_human-level_2019,
  title = {Human-Level Performance in {{3D}} Multiplayer Games with Population-Based Reinforcement Learning},
  author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Casta{\~n}eda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
  year = {2019},
  month = may,
  volume = {364},
  pages = {859--865},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aau6249},
  abstract = {Artificial teamwork
Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans.
Science, this issue p. 859
Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.
Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode.
Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode.},
  copyright = {Copyright \textcopyright{} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  file = {/home/juan/Zotero/storage/8N33BLV2/859.html},
  journal = {Science},
  language = {en},
  number = {6443},
  pmid = {31147514}
}

@article{jaderberg_population_2017,
  title = {Population {{Based Training}} of {{Neural Networks}}},
  author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year = {2017},
  month = nov,
  abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present \textbackslash{}emph\{Population Based Training (PBT)\}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
  archivePrefix = {arXiv},
  eprint = {1711.09846},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/jaderberg et al_2017_population based training of neural networks.pdf;/home/juan/Zotero/storage/EZGB5XSZ/1711.html},
  journal = {arXiv:1711.09846 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@book{james_introduction_2013,
  title = {An {{Introduction}} to {{Statistical Learning}}: With {{Applications}} in {{R}}},
  shorttitle = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  month = aug,
  edition = {1st ed. 2013, Corr. 5th printing 2015 edition},
  publisher = {{Springer}},
  abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/james et al_2013_an introduction to statistical learning.pdf},
  isbn = {978-1-4614-7137-0},
  language = {English}
}

@inproceedings{jauch_bayesian_2016,
  title = {Bayesian {{Optimization}} with {{Shape Constraints}}},
  booktitle = {{{arXiv}}:1612.08915 [Stat]},
  author = {Jauch, Michael and Pe{\~n}a, V{\'i}ctor},
  year = {2016},
  month = dec,
  abstract = {In typical applications of Bayesian optimization, minimal assumptions are made about the objective function being optimized. This is true even when researchers have prior information about the shape of the function with respect to one or more argument. We make the case that shape constraints are often appropriate in at least two important application areas of Bayesian optimization: (1) hyperparameter tuning of machine learning algorithms and (2) decision analysis with utility functions. We describe a methodology for incorporating a variety of shape constraints within the usual Bayesian optimization framework and present positive results from simple applications which suggest that Bayesian optimization with shape constraints is a promising topic for further research.},
  archivePrefix = {arXiv},
  eprint = {1612.08915},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/jauch_peña_2016_bayesian optimization with shape constraints.pdf;/home/juan/Zotero/storage/K8F4AHAX/1612.html},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{jin_efficient_2018,
  title = {Efficient {{Neural Architecture Search}} with {{Network Morphism}}},
  author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
  year = {2018},
  month = jun,
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/jin et al_2018_efficient neural architecture search with network morphism.pdf;/home/juan/Zotero/storage/CPAE3Z7A/1806.html},
  language = {en}
}

@book{johnson_theoreticians_2001,
  title = {A {{Theoretician}}'s {{Guide}} to the {{Experimental Analysis}} of {{Algorithms}}},
  author = {Johnson, David},
  year = {2001},
  abstract = {This paper presents an informal discussion of issues that arise when one attempts to analyze algorithms experimentally. It is based on lessons learned by the author over the course of more than a decade of experimentation, survey paper writing, refereeing, and lively discussions with other experimentalists. Although written from the perspective of a theoretical computer scientist, it is intended to be of use to researchers from all fields who want to study algorithms experimentally. It has two goals: first, to provide a useful guide to new experimentalists about how such work can best be performed and written up, and second, to challenge current researchers to think about whether their own work might be improved from a scientific point of view. With the latter purpose in mind, the author hopes that at least a few of his recommendations will be considered controversial.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/johnson_2001_a theoretician's guide to the experimental analysis of algorithms.pdf;/home/juan/Zotero/storage/55STSSSJ/summary.html}
}

@article{jones_efficient_1998,
  title = {Efficient {{Global Optimization}} of {{Expensive Black}}-{{Box Functions}}},
  author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  year = {1998},
  month = dec,
  volume = {13},
  pages = {455--492},
  issn = {0925-5001, 1573-2916},
  doi = {10.1023/A:1008306431147},
  abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/jones et al_1998_efficient global optimization of expensive black-box functions.pdf;/home/juan/Zotero/storage/ADGPGQ5K/10.html},
  journal = {Journal of Global Optimization},
  keywords = {Bayesian global optimization,Computer Science; general,Kriging,Operation Research/Decision Theory,Optimization,Random function,Real Functions,Response surface,Stochastic process,Visualization},
  language = {en},
  number = {4}
}

@article{jones_lipschitzian_1993,
  title = {Lipschitzian Optimization without the {{Lipschitz}} Constant},
  author = {Jones, D. R. and Perttunen, C. D. and Stuckman, B. E.},
  year = {1993},
  month = oct,
  volume = {79},
  pages = {157--181},
  issn = {0022-3239, 1573-2878},
  doi = {10.1007/BF00941892},
  abstract = {We present a new algorithm for finding the global minimum of a multivariate function subject to simple bounds. The algorithm is a modification of the standard Lipschitzian approach that eliminates the need to specify a Lipschitz constant. This is done by carrying out simultaneous searches using all possible constants from zero to infinity. On nine standard test functions, the new algorithm converges in fewer function evaluations than most competing methods. The motivation for the new algorithm stems from a different way of looking at the Lipschitz constant. In particular, the Lipschitz constant is viewed as a weighting parameter that indicates how much emphasis to place on global versus local search. In standard Lipschitzian methods, this constant is usually large because it must equal or exceed the maximum rate of change of the objective function. As a result, these methods place a high emphasis on global search and exhibit slow convergence. In contrast, the new algorithm carries out simultaneous searches using all possible constants, and therefore operates at both the global and local level. Once the global part of the algorithm finds the basin of convergence of the optimum, the local part of the algorithm quickly and automatically exploits it. This accounts for the fast convergence of the new algorithm on the test functions.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/jones et al_1993_lipschitzian optimization without the lipschitz constant.pdf;/home/juan/Zotero/storage/2S43ZWC3/BF00941892.html},
  journal = {Journal of Optimization Theory and Applications},
  keywords = {Applications of Mathematics,Calculus of Variations and Optimal Control,Engineering; general,Global optimization,Lipschitzian optimization,Operation Research/Decision Theory,Optimization,space covering,space partitioning,Theory of Computation},
  language = {en},
  number = {1}
}

@article{kaelbling_reinforcement_1996,
  title = {Reinforcement {{Learning}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning}}},
  author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
  year = {1996},
  month = apr,
  abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {cs/9605103},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/kaelbling et al_1996_reinforcement learning.pdf;/home/juan/Zotero/storage/BCJ5392A/9605103.html},
  journal = {arXiv:cs/9605103},
  keywords = {Computer Science - Artificial Intelligence}
}

@article{kaiser_model-based_2019,
  title = {Model-{{Based Reinforcement Learning}} for {{Atari}}},
  author = {Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H. and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
  year = {2019},
  month = jun,
  abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of \$100\$K interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.},
  archivePrefix = {arXiv},
  eprint = {1903.00374},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/kaiser et al_2019_model-based reinforcement learning for atari.pdf;/home/juan/Zotero/storage/82JUGG6N/1903.html},
  journal = {arXiv:1903.00374 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kandasamy_neural_2018,
  title = {Neural {{Architecture Search}} with {{Bayesian Optimisation}} and {{Optimal Transport}}},
  author = {Kandasamy, Kirthevasan and Neiswanger, Willie and Schneider, Jeff and Poczos, Barnabas and Xing, Eric},
  year = {2018},
  month = feb,
  abstract = {Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function \$f\$ which is only accessible via point evaluations. It is typically used in settings where \$f\$ is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network \textbackslash{}emph\{architectures\}. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.},
  archivePrefix = {arXiv},
  eprint = {1802.07191},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/kandasamy et al_2018_neural architecture search with bayesian optimisation and optimal transport.pdf;/home/juan/Zotero/storage/NU5GWVIQ/1802.html},
  journal = {arXiv:1802.07191 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kapturowski_recurrent_2019,
  title = {{{RECURRENT EXPERIENCE REPLAY IN DISTRIBUTED REINFORCEMENT LEARNING}}},
  author = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  year = {2019},
  pages = {19},
  abstract = {Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyperparameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the first agent to exceed human-level performance in 52 of the 57 Atari games.},
  file = {/home/juan/Zotero/storage/GGPKVLSN/Kapturowski et al. - 2019 - RECURRENT EXPERIENCE REPLAY IN DISTRIBUTED REINFOR.pdf},
  language = {en}
}

@article{kingma_adam_2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/kingma_ba_2017_adam.pdf;/home/juan/Zotero/storage/X33LH552/1412.html},
  journal = {arXiv:1412.6980 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{kingma_auto-encoding_2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/8IU8IEI4/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf},
  journal = {arXiv:1312.6114 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kuleshov_algorithms_2014,
  title = {Algorithms for Multi-Armed Bandit Problems},
  author = {Kuleshov, Volodymyr and Precup, Doina},
  year = {2014},
  month = feb,
  abstract = {Although many algorithms for the multi-armed bandit problem are well-understood theoretically, empirical confirmation of their effectiveness is generally scarce. This paper presents a thorough empirical study of the most popular multi-armed bandit algorithms. Three important observations can be made from our results. Firstly, simple heuristics such as epsilon-greedy and Boltzmann exploration outperform theoretically sound algorithms on most settings by a significant margin. Secondly, the performance of most algorithms varies dramatically with the parameters of the bandit problem. Our study identifies for each algorithm the settings where it performs well, and the settings where it performs poorly. Thirdly, the algorithms' performance relative each to other is affected only by the number of bandit arms and the variance of the rewards. This finding may guide the design of subsequent empirical evaluations. In the second part of the paper, we turn our attention to an important area of application of bandit algorithms: clinical trials. Although the design of clinical trials has been one of the principal practical problems motivating research on multi-armed bandits, bandit algorithms have never been evaluated as potential treatment allocation strategies. Using data from a real study, we simulate the outcome that a 2001-2002 clinical trial would have had if bandit algorithms had been used to allocate patients to treatments. We find that an adaptive trial would have successfully treated at least 50\% more patients, while significantly reducing the number of adverse effects and increasing patient retention. At the end of the trial, the best treatment could have still been identified with a high level of statistical confidence. Our findings demonstrate that bandit algorithms are attractive alternatives to current adaptive treatment allocation strategies.},
  archivePrefix = {arXiv},
  eprint = {1402.6028},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/kuleshov_precup_2014_algorithms for multi-armed bandit problems.pdf;/home/juan/Zotero/storage/5XUX29SI/1402.html},
  journal = {arXiv:1402.6028 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning},
  primaryClass = {cs}
}

@article{kurach_google_2019,
  title = {Google {{Research Football}}: {{A Novel Reinforcement Learning Environment}}},
  shorttitle = {Google {{Research Football}}},
  author = {Kurach, Karol and Raichuk, Anton and Sta{\'n}czyk, Piotr and Zaj{\k{a}}c, Micha{\l} and Bachem, Olivier and Espeholt, Lasse and Riquelme, Carlos and Vincent, Damien and Michalski, Marcin and Bousquet, Olivier and Gelly, Sylvain},
  year = {2019},
  month = jul,
  abstract = {Recent progress in the field of reinforcement learning has been accelerated by virtual learning environments such as video games, where novel algorithms and ideas can be quickly tested in a safe and reproducible manner. We introduce the Google Research Football Environment, a new reinforcement learning environment where agents are trained to play football in an advanced, physics-based 3D simulator. The resulting environment is challenging, easy to use and customize, and it is available under a permissive open-source license. In addition, it provides support for multiplayer and multi-agent experiments. We propose three full-game scenarios of varying difficulty with the Football Benchmarks and report baseline results for three commonly used reinforcement algorithms (IMPALA, PPO, and Ape-X DQN). We also provide a diverse set of simpler scenarios with the Football Academy and showcase several promising research directions.},
  archivePrefix = {arXiv},
  eprint = {1907.11180},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/kurach et al_2019_google research football.pdf;/home/juan/Zotero/storage/FDUQESJJ/1907.html},
  journal = {arXiv:1907.11180 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kuss_gaussian_nodate,
  title = {Gaussian {{Processes}} in {{Reinforcement Learning}}},
  author = {Kuss, Malte and Rasmussen, Carl E},
  pages = {8},
  abstract = {We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/kuss_rasmussen_gaussian processes in reinforcement learning.pdf},
  language = {en}
}

@book{laird_soar_2012,
  ids = {laird\_soar\_2012-1},
  title = {The {{Soar}} Cognitive Architecture},
  author = {Laird, John},
  year = {2012},
  publisher = {{MIT Press}},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/laird_2012_the soar cognitive architecture.pdf},
  isbn = {978-0-262-12296-2},
  keywords = {Artificial intelligence,Software architecture},
  lccn = {Q335 .L329 2012}
}

@article{lake_human-level_2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
  year = {2015},
  month = dec,
  volume = {350},
  pages = {1332--1338},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab3050},
  abstract = {Handwritten characters drawn by a model
Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look ``right'' as judged by Turing-like tests of the model's output in comparison to what real humans produce.
Science, this issue p. 1332
People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms\textemdash{}for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several ``visual Turing tests'' probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.
Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model.
Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model.},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  file = {/home/juan/Zotero/storage/P5ANAVUW/1332.html},
  journal = {Science},
  language = {en},
  number = {6266},
  pmid = {26659050}
}

@book{lapan_deep_2018,
  ids = {lapan\_deep\_2018},
  title = {Deep Reinforcement Learning Hands-on: Apply Modern {{RL}} Methods, with Deep {{Q}}-Networks, Value Iteration, Policy Gradients, {{TRPO}},{{AlphaGo Zero}} and More},
  shorttitle = {Deep Reinforcement Learning Hands-On},
  author = {Lapan, Maxim},
  year = {2018},
  publisher = {{Packt Publishing}},
  address = {{Birmingham Mumbai}},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/lapan_2018_deep reinforcement learning hands-on.pdf},
  isbn = {978-1-78883-424-7},
  language = {en},
  series = {Expert Insight}
}

@inproceedings{laroche_reinforcement_2017,
  title = {Reinforcement {{Learning Algorithm Selection}}},
  booktitle = {{{arXiv}}:1701.08810 [Cs, Math, Stat]},
  author = {Laroche, Romain and Feraud, Raphael},
  year = {2017},
  month = jan,
  abstract = {This paper formalises the problem of online algorithm selection in the context of Reinforcement Learning. The setup is as follows: given an episodic task and a finite number of off-policy RL algorithms, a meta-algorithm has to decide which RL algorithm is in control during the next episode so as to maximize the expected return. The article presents a novel meta-algorithm, called Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is to freeze the policy updates at each epoch, and to leave a rebooted stochastic bandit in charge of the algorithm selection. Under some assumptions, a thorough theoretical analysis demonstrates its near-optimality considering the structural sampling budget limitations. ESBAS is first empirically evaluated on a dialogue task where it is shown to outperform each individual algorithm in most configurations. ESBAS is then adapted to a true online setting where algorithms update their policies after each transition, which we call SSBAS. SSBAS is evaluated on a fruit collection task where it is shown to adapt the stepsize parameter more efficiently than the classical hyperbolic decay, and on an Atari game, where it improves the performance by a wide margin.},
  archivePrefix = {arXiv},
  eprint = {1701.08810},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/laroche_feraud_2017_reinforcement learning algorithm selection.pdf;/home/juan/Zotero/storage/EX5W59VI/1701.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{lemke_metalearning_2015,
  title = {Metalearning: A Survey of Trends and Technologies},
  shorttitle = {Metalearning},
  author = {Lemke, Christiane and Budka, Marcin and Gabrys, Bogdan},
  year = {2015},
  month = jun,
  volume = {44},
  pages = {117--130},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-013-9406-y},
  abstract = {Metalearning attracted considerable interest in the machine learning community in the last years. Yet, some disagreement remains on what does or what does not constitute a metalearning problem and in which contexts the term is used in. This survey aims at giving an all-encompassing overview of the research directions pursued under the umbrella of metalearning, reconciling different definitions given in scientific literature, listing the choices involved when designing a metalearning system and identifying some of the future research challenges in this domain.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/lemke et al_2015_metalearning.pdf;/home/juan/Zotero/storage/APXQEKJX/s10462-013-9406-y.html},
  journal = {Artificial Intelligence Review},
  language = {en},
  number = {1}
}

@article{li_deep_2016,
  title = {Deep {{Reinforcement Learning}} for {{Dialogue Generation}}},
  author = {Li, Jiwei and Monroe, Will and Ritter, Alan and Galley, Michel and Gao, Jianfeng and Jurafsky, Dan},
  year = {2016},
  month = jun,
  abstract = {Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.},
  archivePrefix = {arXiv},
  eprint = {1606.01541},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/VBCGHITB/Li et al. - 2016 - Deep Reinforcement Learning for Dialogue Generatio.pdf;/home/juan/Zotero/storage/NK8BF625/1606.html},
  journal = {arXiv:1606.01541 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{li_deep_2017,
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  author = {Li, Yuxi},
  year = {2017},
  month = jan,
  abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
  archivePrefix = {arXiv},
  eprint = {1701.07274},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/li_2017_deep reinforcement learning.pdf;/home/juan/Zotero/storage/Z9RPTXAT/1701.html},
  journal = {arXiv:1701.07274 [cs]},
  keywords = {Computer Science - Learning},
  primaryClass = {cs}
}

@article{li_hyperband_2016,
  title = {Hyperband: {{A Novel Bandit}}-{{Based Approach}} to {{Hyperparameter Optimization}}},
  shorttitle = {Hyperband},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2016},
  month = mar,
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with state-of-the-art methods on a suite of hyperparameter optimization problems. We observe that Hyperband provides five times to thirty times speedup over state-of-the-art Bayesian optimization algorithms on a variety of deep-learning and kernel-based learning problems.},
  archivePrefix = {arXiv},
  eprint = {1603.06560},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/li et al_2016_hyperband.pdf;/home/juan/Zotero/storage/5UZSWBBD/1603.html},
  journal = {arXiv:1603.06560 [cs, stat]},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{li_random_2019,
  title = {Random {{Search}} and {{Reproducibility}} for {{Neural Architecture Search}}},
  author = {Li, Liam and Talwalkar, Ameet},
  year = {2019},
  month = feb,
  abstract = {Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In this work, in order to help ground the empirical results in this field, we propose new NAS baselines that build off the following observations: (i) NAS is a specialized hyperparameter optimization problem; and (ii) random search is a competitive baseline for hyperparameter optimization. Leveraging these observations, we evaluate both random search with early-stopping and a novel random search with weight-sharing algorithm on two standard NAS benchmarks\textemdash{}PTB and CIFAR-10. Our results show that random search with early-stopping is a competitive NAS baseline, e.g., it performs at least as well as ENAS [41], a leading NAS method, on both benchmarks. Additionally, random search with weight-sharing outperforms random search with early-stopping, achieving a state-of-the-art NAS result on PTB and a highly competitive result on CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS results. We note the lack of source material needed to exactly reproduce these results, and further discuss the robustness of published results given the various sources of variability in NAS experimental setups. Relatedly, we provide all information (code, random seeds, documentation) needed to exactly reproduce our results, and report our random search with weight-sharing results for each benchmark on multiple runs.},
  archivePrefix = {arXiv},
  eprint = {1902.07638},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/LKZJJ2RC/Li and Talwalkar - 2019 - Random Search and Reproducibility for Neural Archi.pdf},
  journal = {arXiv:1902.07638 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{liang_rllib_2017,
  title = {{{RLlib}}: {{Abstractions}} for {{Distributed Reinforcement Learning}}},
  shorttitle = {{{RLlib}}},
  author = {Liang, Eric and Liaw, Richard and Moritz, Philipp and Nishihara, Robert and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph E. and Jordan, Michael I. and Stoica, Ion},
  year = {2017},
  month = dec,
  abstract = {Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available at https://rllib.io/.},
  archivePrefix = {arXiv},
  eprint = {1712.09381},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/liang et al_2017_rllib.pdf;/home/juan/Zotero/storage/6SKD75IB/1712.html},
  journal = {arXiv:1712.09381 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{liaw_tune_2018,
  title = {Tune: {{A Research Platform}} for {{Distributed Model Selection}} and {{Training}}},
  shorttitle = {Tune},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2018},
  month = jul,
  abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters, and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
  archivePrefix = {arXiv},
  eprint = {1807.05118},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/4DH2NGPM/Liaw et al. - 2018 - Tune A Research Platform for Distributed Model Se.pdf},
  journal = {arXiv:1807.05118 [cs, stat]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{liessner_hyperparameter_2019,
  title = {Hyperparameter {{Optimization}} for {{Deep Reinforcement Learning}} in {{Vehicle Energy Management}}:},
  shorttitle = {Hyperparameter {{Optimization}} for {{Deep Reinforcement Learning}} in {{Vehicle Energy Management}}},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Agents}} and {{Artificial Intelligence}}},
  author = {Liessner, Roman and Schmitt, Jakob and Dietermann, Ansgar and B{\"a}ker, Bernard},
  year = {2019},
  pages = {134--144},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {{Prague, Czech Republic}},
  doi = {10.5220/0007364701340144},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/liessner et al_2019_hyperparameter optimization for deep reinforcement learning in vehicle energy.pdf;/home/juan/Zotero/storage/3SH2CNR7/Link.html},
  isbn = {978-989-758-350-6},
  language = {en}
}

@article{lillicrap_continuous_2015,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2015},
  month = sep,
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archivePrefix = {arXiv},
  eprint = {1509.02971},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/lillicrap et al_2015_continuous control with deep reinforcement learning.pdf;/home/juan/Zotero/storage/ZEJZ6BJQ/1509.html},
  journal = {arXiv:1509.02971 [cs, stat]},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{lillicrap_continuous_2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies ``end-to-end'': directly from raw pixel inputs.},
  archivePrefix = {arXiv},
  eprint = {1509.02971},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/7PX6F4VJ/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf},
  journal = {arXiv:1509.02971 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@book{lindauer_smac_2017,
  title = {{{SMAC}} v3: {{Algorithm Configuration}} in {{Python}}},
  author = {Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Falkner, Stefan and Biedenkapp, Andr{\'e} and Hutter, Frank},
  year = {2017},
  publisher = {{GitHub}}
}

@phdthesis{lizotte_practical_2008,
  title = {Practical {{Bayesian Optimization}}},
  author = {Lizotte, Daniel James},
  year = {2008},
  address = {{Edmonton, Alta., Canada}},
  abstract = {Global optimization of non-convex functions over real vector spaces is a problem of widespread theoretical and practical interest. In the past fifty years, research in global optimization has produced many important approaches including Lipschitz optimization, simulated annealing, homotopy methods, genetic algorithms, and Bayesian response-surface methods. This work examines the last of these approaches. The Bayesian response-surface approach to global optimization maintains a posterior model of the function being optimized by combining a prior over functions with accumulating function evaluations. The model is then used to compute which point the method should acquire next in its search for the optimum of the function. Bayesian methods can be some of the most efficient approaches to optimization in terms of the number of function evaluations required, but they have significant drawbacks: Current approaches are needlessly data-inefficient, approximations to the Bayes-optimal acquisition criterion are poorly studied, and current approaches do not take advantage of the small-scale properties of differentiable functions near local optima. This work addresses each of these problems to make Bayesian methods more widely applicable.},
  school = {University of Alberta},
  type = {{{PhD Thesis}}}
}

@article{loshchilov_cma-es_2016,
  title = {{{CMA}}-{{ES}} for {{Hyperparameter Optimization}} of {{Deep Neural Networks}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2016},
  month = apr,
  abstract = {Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. As an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy example comparing CMA-ES and state-of-the-art Bayesian optimization algorithms for tuning the hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel.},
  archivePrefix = {arXiv},
  eprint = {1604.07269},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/loshchilov_hutter_2016_cma-es for hyperparameter optimization of deep neural networks.pdf;/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/loshchilov_hutter_2016_cma-es for hyperparameter optimization of deep neural networks2.pdf;/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/loshchilov_hutter_2016_cma-es for hyperparameter optimization of deep neural networks3.pdf;/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/loshchilov_hutter_2016_cma-es for hyperparameter optimization of deep neural networks4.pdf;/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/loshchilov_hutter_2016_cma-es for hyperparameter optimization of deep neural networks5.pdf;/home/juan/Zotero/storage/TG3ICA7N/1604.html},
  journal = {arXiv:1604.07269 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@book{mackay_information_2003,
  title = {Information {{Theory}}, {{Inference}} and {{Learning Algorithms}}},
  author = {MacKay, David J. C.},
  year = {2003},
  month = sep,
  edition = {Sixth Printing 2007 edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK; New York}},
  abstract = {Information theory and inference, taught together in this exciting textbook, lie at the heart of many important areas of modern technology - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics and cryptography. The book introduces theory in tandem with applications. Information theory is taught alongside practical communication systems such as arithmetic coding for data compression and sparse-graph codes for error-correction. Inference techniques, including message-passing algorithms, Monte Carlo methods and variational approximations, are developed alongside applications to clustering, convolutional codes, independent component analysis, and neural networks. Uniquely, the book covers state-of-the-art error-correcting codes, including low-density-parity-check codes, turbo codes, and digital fountain codes - the twenty-first-century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, the book is ideal for self-learning, and for undergraduate or graduate courses. It also provides an unparalleled entry point for professionals in areas as diverse as computational biology, financial engineering and machine learning.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/mackay_2003_information theory, inference and learning algorithms.pdf;/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/mackay_2003_information theory, inference and learning algorithms2.pdf},
  isbn = {978-0-521-64298-9},
  language = {English}
}

@inproceedings{maclaurin_gradient-based_2015,
  title = {Gradient-Based {{Hyperparameter Optimization}} through {{Reversible Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  year = {2015},
  month = jun,
  pages = {2113--2122},
  abstract = {Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by ch...},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/maclaurin et al_2015_gradient-based hyperparameter optimization through reversible learning.pdf;/home/juan/Zotero/storage/84KWYD9F/maclaurin15.html},
  language = {en}
}

@inproceedings{mahmood_tuning-free_2012,
  title = {Tuning-Free Step-Size Adaptation},
  booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Mahmood, Ashique Rupam and Sutton, Richard and Degris, Thomas and Pilarski, Patrick},
  year = {2012},
  month = mar,
  doi = {10.1109/ICASSP.2012.6288330},
  abstract = {Incremental learning algorithms based on gradient descent are effective and popular in online supervised learning, reinforcement learning, signal processing, and many other application areas. An oft-noted drawback of these algorithms is that they include a step-size parameter that needs to be tuned for best performance, which may require manual intervention and significant domain knowledge or additional data. In many cases, an entire vector of step-size parameters (e.g., one for each input feature) needs to be tuned in order to attain the best performance of the algorithm. To address this, several methods have been proposed for adapting step sizes online. For example, Sutton's IDBD method can find the best vector step size for the LMS algorithm, and Schraudolph's ELK1 method, an extension of IDBD to neural networks, has proven effective on large applications, such as 3D hand tracking. However, to date all such step-size adaptation methods have included a tunable step-size parameter of their own, which we call the meta-step-size parameter. In this paper we show that the performance of existing step-size adaptation methods are strongly dependent on the choice of their meta-step-size parameter and that their meta-step-size parameter cannot be set reliably in a problem-independent way. We introduce a series of modifications and normalizations to the IDBD method that together eliminate the need to tune the meta-step-size parameter to the particular problem. We show that the resulting overall algorithm, called Autostep, performs as well or better than the existing step-size adaptation methods on a number of idealized and robot prediction problems and does not require any tuning of its meta-step-size parameter. The ideas behind Autostep are not restricted to the IDBD method and the same principles are potentially applicable to other incremental learning settings, such as reinforcement learning.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/mahmood et al_2012_tuning-free step-size adaptation.pdf}
}

@article{mania_simple_2018,
  title = {Simple Random Search Provides a Competitive Approach to Reinforcement Learning},
  author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  year = {2018},
  month = mar,
  abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.},
  archivePrefix = {arXiv},
  eprint = {1803.07055},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/mania et al_2018_simple random search provides a competitive approach to reinforcement learning.pdf;/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/mania et al_2018_simple random search provides a competitive approach to reinforcement learning2.pdf;/home/juan/Zotero/storage/IH33LB5H/1803.html;/home/juan/Zotero/storage/N683LASS/1803.html},
  journal = {arXiv:1803.07055 [cs, math, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@inproceedings{mannor_bias_2004,
  title = {Bias and Variance in Value Function Estimation},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  author = {Mannor, Shie and Simester, Duncan and Sun, Peng and Tsitsiklis, John N.},
  year = {2004},
  pages = {72},
  publisher = {{ACM Press}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1015330.1015402},
  abstract = {We consider the bias and variance of value function estimation that are caused by using an empirical model instead of the true model. We analyze these bias and variance for Markov processes from a classical (frequentist) statistical point of view, and in a Bayesian setting. Using a second order approximation, we provide explicit expressions for the bias and variance in terms of the transition counts and the reward statistics. We present supporting experiments with artificial Markov chains and with a large transactional database provided by a mail-order catalog firm.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/mannor et al_2004_bias and variance in value function estimation.pdf},
  language = {en}
}

@article{mannor_mean-variance_2011,
  title = {Mean-{{Variance Optimization}} in {{Markov Decision Processes}}},
  author = {Mannor, Shie and Tsitsiklis, John},
  year = {2011},
  month = apr,
  abstract = {We consider finite horizon Markov decision processes under performance measures that involve both the mean and the variance of the cumulative reward. We show that either randomized or history-based policies can improve performance. We prove that the complexity of computing a policy that maximizes the mean reward under a variance constraint is NP-hard for some cases, and strongly NP-hard for others. We finally offer pseudopolynomial exact and approximation algorithms.},
  archivePrefix = {arXiv},
  eprint = {1104.5601},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/mannor_tsitsiklis_2011_mean-variance optimization in markov decision processes.pdf},
  journal = {arXiv:1104.5601 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{mckay_comparison_1979,
  title = {A {{Comparison}} of {{Three Methods}} for {{Selecting Values}} of {{Input Variables}} in the {{Analysis}} of {{Output}} from a {{Computer Code}}},
  author = {McKay, M. D. and Beckman, R. J. and Conover, W. J.},
  year = {1979},
  volume = {21},
  pages = {239--245},
  issn = {0040-1706},
  doi = {10.2307/1268522},
  abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.},
  journal = {Technometrics},
  number = {2}
}

@article{mehta_high-bias_2018,
  title = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
  year = {2018},
  month = mar,
  abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias-variance tradeoff, overfitting, regularization, and generalization before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton-proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists maybe able to contribute. (Notebooks are available at https://physics.bu.edu/\textasciitilde{}pankajm/MLnotebooks.html )},
  archivePrefix = {arXiv},
  eprint = {1803.08823},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/mehta et al_2018_a high-bias, low-variance introduction to machine learning for physicists.pdf;/home/juan/Zotero/storage/UUT6YI64/1803.html},
  journal = {arXiv:1803.08823 [cond-mat, physics:physics, stat]},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics,Physics - Computational Physics,Statistics - Machine Learning},
  primaryClass = {cond-mat, physics:physics, stat}
}

@inproceedings{mendoza_towards_2016,
  title = {Towards {{Automatically}}-{{Tuned Neural Networks}}},
  booktitle = {Workshop on {{Automatic Machine Learning}}},
  author = {Mendoza, Hector and Klein, Aaron and Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
  year = {2016},
  month = dec,
  pages = {58--65},
  abstract = {Recent advances in AutoML have led to automated tools that can compete with machine learning experts on supervised learning tasks. However, current AutoML tools do not yet support modern neural net...},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/mendoza et al_2016_towards automatically-tuned neural networks.pdf;/home/juan/Zotero/storage/K6QHYGDE/mendoza_towards_2016.html},
  language = {en}
}

@article{miyashita_learning_2000,
  title = {Learning Scheduling Control Knowledge through Reinforcements},
  author = {Miyashita, Kazuo},
  year = {2000},
  month = mar,
  volume = {7},
  pages = {125--138},
  issn = {0969-6016},
  doi = {10.1016/S0969-6016(00)00014-9},
  abstract = {This paper introduces a method of learning search control knowledge in schedule optimization problems through application of reinforcement learning. Reinforcement learning is an effective approach for the problem faced by the agent that learns its behavior through trial-and-error interactions with a dynamic environment. Nevertheless, reinforcement learning has a difficulty of slow convergence when applied to the problems with a large state space. The paper discusses the case-based function approximation technique, which makes reinforcement learning applicable to the large scale problems such as a job-shop scheduling problem. To show effectiveness of the approach, reinforcement learning is applied to acquire search control knowledge in repair-based schedule optimization process. Preliminary experiment results show that repair-action selection made by learned search control knowledge succeeded in improving scheduling quality efficiently.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/miyashita_2000_learning scheduling control knowledge through reinforcements.pdf;/home/juan/Zotero/storage/3ZT9NALW/S0969601600000149.html},
  journal = {International Transactions in Operational Research},
  keywords = {Case-based function approximation,Reinforcement learning,Scheduling problem,Search control knowledge},
  number = {2}
}

@article{mnih_asynchronous_2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = feb,
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  archivePrefix = {arXiv},
  eprint = {1602.01783},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/mnih et al_2016_asynchronous methods for deep reinforcement learning.pdf;/home/juan/Zotero/storage/2HY5KRMK/1602.html},
  journal = {arXiv:1602.01783 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{mnih_human-level_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  volume = {518},
  pages = {529--533},
  issn = {0028-0836},
  doi = {10.1038/nature14236},
  abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
  copyright = {\textcopyright{} 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/mnih et al_2015_human-level control through deep reinforcement learning.pdf;/home/juan/Zotero/storage/2KFUWG4R/nature14236.html},
  journal = {Nature},
  keywords = {Computer science},
  language = {en},
  number = {7540}
}

@article{mnih_playing_2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archivePrefix = {arXiv},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/mnih et al_2013_playing atari with deep reinforcement learning.pdf;/home/juan/Zotero/storage/VEWEMAPZ/1312.html},
  journal = {arXiv:1312.5602 [cs]},
  keywords = {Computer Science - Learning},
  primaryClass = {cs}
}

@incollection{mockus_application_1978,
  title = {The Application of {{Bayesian}} Methods for Seeking the Extremum},
  booktitle = {Towards {{Global Optimisation}} 2},
  author = {Mo{\v c}kus, Jonas and Tiesis, Vytautas and Zilinskas, A.},
  editor = {Dixon, L.C.W. and Szego, G.P.},
  year = {1978},
  pages = {117--129},
  publisher = {{North-Holand}},
  abstract = {The objective function of the multi-extremal optimisation problem is modelled by the realisation of some stochastic function. The sequence of solution points seeks to minimise the expected deviation of the objective record from the extremum.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/močkus et al_1978_the application of bayesian methods for seeking the extremum.pdf}
}

@article{mockus_application_1994,
  title = {Application of {{Bayesian}} Approach to Numerical Methods of Global and Stochastic Optimization},
  author = {Mockus, Jonas},
  year = {1994},
  month = jun,
  volume = {4},
  pages = {347--365},
  issn = {0925-5001, 1573-2916},
  doi = {10.1007/BF01099263},
  abstract = {In this paper a review of application of Bayesian approach to global and stochastic optimization of continuous multimodal functions is given. Advantages and disadvantages of Bayesian approach (average case analysis), comparing it with more usual minimax approach (worst case analysis) are discussed. New interactive version of software for global optimization is discussed. Practical multidimensional problems of global optimization are considered},
  file = {/home/juan/Zotero/storage/GU9KIAUQ/BF01099263.html},
  journal = {Journal of Global Optimization},
  language = {en},
  number = {4}
}

@incollection{mockus_bayesian_1975,
  title = {On {{Bayesian Methods}} for {{Seeking}} the {{Extremum}}},
  booktitle = {Optimization {{Techniques IFIP Technical Conference}}},
  author = {Mo{\v c}kus, Jonas},
  editor = {Marchuk, Prof Dr G. I.},
  year = {1975},
  pages = {400--404},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-662-38527-2_55},
  abstract = {Many well known methods for seeking the extremum had been developed on the basis of quadratic approximation.},
  copyright = {\textcopyright{}1975 Springer-Verlag Berlin Heidelberg},
  file = {/home/juan/Zotero/storage/AIPGZTPR/978-3-662-38527-2_55.html},
  isbn = {978-3-662-37713-0 978-3-662-38527-2},
  keywords = {Control; Robotics; Mechatronics,Programming Languages; Compilers; Interpreters},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{mohri_foundations_2018,
  title = {Foundations of {{Machine Learning}}},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  month = dec,
  edition = {second edition edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {A new edition of a graduate-level machine learning textbook that focuses on the analysis and theory of algorithms.This book is a general introduction to machine learning that can serve as a textbook for graduate students and a reference for researchers. It covers fundamental modern topics in machine learning while providing the theoretical basis and conceptual tools needed for the discussion and justification of algorithms. It also describes several key aspects of the application of these algorithms. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning is unique in its focus on the analysis and theory of algorithms. The first four chapters lay the theoretical foundation for what follows; subsequent chapters are mostly self-contained. Topics covered include the Probably Approximately Correct (PAC) learning framework; generalization bounds based on Rademacher complexity and VC-dimension; Support Vector Machines (SVMs); kernel methods; boosting; on-line learning; multi-class classification; ranking; regression; algorithmic stability; dimensionality reduction; learning automata and languages; and reinforcement learning. Each chapter ends with a set of exercises. Appendixes provide additional material including concise probability review.This second edition offers three new chapters, on model selection, maximum entropy models, and conditional entropy models. New material in the appendixes includes a major section on Fenchel duality, expanded coverage of concentration inequalities, and an entirely new entry on information theory. More than half of the exercises are new to this edition.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/mohri et al_2018_foundations of machine learning.pdf},
  isbn = {978-0-262-03940-6},
  language = {English}
}

@article{moritz_ray_2017,
  title = {Ray: {{A Distributed Framework}} for {{Emerging AI Applications}}},
  shorttitle = {Ray},
  author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Paul, William and Jordan, Michael I. and Stoica, Ion},
  year = {2017},
  month = dec,
  abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a dynamic task graph computation model that supports both the task-parallel and the actor programming models. To meet the performance requirements of AI applications, we propose an architecture that logically centralizes the system's control state using a sharded storage system and a novel bottom-up distributed scheduler. In our experiments, we demonstrate sub-millisecond remote task latencies and linear throughput scaling beyond 1.8 million tasks per second. We empirically validate that Ray speeds up challenging benchmarks and serves as both a natural and performant fit for an emerging class of reinforcement learning applications and algorithms.},
  archivePrefix = {arXiv},
  eprint = {1712.05889},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/moritz et al_2017_ray.pdf;/home/juan/Zotero/storage/V4UXR7V6/1712.html},
  journal = {arXiv:1712.05889 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{murphy_machine_2012,
  title = {Machine {{Learning}}: {{A Probabilistic Perspective}}},
  shorttitle = {Machine {{Learning}}},
  author = {Murphy, Kevin P.},
  year = {2012},
  month = aug,
  edition = {1 edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, MA}},
  abstract = {A comprehensive introduction to machine learning that uses probabilistic models and inference as a unifying approach.Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach.The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package\rule{1em}{1pt}PMTK (probabilistic modeling toolkit)\rule{1em}{1pt}that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/murphy_2012_machine learning.pdf},
  isbn = {978-0-262-01802-9},
  language = {English}
}

@article{nagabandi_deep_2019,
  title = {Deep {{Dynamics Models}} for {{Learning Dexterous Manipulation}}},
  author = {Nagabandi, Anusha and Konoglie, Kurt and Levine, Sergey and Kumar, Vikash},
  year = {2019},
  month = sep,
  abstract = {Dexterous multi-fingered hands can provide robots with the ability to flexibly perform a wide range of manipulation skills. However, many of the more complex behaviors are also notoriously difficult to control: Performing in-hand object manipulation, executing finger gaits to move objects, and exhibiting precise fine motor skills such as writing, all require finely balancing contact forces, breaking and reestablishing contacts repeatedly, and maintaining control of unactuated objects. Learning-based techniques provide the appealing possibility of acquiring these skills directly from data, but current learning approaches either require large amounts of data and produce task-specific policies, or they have not yet been shown to scale up to more complex and realistic tasks requiring fine motor skills. In this work, we demonstrate that our method of online planning with deep dynamics models (PDDM) addresses both of these limitations; we show that improvements in learned dynamics models, together with improvements in online model-predictive control, can indeed enable efficient and effective learning of flexible contact-rich dexterous manipulation skills -- and that too, on a 24-DoF anthropomorphic hand in the real world, using just 4 hours of purely real-world data to learn to simultaneously coordinate multiple free-floating objects. Videos can be found at https://sites.google.com/view/pddm/},
  archivePrefix = {arXiv},
  eprint = {1909.11652},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/nagabandi et al_2019_deep dynamics models for learning dexterous manipulation.pdf;/home/juan/Zotero/storage/23B5BC2U/1909.html},
  journal = {arXiv:1909.11652 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{nikos_vlassis_bayesian_2012,
  title = {Bayesian {{Reinforcement Learning}}},
  author = {Nikos Vlassis, Mohammad Ghavamzadeh},
  year = {2012},
  pages = {359--386},
  doi = {10.1007/978-3-642-27645-3_11},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/nikos vlassis_2012_bayesian reinforcement learning.pdf;/home/juan/Zotero/storage/8A997BK5/278656617_Bayesian_Reinforcement_Learning.html}
}

@misc{noauthor_[1909.01387]_nodate,
  title = {[1909.01387] {{Making Efficient Use}} of {{Demonstrations}} to {{Solve Hard Exploration Problems}}},
  file = {/home/juan/Zotero/storage/VLFKRZBN/1909.html},
  howpublished = {https://arxiv.org/abs/1909.01387}
}

@article{noauthor_long_nodate,
  title = {Long {{Short}}-{{Term Memory}} | {{Neural Computation}} | {{MIT Press Journals}}},
  file = {/home/juan/Zotero/storage/CEJI6E69/neco.1997.9.8.html}
}

@misc{noauthor_openai_2018,
  title = {{{OpenAI Five}}},
  year = {2018},
  month = jun,
  abstract = {Our team of five neural networks, OpenAI Five, has started to defeat amateur human teams at Dota 2.},
  file = {/home/juan/Zotero/storage/4LNG4QYK/openai-five.html},
  howpublished = {https://blog.openai.com/openai-five/},
  journal = {OpenAI Blog}
}

@misc{noauthor_papers_nodate,
  title = {Papers {{With Code}} : {{Neural Architecture Search}}},
  shorttitle = {Papers {{With Code}}},
  abstract = {See leaderboards and papers with code for Neural Architecture Search},
  file = {/home/juan/Zotero/storage/MPUH2QTR/architecture-search.html},
  howpublished = {http://paperswithcode.com/task/architecture-search?page=2},
  language = {en}
}

@misc{noauthor_pattern_nodate,
  title = {Pattern {{Recognition}} and {{Machine Learning}} ({{Information Science}} and {{Statistics}}): {{Christopher M}}. {{Bishop}}: 9780387310732: {{Amazon}}.Com: {{Gateway}}},
  file = {/home/juan/Zotero/storage/5Z8L574M/ref=sr_1_1.html},
  howpublished = {https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr\_1\_1?keywords=bishop+machine+learning\&qid=1556659235\&s=gateway\&sr=8-1}
}

@misc{noauthor_simulated_nodate,
  title = {Simulated {{Policy Learning}} in {{Video Models}}},
  abstract = {Posted by {\L}ukasz Kaiser and Dumitru Erhan, Research Scientists, Google AI     Deep reinforcement learning  (RL) techniques can be used to le...},
  file = {/home/juan/Zotero/storage/6IXJ4TP8/simulated-policy-learning-in-video.html},
  journal = {Google AI Blog},
  language = {en}
}

@misc{noauthor_soar_2017,
  title = {Soar, a General Cognitive Architecture for Systems That Exhibit Intelligent Behavior},
  year = {2017},
  month = may,
  file = {/home/juan/Zotero/storage/ETM59QXT/Soar.html},
  howpublished = {University of Michigan Soar Group}
}

@misc{noauthor_true_nodate,
  title = {True {{Online Temporal}}-{{Difference Learning}}},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/true online temporal-difference learning.pdf},
  howpublished = {https://www.researchgate.net/publication/287250918\_True\_Online\_Temporal-Difference\_Learning}
}

@article{oh_combinatorial_2019,
  title = {Combinatorial {{Bayesian Optimization}} Using {{Graph Representations}}},
  author = {Oh, Changyong and Tomczak, Jakub M. and Gavves, Efstratios and Welling, Max},
  year = {2019},
  month = feb,
  abstract = {This paper focuses on Bayesian Optimization - typically considered with continuous inputs - for discrete search input spaces, including integer, categorical or graph structured input variables. In Gaussian process-based Bayesian Optimization a problem arises, as it is not straightforward to define a proper kernel on discrete input structures, where no natural notion of smoothness or similarity could be provided. We propose COMBO, a method that represents values of discrete variables as vertices of a graph and then use the diffusion kernel on that graph. As the graph size explodes with the number of categorical variables and categories, we propose the graph Cartesian product to decompose the graph into smaller sub-graphs, enabling kernel computation in linear time with respect to the number of input variables. Moreover, in our formulation we learn a scale parameter per subgraph. In empirical studies on four discrete optimization problems we demonstrate that our method is on par or outperforms the state-of-the-art in discrete Bayesian optimization.},
  archivePrefix = {arXiv},
  eprint = {1902.00448},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/oh et al_2019_combinatorial bayesian optimization using graph representations.pdf},
  journal = {arXiv:1902.00448 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@misc{OpenAI_dota,
  title = {{{OpenAI Five}}},
  author = {{OpenAI}},
  year = {2018},
  howpublished = {https://blog.openai.com/openai-five/}
}

@article{openai2019dota,
  title = {Dota 2 with Large Scale Deep Reinforcement Learning},
  author = {{OpenAI} and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and {de Oliveira Pinto}, Henrique Pond{\'e} and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  year = {2019},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/openai et al_2019_dota 2 with large scale deep reinforcement learning.pdf},
  journal = {arXiv preprint}
}

@article{osband_behaviour_2019,
  title = {Behaviour {{Suite}} for {{Reinforcement Learning}}},
  author = {Osband, Ian and Doron, Yotam and Hessel, Matteo and Aslanides, John and Sezener, Eren and Saraiva, Andre and McKinney, Katrina and Lattimore, Tor and Szepezvari, Csaba and Singh, Satinder and Van Roy, Benjamin and Sutton, Richard and Silver, David and Van Hasselt, Hado},
  year = {2019},
  month = aug,
  abstract = {This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source github.com/deepmind/bsuite, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.},
  archivePrefix = {arXiv},
  eprint = {1908.03568},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/osband et al_2019_behaviour suite for reinforcement learning.pdf},
  journal = {arXiv:1908.03568 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{osband_deep_2016,
  title = {Deep {{Exploration}} via {{Bootstrapped DQN}}},
  author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  year = {2016},
  month = jul,
  abstract = {Efficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as -greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex environments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.},
  archivePrefix = {arXiv},
  eprint = {1602.04621},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/GKCMBTZ7/Osband et al. - 2016 - Deep Exploration via Bootstrapped DQN.pdf},
  journal = {arXiv:1602.04621 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@book{otterlo_logic_2009,
  title = {The Logic of Adaptive Behavior: Knowledge Representation and Algorithms for Adaptive Sequential Decision Making under Uncertainty in First-Order and Relational Domains},
  shorttitle = {The Logic of Adaptive Behavior},
  author = {van Otterlo, Martijn},
  year = {2009},
  publisher = {{IOS Press}},
  address = {{Amsterdam}},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/otterlo_2009_the logic of adaptive behavior2.pdf},
  isbn = {978-1-58603-969-1},
  language = {eng},
  note = {OCLC: 313654110},
  number = {192},
  series = {Frontiers in Artificial Intelligence and Applications}
}

@article{paine_making_2019,
  title = {Making {{Efficient Use}} of {{Demonstrations}} to {{Solve Hard Exploration Problems}}},
  author = {Paine, Tom Le and Gulcehre, Caglar and Shahriari, Bobak and Denil, Misha and Hoffman, Matt and Soyer, Hubert and Tanburn, Richard and Kapturowski, Steven and Rabinowitz, Neil and Williams, Duncan and {Barth-Maron}, Gabriel and Wang, Ziyu and {de Freitas}, Nando and Team, Worlds},
  year = {2019},
  month = sep,
  abstract = {This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.},
  archivePrefix = {arXiv},
  eprint = {1909.01387},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/paine et al_2019_making efficient use of demonstrations to solve hard exploration problems.pdf},
  journal = {arXiv:1909.01387 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@phdthesis{palombarini_enfoque_nodate,
  title = {{Un Enfoque Cognitivo a la Reparaci{\'o}n Autom{\'a}tica de Planes y Schedules Integrando Aprendizaje por Refuerzos con Abstracciones L{\'o}gicas y Relacionales}},
  author = {Palombarini, Jorge},
  file = {C\:\\Users\\JuanCruz\\Documents\\Dropbox\\Intercambio Grupo de Investigacion\\Papers Tesis\\Version final\\Tesis Doctoral Jorge A Palombarini.pdf},
  language = {Espa{\~n}ol}
}

@inproceedings{palombarini_generating_2014,
  title = {Generating {{Rescheduling Knowledge}} Using {{Reinforcement Learning}} in a {{Cognitive Architecture}}},
  booktitle = {Anales Del 15\textordmasculine{} {{Simposio Argentino}} de {{Inteligencia Artificial}}},
  author = {Palombarini, Jorge A. and Barsce, Juan Cruz and Mart{\'i}nez, Ernesto C.},
  year = {2014},
  address = {{Universidad de Palermo, Buenos Aires}},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/palombarini et al_2014_generating rescheduling knowledge using reinforcement learning in a cognitive.pdf;C\:\\Users\\JuanCruz\\Documents\\Dropbox\\Intercambio Grupo de Investigacion\\JAIIO 2014\\splnproc1110\\Palombarini Barsce Martínez - Generating Rescheduling Knowledge using Reinforcement Learning in a Cognitive Architecture.pdf}
}

@incollection{palombarini_learning_2009,
  title = {Learning to {{Repair Plans}} and {{Schedules Using}} a {{Relational}} ({{Deictic}}) {{Representation}}},
  booktitle = {Computer {{Aided Chemical Engineering}}},
  author = {Palombarini, Jorge and Mart{\'i}nez, Ernesto},
  editor = {{de Brito Alves}, Rita Maria and {do Nascimento}, Caludio Augusto Oller and Biscaia, Evaristo Chalbaud},
  year = {2009},
  month = jan,
  volume = {27},
  pages = {1377--1382},
  publisher = {{Elsevier}},
  doi = {10.1016/S1570-7946(09)70620-0},
  abstract = {Unplanned and abnormal events may have a significant impact in the feasibility of plans and schedules which requires to repair them 'on-the-fly' to guarantee due date compliance of orders-in-progress and negotiating delivery conditions for new orders. In this work, a repair-based rescheduling approach based on the integration of intensive simulations with logical and relational reinforcement learning is proposed. Based on a deictic representation of schedule states a number of repair operators have been designed to guide the search for a goal state. The knowledge generated via simulation is encoded in a relational regressi{\'o}n tree for the Q-value function defining the utility of applying a given repair operator at a given schedule state. A prototype implementation is discussed using a representative example of 3 batch extruders processing orders for 4 different products. The learning curve for the problem of inserting a new order vividly illustrates the advantages of logical and relational learning in rescheduling.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/palombarini_martínez_2009_learning to repair plans and schedules using a relational (deictic).pdf;/home/juan/Zotero/storage/K8FV4W2B/S1570794609706200.html},
  keywords = {artificial intelligence,batch plants,relational modeling,rescheduling},
  series = {10th {{International Symposium}} on {{Process Systems Engineering}}: {{Part A}}}
}

@article{palombarini_smartgantt_2012,
  title = {{{SmartGantt}} \textendash{} {{An}} Intelligent System for Real Time Rescheduling Based on Relational Reinforcement Learning},
  author = {Palombarini, Jorge and Mart{\'i}nez, Ernesto},
  year = {Setiembre 1, 2012},
  volume = {39},
  pages = {10251--10268},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2012.02.176},
  abstract = {With the current trend towards cognitive manufacturing systems to deal with unforeseen events and disturbances that constantly demand real-time repair decisions, learning/reasoning skills and interactive capabilities are important functionalities for rescheduling a shop-floor on the fly taking into account several objectives and goal states. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. Deictic representations of schedules based on focal points are used to define a repair policy which generates a goal-directed sequence of repair operators to face unplanned events and operational disturbances. An industrial example where rescheduling is needed due to the arrival of a new/rush order, or whenever raw material delay/shortage or machine breakdown events occur are discussed using the SmartGantt prototype for interactive rescheduling in real-time. SmartGantt demonstrates that due date compliance of orders-in-progress, negotiating delivery conditions of new orders and ensuring distributed production control can be dramatically improved by means of relational reinforcement learning and a deictic representation of rescheduling tasks.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/palombarini_martínez_2012_smartgantt – an intelligent system for real time rescheduling based on.pdf;/home/juan/Zotero/storage/NKVXNFNT/S0957417412004393.html},
  journal = {Expert Systems with Applications},
  keywords = {Automated planning,Information systems,Manufacturing systems,Real-time rescheduling,Reinforcement learning,Relational abstractions},
  number = {11}
}

@article{palombarini_smartgantt_2012-2,
  title = {{{SmartGantt}} \textendash{} {{An}} Interactive System for Generating and Updating Rescheduling Knowledge Using Relational Abstractions},
  author = {Palombarini, Jorge and Mart{\'i}nez, Ernesto},
  year = {2012},
  month = dec,
  volume = {47},
  pages = {202--216},
  issn = {0098-1354},
  doi = {10.1016/j.compchemeng.2012.06.021},
  abstract = {Generating and updating rescheduling knowledge that can be used in real time has become a key issue in reactive scheduling due to the dynamic and uncertain nature of industrial environments and the emergent trend towards cognitive systems in production planning and execution control. Disruptive events have a significant impact on the feasibility of plans and schedules. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. An industrial example where a current schedule must be repaired in response to unplanned events such as the arrival of a rush order, raw material delay, or an equipment failure which gives rise to the need for rescheduling is discussed. A software prototype (SmartGantt) for interactive schedule repair in real-time is presented. Results demonstrate that responsiveness is dramatically improved by using relational reinforcement learning and relational abstractions to develop a repair policy.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/palombarini_martínez_2012_smartgantt – an interactive system for generating and updating rescheduling.pdf;/home/juan/Zotero/storage/M6ETL3LD/S0098135412001974.html},
  journal = {Computers \& Chemical Engineering},
  keywords = {Batch plant management,Cognitive production systems,Manufacturing control,Relational reinforcement learning,Rescheduling,Uncertainty},
  series = {{{FOCAPO}} 2012}
}

@article{parisotto_concurrent_2019,
  title = {Concurrent {{Meta Reinforcement Learning}}},
  author = {Parisotto, Emilio and Ghosh, Soham and Yalamanchi, Sai Bhargav and Chinnaobireddy, Varsha and Wu, Yuhuai and Salakhutdinov, Ruslan},
  year = {2019},
  month = mar,
  abstract = {State-of-the-art meta reinforcement learning algorithms typically assume the setting of a single agent interacting with its environment in a sequential manner. A negative side-effect of this sequential execution paradigm is that, as the environment becomes more and more challenging, and thus requiring more interaction episodes for the meta-learner, it needs the agent to reason over longer and longer time-scales. To combat the difficulty of long time-scale credit assignment, we propose an alternative parallel framework, which we name ``Concurrent Meta-Reinforcement Learning'' (CMRL), that transforms the temporal credit assignment problem into a multi-agent reinforcement learning one. In this multi-agent setting, a set of parallel agents are executed in the same environment and each of these "rollout" agents are given the means to communicate with each other. The goal of the communication is to coordinate, in a collaborative manner, the most efficient exploration of the shared task the agents are currently assigned. This coordination therefore represents the meta-learning aspect of the framework, as each agent can be assigned or assign itself a particular section of the current task's state space. This framework is in contrast to standard RL methods that assume that each parallel rollout occurs independently, which can potentially waste computation if many of the rollouts end up sampling the same part of the state space. Furthermore, the parallel setting enables us to define several reward sharing functions and auxiliary losses that are non-trivial to apply in the sequential setting. We demonstrate the effectiveness of our proposed CMRL at improving over sequential methods in a variety of challenging tasks.},
  archivePrefix = {arXiv},
  eprint = {1903.02710},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/parisotto et al_2019_concurrent meta reinforcement learning.pdf},
  journal = {arXiv:1903.02710 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{pathak_curiosity-driven_2017,
  title = {Curiosity-Driven {{Exploration}} by {{Self}}-Supervised {{Prediction}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  year = {2017},
  month = jul,
  pages = {2778--2787},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to expl...},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/pathak et al_2017_curiosity-driven exploration by self-supervised prediction.pdf;/home/juan/Zotero/storage/BHNAT5ZY/pathak17a.html},
  language = {en}
}

@article{paul_fast_2019,
  title = {Fast {{Efficient Hyperparameter Tuning}} for {{Policy Gradients}}},
  author = {Paul, Supratik and Kurin, Vitaly and Whiteson, Shimon},
  year = {2019},
  month = feb,
  abstract = {The performance of policy gradient methods is sensitive to hyperparameter settings that must be tuned for any new application. Widely used grid search methods for tuning hyperparameters are sample inefficient and computationally expensive. More advanced methods like Population Based Training that learn optimal schedules for hyperparameters instead of fixed settings can yield better results, but are also sample inefficient and computationally expensive. In this paper, we propose Hyperparameter Optimisation on the Fly (HOOF), a gradient-free meta-learning algorithm that can automatically learn an optimal schedule for hyperparameters that affect the policy update directly through the gradient. The main idea is to use existing trajectories sampled by the policy gradient method to optimise a one-step improvement objective, yielding a sample and computationally efficient algorithm that is easy to implement. Our experimental results across multiple domains and algorithms show that using HOOF to learn these hyperparameter schedules leads to faster learning with improved performance.},
  archivePrefix = {arXiv},
  eprint = {1902.06583},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/paul et al_2019_fast efficient hyperparameter tuning for policy gradients.pdf;/home/juan/Zotero/storage/GPRXPANZ/1902.html},
  journal = {arXiv:1902.06583 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{pautrat_bayesian_2017,
  title = {Bayesian {{Optimization}} with {{Automatic Prior Selection}} for {{Data}}-{{Efficient Direct Policy Search}}},
  author = {Pautrat, R{\'e}mi and Chatzilygeroudis, Konstantinos and Mouret, Jean-Baptiste},
  year = {2017},
  month = sep,
  abstract = {One of the most interesting features of Bayesian optimization for direct policy search is that it can leverage priors (e.g., from simulation or from previous tasks) to accelerate learning on a robot. In this paper, we are interested in situations for which several priors exist but we do not know in advance which one fits best the current situation. We tackle this problem by introducing a novel acquisition function, called Most Likely Expected Improvement (MLEI), that combines the likelihood of the priors and the expected improvement. We evaluate this new acquisition function on a transfer learning task for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has to learn to walk on flat ground and on stairs, with priors corresponding to different stairs and different kinds of damages. Our results show that MLEI effectively identifies and exploits the priors, even when there is no obvious match between the current situations and the priors.},
  archivePrefix = {arXiv},
  eprint = {1709.06919},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/pautrat et al_2017_bayesian optimization with automatic prior selection for data-efficient direct.pdf;/home/juan/Zotero/storage/WKZXER2I/1709.html},
  journal = {arXiv:1709.06919 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  primaryClass = {cs}
}

@incollection{peng_incremental_1994,
  title = {Incremental {{Multi}}-{{Step Q}}-{{Learning}}},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  author = {Peng, Jing and Williams, Ronald J.},
  editor = {Cohen, William W. and Hirsh, Haym},
  year = {1994},
  pages = {226--232},
  publisher = {{Morgan Kaufmann}},
  address = {{San Francisco (CA)}},
  doi = {10.1016/B978-1-55860-335-6.50035-0},
  abstract = {This paper presents a novel incremental algorithm that combines Q-learning, a well-known dynamic programming-based reinforcement learning method, with the TD(A) return estimation process, which is typically used in actor-critic learning, another well-known dynamic programming-based reinforcement learning method. The parameter A is used to distribute credit throughout sequences of actions, leading to faster learning and also helping to alleviate the non-Markovian effect of coarse state-space quantization. The resulting algorithm, Q({$\lambda$})-learning, thus combines some of the best features of the Q-learning and actor-critic learning paradigms. The behavior of this algorithm is demonstrated through computer simulations of the standard benchmark control problem of learning to balance a pole on a cart.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/peng_williams_1994_incremental multi-step q-learning.pdf},
  isbn = {978-1-55860-335-6}
}

@article{peters_natural_2008,
  title = {Natural {{Actor}}-{{Critic}}},
  author = {Peters, Jan and Schaal, Stefan},
  year = {2008},
  month = mar,
  volume = {71},
  pages = {1180--1190},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2007.11.026},
  abstract = {In this paper, we suggest a novel reinforcement learning architecture, the Natural Actor-Critic. The actor updates are achieved using stochastic policy gradients employing Amari's natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke's Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm.},
  file = {/home/juan/Zotero/storage/3RVJJPS9/S0925231208000532.html},
  journal = {Neurocomputing},
  keywords = {Actor-Critic methods,Compatible function approximation,Natural gradients,Policy-gradient methods,Reinforcement learning,Robot learning},
  number = {7},
  series = {Progress in {{Modeling}}, {{Theory}}, and {{Application}} of {{Computational Intelligenc}}}
}

@article{peters_reinforcement_2008,
  title = {Reinforcement Learning of Motor Skills with Policy Gradients},
  author = {Peters, Jan and Schaal, Stefan},
  year = {2008},
  month = may,
  volume = {21},
  pages = {682--697},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2008.02.003},
  abstract = {Autonomous learning is one of the hallmarks of human and animal behavior, and understanding the principles of learning will be crucial in order to achieve true autonomy in advanced machines like humanoid robots. In this paper, we examine learning of complex motor skills with human-like limbs. While supervised learning can offer useful tools for bootstrapping behavior, e.g.,~by learning from demonstration, it is only reinforcement learning that offers a general approach to the final trial-and-error improvement that is needed by each individual acquiring a skill. Neither neurobiological nor machine learning studies have, so far, offered compelling results on how reinforcement learning can be scaled to the high-dimensional continuous state and action spaces of humans or humanoids. Here, we combine two recent research developments on learning motor control in order to achieve this scaling. First, we interpret the idea of modular motor control by means of motor primitives as a suitable way to generate parameterized control policies for reinforcement learning. Second, we combine motor primitives with the theory of stochastic policy gradient learning, which currently seems to be the only feasible framework for reinforcement learning for humanoids. We evaluate different policy gradient methods with a focus on their applicability to parameterized motor primitives. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm.},
  file = {/home/juan/Zotero/storage/NAEU2XIK/S0893608008000701.html},
  journal = {Neural Networks},
  keywords = {Motor primitives,Motor skills,Natural Actor-Critic,Natural gradients,Policy gradient methods,Reinforcement learning},
  number = {4},
  series = {Robotics and {{Neuroscience}}}
}

@article{petersen_[_nodate,
  title = {[ {{http://matrixcookbook.com}} ]},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  pages = {72},
  file = {/home/juan/Zotero/storage/F3I5Q7GF/Petersen and Pedersen - [ httpmatrixcookbook.com ].pdf},
  language = {en}
}

@article{pham_efficient_2018,
  title = {Efficient {{Neural Architecture Search}} via {{Parameter Sharing}}},
  author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
  year = {2018},
  month = feb,
  abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller discovers neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on a validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, while using much fewer GPUhours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS finds a novel architecture that achieves 2.89\% test error, which is on par with the 2.65\% test error of NASNet (Zoph et al., 2018).},
  archivePrefix = {arXiv},
  eprint = {1802.03268},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/8C9HLAVB/Pham et al. - 2018 - Efficient Neural Architecture Search via Parameter.pdf},
  journal = {arXiv:1802.03268 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{polson_deep_2017,
  title = {Deep {{Learning}}: {{A Bayesian Perspective}}},
  shorttitle = {Deep {{Learning}}},
  author = {Polson, Nicholas and Sokolov, Vadim},
  year = {2017},
  month = jun,
  abstract = {Deep learning is a form of machine learning for nonlinear high dimensional data reduction and prediction. A Bayesian probabilistic perspective provides a number of advantages. Specifically statistical interpretation and properties, more efficient algorithms for optimisation and hyper-parameter tuning, and an explanation of predictive performance. Traditional high-dimensional statistical techniques; principal component analysis (PCA), partial least squares (PLS), reduced rank regression (RRR), projection pursuit regression (PPR) are shown to be shallow learners. Their deep learning counterparts exploit multiple layers of of data reduction which leads to performance gains. Stochastic gradient descent (SGD) training and optimisation and Dropout (DO) provides model and variable selection. Bayesian regularization is central to finding networks and provides a framework for optimal bias-variance trade-off to achieve good out-of sample performance. Constructing good Bayesian predictors in high dimensions is discussed. To illustrate our methodology, we provide an analysis of first time international bookings on Airbnb. Finally, we conclude with directions for future research.},
  archivePrefix = {arXiv},
  eprint = {1706.00473},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/polson_sokolov_2017_deep learning.pdf;/home/juan/Zotero/storage/44N7JMRC/1706.html},
  journal = {arXiv:1706.00473 [cs, stat]},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@book{powell_approximate_2011,
  title = {Approximate {{Dynamic Programming}}: {{Solving}} the {{Curses}} of {{Dimensionality}}, 2nd {{Edition}}},
  shorttitle = {Approximate {{Dynamic Programming}}},
  author = {Powell, Warren B.},
  year = {2011},
  month = sep,
  edition = {2 edition},
  publisher = {{Wiley}},
  address = {{Hoboken, NJ}},
  abstract = {Praise for the First Edition "Finally, a book devoted to dynamic programming and written using the language of operations research (OR)! This beautiful book fills a gap in the libraries of OR specialists and practitioners." \textemdash{}Computing Reviews This new edition showcases a focus on modeling and computation for complex classes of approximate dynamic programming problems Understanding approximate dynamic programming (ADP) is vital in order to develop practical and high-quality solutions to complex industrial problems, particularly when those problems involve making decisions in the presence of uncertainty. Approximate Dynamic Programming, Second Edition uniquely integrates four distinct disciplines\textemdash{}Markov decision processes, mathematical programming, simulation, and statistics\textemdash{}to demonstrate how to successfully approach, model, and solve a wide range of real-life problems using ADP. The book continues to bridge the gap between computer science, simulation, and operations research and now adopts the notation and vocabulary of reinforcement learning as well as stochastic search and simulation optimization. The author outlines the essential algorithms that serve as a starting point in the design of practical solutions for real problems. The three curses of dimensionality that impact complex problems are introduced and detailed coverage of implementation challenges is provided. The Second Edition also features:   A new chapter describing four fundamental classes of policies for working with diverse stochastic optimization problems: myopic policies, look-ahead policies, policy function approximations, and policies based on value function approximations   A new chapter on policy search that brings together stochastic search and simulation optimization concepts and introduces a new class of optimal learning strategies   Updated coverage of the exploration exploitation problem in ADP, now including a recently developed method for doing active learning in the presence of a physical state, using the concept of the knowledge gradient   A new sequence of chapters describing statistical methods for approximating value functions, estimating the value of a fixed policy, and value function approximation while searching for optimal policies   The presented coverage of ADP emphasizes models and algorithms, focusing on related applications and computation while also discussing the theoretical side of the topic that explores proofs of convergence and rate of convergence. A related website features an ongoing discussion of the evolving fields of approximation dynamic programming and reinforcement learning, along with additional readings, software, and datasets. Requiring only a basic understanding of statistics and probability, Approximate Dynamic Programming, Second Edition is an excellent book for industrial engineering and operations research courses at the upper-undergraduate and graduate levels. It also serves as a valuable reference for researchers and professionals who utilize dynamic programming, stochastic programming, and control theory to solve problems in their everyday work.},
  isbn = {978-0-470-60445-8},
  language = {English}
}

@book{powell_optimal_2012,
  title = {Optimal {{Learning}}},
  author = {Powell, Warren B. and Ryzhov, Ilya O.},
  year = {2012},
  month = apr,
  edition = {1 edition},
  publisher = {{Wiley}},
  address = {{Hoboken, NJ}},
  abstract = {Learn the science of collecting information to make effective decisions Everyday decisions are made without the benefit of accurate information. Optimal Learning develops the needed principles for gathering information to make decisions, especially when collecting information is time-consuming and expensive. Designed for readers with an elementary background in probability and statistics, the book presents effective and practical policies illustrated in a wide range of applications, from energy, homeland security, and transportation to engineering, health, and business. This book covers the fundamental dimensions of a learning problem and presents a simple method for testing and comparing policies for learning. Special attention is given to the knowledge gradient policy and its use with a wide range of belief models, including lookup table and parametric and for online and offline problems. Three sections develop ideas with increasing levels of sophistication:  Fundamentals explores fundamental topics, including adaptive learning, ranking and selection, the knowledge gradient, and bandit problems Extensions and Applications features coverage of linear belief models, subset selection models, scalar function optimization, optimal bidding, and stopping problems Advanced Topics explores complex methods including simulation optimization, active learning in mathematical programming, and optimal continuous measurements  Each chapter identifies a specific learning problem, presents the related, practical algorithms for implementation, and concludes with numerous exercises. A related website features additional applications and downloadable software, including MATLAB and the Optimal Learning Calculator, a spreadsheet-based package that provides an introduc\-tion to learning and a variety of policies for learning.},
  isbn = {978-0-470-59669-2},
  language = {English}
}

@book{press_numerical_2007,
  title = {Numerical {{Recipes}} 3rd {{Edition}}: {{The Art}} of {{Scientific Computing}}},
  shorttitle = {Numerical {{Recipes}} 3rd {{Edition}}},
  author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
  year = {2007},
  month = sep,
  edition = {3 edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  abstract = {Co-authored by four leading scientists from academia and industry, Numerical Recipes Third Edition starts with basic mathematics and computer science and proceeds to complete, working routines.  Widely recognized as the most comprehensive, accessible and practical basis for scientific computing, this new edition incorporates more than 400 Numerical Recipes routines, many of them new or upgraded.  The executable C++ code, now printed in color for easy reading, adopts an object-oriented style particularly suited to scientific applications. The whole book is presented in the informal, easy-to-read style that made earlier editions so popular.  Please visit www.nr.com or www.cambridge.org/us/numericalrecipes for more details.   More information concerning licenses is available at:   www.nr.com/licenses     New key features:   2 new chapters, 25 new sections, 25\% longer than Second Edition  Thorough upgrades throughout the text  Over 100 completely new routines and upgrades of many more.  New Classification and Inference chapter, including Gaussian mixture models, HMMs, hierarchical clustering, Support Vector MachinesNew Computational Geometry chapter covers KD trees, quad- and octrees, Delaunay triangulation, and algorithms for lines, polygons, triangles, and spheres  New sections include interior point methods for linear programming, Monte Carlo Markov Chains, spectral and pseudospectral methods for PDEs, and many new statistical distributions  An expanded treatment of ODEs with completely new routines      Plus comprehensive coverage of    linear algebra, interpolation, special functions, random numbers, nonlinear sets of equations, optimization, eigensystems, Fourier methods and wavelets, statistical tests, ODEs and PDEs, integral equations, and inverse theory},
  isbn = {978-0-521-88068-8},
  language = {English}
}

@article{preuss_sequential_2017,
  title = {Sequential {{Batch Design}} for {{Gaussian Processes Employing Marginalization}} \textdagger{}},
  author = {Preuss, Roland and {von Toussaint}, Udo},
  year = {2017},
  month = feb,
  volume = {19},
  pages = {84},
  doi = {10.3390/e19020084},
  abstract = {Within the Bayesian framework, we utilize Gaussian processes for parametric studies of long running computer codes. Since the simulations are expensive, it is necessary to exploit the computational budget in the best possible manner. Employing the sum over variances \textemdash{}being indicators for the quality of the fit\textemdash{}as the utility function, we establish an optimized and automated sequential parameter selection procedure. However, it is also often desirable to utilize the parallel running capabilities of present computer technology and abandon the sequential parameter selection for a faster overall turn-around time (wall-clock time). This paper proposes to achieve this by marginalizing over the expected outcomes at optimized test points in order to set up a pool of starting values for batch execution. For a one-dimensional test case, the numerical results are validated with the analytical solution. Eventually, a systematic convergence study demonstrates the advantage of the optimized approach over randomly chosen parameter settings.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/preuss_von toussaint_2017_sequential batch design for gaussian processes employing marginalization †.pdf;/home/juan/Zotero/storage/NQ8M74KI/84.html},
  journal = {Entropy},
  keywords = {batch execution,Gaussian process,parallelization,parametric studies},
  language = {en},
  number = {2}
}

@article{price_convergent_2002,
  title = {A {{Convergent Variant}} of the {{Nelder}}\textendash{{Mead Algorithm}}},
  author = {Price, C. J. and Coope, I. D. and Byatt, D.},
  year = {2002},
  month = apr,
  volume = {113},
  pages = {5--19},
  issn = {0022-3239, 1573-2878},
  doi = {10.1023/A:1014849028575},
  abstract = {The Nelder\textendash{}Mead algorithm (1965) for unconstrained optimization has been used extensively to solve parameter estimation and other problems. Despite its age, it is still the method of choice for many practitioners in the fields of statistics, engineering, and the physical and medical sciences because it is easy to code and very easy to use. It belongs to a class of methods which do not require derivatives and which are often claimed to be robust for problems with discontinuities or where the function values are noisy. Recently (1998), it has been shown that the method can fail to converge or converge to nonsolutions on certain classes of problems. Only very limited convergence results exist for a restricted class of problems in one or two dimensions. In this paper, a provably convergent variant of the Nelder\textendash{}Mead simplex method is presented and analyzed. Numerical results are included to show that the modified algorithm is effective in practice.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/price et al_2002_a convergent variant of the nelder–mead algorithm.pdf;/home/juan/Zotero/storage/A5HCHVR2/A1014849028575.html},
  journal = {Journal of Optimization Theory and Applications},
  keywords = {Applications of Mathematics,Calculus of Variations and Optimal Control,derivative free optimization,Engineering; general,frame based methods,Nelder–Mead algorithm,Operation Research/Decision Theory,Optimization,polytope,positive basis methods,simplex,Theory of Computation},
  language = {en},
  number = {1}
}

@article{rabinowitz_machine_2018,
  title = {Machine {{Theory}} of {{Mind}}},
  author = {Rabinowitz, Neil C. and Perbet, Frank and Song, H. Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
  year = {2018},
  month = feb,
  abstract = {Theory of mind (ToM; Premack \& Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test (Wimmer \& Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.},
  archivePrefix = {arXiv},
  eprint = {1802.07740},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/rabinowitz et al_2018_machine theory of mind.pdf;/home/juan/Zotero/storage/KV5YJLW9/1802.html},
  journal = {arXiv:1802.07740 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@article{rajeswaran_towards_2017,
  title = {Towards {{Generalization}} and {{Simplicity}} in {{Continuous Control}}},
  author = {Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel and Kakade, Sham},
  year = {2017},
  month = mar,
  abstract = {This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of continuous control tasks, including the OpenAI gym benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, existing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory-centric policies. Training with a diverse initial state distribution is shown to produce more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the supplementary video.},
  archivePrefix = {arXiv},
  eprint = {1703.02660},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/rajeswaran et al_2017_towards generalization and simplicity in continuous control.pdf;/home/juan/Zotero/storage/C9NK2F8Y/1703.html},
  journal = {arXiv:1703.02660 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control},
  primaryClass = {cs}
}

@article{ramos_deconstructing_2018,
  title = {Deconstructing {{Cross}}-{{Entropy}} for {{Probabilistic Binary Classifiers}}},
  author = {Ramos, Daniel and {Franco-Pedroso}, Javier and {Lozano-Diez}, Alicia and {Gonzalez-Rodriguez}, Joaquin},
  year = {2018},
  month = mar,
  volume = {20},
  pages = {208},
  doi = {10.3390/e20030208},
  abstract = {In this work, we analyze the cross-entropy function, widely used in classifiers both as a performance measure and as an optimization objective. We contextualize cross-entropy in the light of Bayesian decision theory, the formal probabilistic framework for making decisions, and we thoroughly analyze its motivation, meaning and interpretation from an information-theoretical point of view. In this sense, this article presents several contributions: First, we explicitly analyze the contribution to cross-entropy of (i) prior knowledge; and (ii) the value of the features in the form of a likelihood ratio. Second, we introduce a decomposition of cross-entropy into two components: discrimination and calibration. This decomposition enables the measurement of different performance aspects of a classifier in a more precise way; and justifies previously reported strategies to obtain reliable probabilities by means of the calibration of the output of a discriminating classifier. Third, we give different information-theoretical interpretations of cross-entropy, which can be useful in different application scenarios, and which are related to the concept of reference probabilities. Fourth, we present an analysis tool, the Empirical Cross-Entropy (ECE) plot, a compact representation of cross-entropy and its aforementioned decomposition. We show the power of ECE plots, as compared to other classical performance representations, in two diverse experimental examples: a speaker verification system, and a forensic case where some glass findings are present.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/ramos et al_2018_deconstructing cross-entropy for probabilistic binary classifiers.pdf;/home/juan/Zotero/storage/K79TM7BK/208.html},
  journal = {Entropy},
  keywords = {Bayesian,calibration,classifier,cross-entropy,discrimination,ECE plot,probabilistic},
  language = {en},
  number = {3}
}

@inproceedings{rana_high_2017,
  title = {High {{Dimensional Bayesian Optimization}} with {{Elastic Gaussian Process}}},
  booktitle = {{{PMLR}}},
  author = {Rana, Santu and Li, Cheng and Gupta, Sunil and Nguyen, Vu and Venkatesh, Svetha},
  year = {2017},
  month = jul,
  pages = {2883--2891},
  abstract = {Bayesian optimization is an efficient way to optimize expensive black-box functions such as designing a new product with highest quality or hyperparameter tuning of a machine learning algorithm. Ho...},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/rana et al_2017_high dimensional bayesian optimization with elastic gaussian process.pdf;/home/juan/Zotero/storage/KUURG6MC/rana17a.html},
  language = {en}
}

@book{rasmussen_gaussian_2008,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2008},
  edition = {3. print},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/rasmussen_williams_2008_gaussian processes for machine learning2.pdf},
  isbn = {978-0-262-18253-9},
  keywords = {Gauß-Prozess,Gaussian processes Data processing,Machine learning Mathematical models,Maschinelles Lernen},
  language = {eng},
  series = {Adaptive Computation and Machine Learning}
}

@article{recht_tour_2018,
  title = {A {{Tour}} of {{Reinforcement Learning}}: {{The View}} from {{Continuous Control}}},
  shorttitle = {A {{Tour}} of {{Reinforcement Learning}}},
  author = {Recht, Benjamin},
  year = {2018},
  month = jun,
  abstract = {This manuscript surveys reinforcement learning from the perspective of optimization and control with a focus on continuous control applications. It surveys the general formulation, terminology, and typical experimental implementations of reinforcement learning and reviews competing solution paradigms. In order to compare the relative merits of various techniques, this survey presents a case study of the Linear Quadratic Regulator (LQR) with unknown dynamics, perhaps the simplest and best-studied problem in optimal control. The manuscript describes how merging techniques from learning theory and control can provide non-asymptotic characterizations of LQR performance and shows that these characterizations tend to match experimental behavior. In turn, when revisiting more complex applications, many of the observed phenomena in LQR persist. In particular, theory and experiment demonstrate the role and importance of models and the cost of generality in reinforcement learning algorithms. This survey concludes with a discussion of some of the challenges in designing learning systems that safely and reliably interact with complex and uncertain environments and how tools from reinforcement learning and control might be combined to approach these challenges.},
  archivePrefix = {arXiv},
  eprint = {1806.09460},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/recht_2018_a tour of reinforcement learning.pdf;/home/juan/Zotero/storage/WZR3DPAD/1806.html},
  journal = {arXiv:1806.09460 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{rolon_agent-based_2012,
  title = {Agent-Based Modeling and Simulation of an Autonomic Manufacturing Execution System},
  author = {Rol{\'o}n, Milagros and Mart{\'i}nez, Ernesto},
  year = {2012},
  month = jan,
  volume = {63},
  pages = {53--78},
  issn = {0166-3615},
  doi = {10.1016/j.compind.2011.10.005},
  abstract = {Production management systems must constantly deal with unplanned disruptive events and disturbances such as arrivals of rush orders, raw material shortage/delays or equipment breakdowns along with a multitude of interactions in the supply chain which constantly demand on-line task rescheduling and order execution control. For responsiveness and agility at the shop-floor, a distributed design for manufacturing execution systems is proposed based on autonomic units that fill the gap between production planning and shop-floor control. An interaction mechanism designed around the concept of order and resource agents implementing the monitor-analyze-plan-execution loop is described. Generative simulation modeling of an autonomic manufacturing execution system (@MES) is proposed in order to evaluate emerging behaviors and macroscopic dynamics in a multiproduct batch plant. Results obtained for an industrial case study using a simulation model of the proposed @MES are presented. The usefulness of agent-based modeling and simulation as a tool for distributed MESs design and to verify performance, stability and disturbance rejection capability of an interaction mechanism is highlighted.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/rolón_martínez_2012_agent-based modeling and simulation of an autonomic manufacturing execution.pdf;/home/juan/Zotero/storage/E7KI6F94/S0166361511001199.html},
  journal = {Computers in Industry},
  keywords = {Distributed scheduling,Intelligent automation,Manufacturing execution systems,Multi-agent simulation,Production control},
  number = {1}
}

@article{rontsis_distributionally_2017,
  title = {Distributionally {{Ambiguous Optimization Techniques}} for {{Batch Bayesian Optimization}}},
  author = {Rontsis, Nikitas and Osborne, Michael A. and Goulart, Paul J.},
  year = {2017},
  month = jul,
  abstract = {We propose a novel, theoretically-grounded, acquisition function for Batch Bayesian optimization informed by insights from distributionally ambiguous optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function, which requires evaluation of a Gaussian Expectation over a multivariate piecewise affine function. Our bound is computed instead by evaluating the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. Unlike alternative approaches, including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be computed exactly - even on large batch sizes - as the solution of a tractable convex optimization problem. Our suggested acquisition function can also be optimized efficiently, since first and second derivative information can be calculated inexpensively as by-products of the acquisition function calculation itself. We derive various novel theorems that ground our work theoretically and we demonstrate superior performance via simple motivating examples, benchmark functions and real-world problems.},
  archivePrefix = {arXiv},
  eprint = {1707.04191},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/rontsis et al_2017_distributionally ambiguous optimization techniques for batch bayesian.pdf;/home/juan/Zotero/storage/ABKETV9I/1707.html},
  journal = {arXiv:1707.04191 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@inproceedings{rossi_bio-inspired_2008,
  title = {Bio-Inspired {{Optimization Techniques}} for {{SVM Parameter Tuning}}},
  booktitle = {2008 10th {{Brazilian Symposium}} on {{Neural Networks}}},
  author = {Rossi, A. L. D. and d Carvalho, A. C. P. L. F.},
  year = {2008},
  month = oct,
  pages = {57--62},
  doi = {10.1109/SBRN.2008.28},
  abstract = {Machine learning techniques have been successfully applied to a large number of classification problems. Among these techniques, support vector machines (SVMs) are well know for the good classification accuracies reported in several studies. However, like many machine learning techniques, the classification performance obtained by SVMs is influenced by the choice of proper values for their free parameters. In this paper, we investigate what is the influence of different optimization techniques inspired by biology when they are used to optimize the free parameters of SVMs. This comparative study also included the default values suggested in the literature for the free parameters and a grid algorithm used for parameter tuning. The results obtained suggest that, although SVMs work well with the default values, they can benefit from the use of an optimization technique for parameter tuning.},
  file = {/home/juan/Zotero/storage/HERZBRRJ/4665892.html},
  keywords = {Ant colony optimization,bio-inspired,bioinspired optimization techniques,Biological system modeling,biology computing,Cells (biology),Gene expression,Genetic algorithms,learning (artificial intelligence),machine learning,machine learning techniques,Neural networks,optimisation,optimization techniques,parameter tuning,Particle swarm optimization,pattern classification,support vector machines,svm,SVM parameter tuning}
}

@article{rucksties_exploring_2010,
  title = {Exploring Parameter Space in Reinforcement Learning},
  author = {R{\"u}ckstie{\ss}, Thomas and Sehnke, Frank and Schaul, Tom and Wierstra, Daan and Sun, Yi and Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = mar,
  volume = {1},
  pages = {14--24},
  issn = {2080-9778},
  doi = {10.2478/s13230-010-0002-4},
  abstract = {This paper discusses parameter-based exploration methods for reinforcement learning. Parameter-based methods perturb parameters of a general function approximator directly, rather than adding noise to the resulting actions. Parameter-based exploration unifies reinforcement learning and black-box optimization, and has several advantages over action perturbation. We review two recent parameter-exploring algorithms: Natural Evolution Strategies and Policy Gradients with Parameter-Based Exploration. Both outperform state-of-the-art algorithms in several complex high-dimensional tasks commonly found in robot control. Furthermore, we describe how a novel exploration method, State-Dependent Exploration, can modify existing algorithms to mimic exploration in parameter space.},
  file = {/home/juan/Zotero/storage/ESIQZK68/10.html},
  journal = {Paladyn},
  language = {en},
  number = {1}
}

@article{rumelhart_learning_1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  volume = {323},
  pages = {533--536},
  issn = {0028-0836},
  doi = {10.1038/323533a0},
  copyright = {\textcopyright{} 1986 Nature Publishing Group},
  file = {/home/juan/Zotero/storage/5U5NAPDM/323533a0.html},
  journal = {Nature},
  language = {en},
  number = {6088}
}

@techreport{rummery_-line_1994,
  title = {On-{{Line Q}}-{{Learning Using Connectionist Systems}}},
  author = {Rummery, G. A. and Niranjan, M.},
  year = {1994},
  institution = {{Cambridge University Engineering Department}},
  abstract = {Reinforcement learning algorithms are a powerful machine learning technique. However, much of the work on these algorithms has been developed with regard to discrete finite-state Markovian problems, which is too restrictive for many real-world environments. Therefore, it is desirable to extend these methods to high dimensional continuous state-spaces, which requires the use of function approximation to generalise the information learnt by the system. In this report, the use of back-propagation neural networks (Rumelhart, Hinton and Williams 1986) is considered in this context. We consider a number of different algorithms based around Q-Learning (Watkins 1989) combined with the Temporal Difference algorithm (Sutton 1988), including a new algorithm (Modified Connectionist Q-Learning), and Q() (Peng and Williams 1994). In addition, we present algorithms for applying these updates on-line during trials, unlike backward replay used by Lin (1993) that requires waiting until the end of each t...},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/rummery_niranjan_1994_on-line q-learning using connectionist systems.pdf;/home/juan/Zotero/storage/RG3CTH76/summary.html},
  number = {166},
  type = {{{CUED}}/{{F}}-{{INFENG}}/{{TR}}}
}

@book{russell_artificial_2010,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter and Davis, Ernest},
  year = {2010},
  edition = {3rd ed},
  publisher = {{Prentice Hall}},
  address = {{Upper Saddle River}},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/russell et al_2010_artificial intelligence.pdf},
  isbn = {978-0-13-604259-4},
  keywords = {Artificial intelligence},
  lccn = {Q335 .R86 2010},
  series = {Prentice {{Hall}} Series in Artificial Intelligence}
}

@inproceedings{santoro_meta-learning_2016,
  title = {Meta-{{Learning}} with {{Memory}}-{{Augmented Neural Networks}}},
  booktitle = {{{PMLR}}},
  author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  year = {2016},
  month = jun,
  pages = {1842--1850},
  abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/santoro et al_2016_meta-learning with memory-augmented neural networks.pdf;/home/juan/Zotero/storage/6PEAA4M7/santoro16.html},
  language = {en}
}

@article{santos_dyna-h_2012,
  title = {Dyna-{{H}}: {{A}} Heuristic Planning Reinforcement Learning Algorithm Applied to Role-Playing Game Strategy Decision Systems},
  shorttitle = {Dyna-},
  author = {Santos, Matilde and Mart{\'i}n H., Jos{\'e} Antonio and L{\'o}pez, Victoria and Botella, Guillermo},
  year = {2012},
  month = aug,
  volume = {32},
  pages = {28--36},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2011.09.008},
  abstract = {In a role-playing game, finding optimal trajectories is one of the most important tasks. In fact, the strategy decision system becomes a key component of a game engine. Determining the way in which decisions are taken (e.g. online, batch or simulated) and the consumed resources in decision making (e.g. execution time, memory) will influence, to a major degree, the game performance. When classical search algorithms such as A{${_\ast}$} can be used, they are the very first option. Nevertheless, such methods rely on precise and complete models of the search space so there are many interesting scenarios where its application is not possible, and hence, model free methods for sequential decision making under uncertainty are the best choice. In this paper, we propose a heuristic planning strategy to incorporate, into a Dyna agent, the ability of heuristic-search in path-finding. The proposed Dyna- H algorithm selects branches more likely to produce outcomes than other branches, just as A{${_\ast}$} does. However, unlike A{${_\ast}$}, it has the advantages of a model-free online reinforcement learning algorithm. We evaluate our proposed algorithm against the one-step Q-learning and Dyna-Q algorithms and found that the Dyna- H , with its advantages, produced clearly superior results.},
  file = {/home/juan/Zotero/storage/3A8FASIJ/S0950705111002097.html},
  journal = {Knowledge-Based Systems},
  keywords = {A-star,Decision-making,Heuristic-search,Path-finding,Reinforcement-learning},
  series = {New {{Trends}} on {{Intelligent Decision Support Systems}}}
}

@article{schaul_prioritized_2015,
  ids = {schaul\_prioritized\_2015},
  title = {Prioritized {{Experience Replay}}},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  year = {2015},
  month = nov,
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new stateof-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  archivePrefix = {arXiv},
  eprint = {1511.05952},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/schaul et al_2015_prioritized experience replay.pdf;/home/juan/Zotero/storage/MHSP7EFP/Schaul et al. - 2015 - Prioritized Experience Replay.pdf;/home/juan/Zotero/storage/K5C48ATH/1511.html},
  journal = {arXiv:1511.05952 [cs]},
  keywords = {Computer Science - Learning,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@techreport{schmidhuber_simple_1999,
  title = {Simple {{Principles Of Metalearning}}},
  author = {Schmidhuber, J and Zhao, Jieyu and Wiering, Marco},
  year = {1999},
  month = feb,
  abstract = {The goal of metalearning is to generate useful shifts of inductive bias by adapting the current learning strategy in a "useful" way. Our learner leads a single life during which actions are continually executed according to the system's internal state and current policy (a modifiable, probabilistic algorithm mapping environmental inputs and internal states to outputs and new internal states). An action is considered a learning algorithm if it can modify the policy. Effects of learning processes on later learning processes are measured using reward/time ratios. Occasional backtracking enforces success histories of still valid policy modifications corresponding to histories of lifelong reward accelerations. The principle allows for plugging in a wide variety of learning algorithms. In particular, it allows for embedding the learner's policy modification strategy within the policy itself (self-reference). To demonstrate the principle's feasibility in cases where conventional reinforcemen...},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/schmidhuber et al_1999_simple principles of metalearning.pdf;/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/schmidhuber et al_1999_simple principles of metalearning2.pdf;/home/juan/Zotero/storage/8ZKREMW6/summary.html}
}

@article{schrittwieser_mastering_2019,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2019},
  month = nov,
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the actionselection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archivePrefix = {arXiv},
  eprint = {1911.08265},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/25DFFFXU/Schrittwieser et al. - 2019 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf},
  journal = {arXiv:1911.08265 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{schulman_proximal_2017,
  ids = {schulman\_proximal\_2017-1},
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = jul,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archivePrefix = {arXiv},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/schulman et al_2017_proximal policy optimization algorithms.pdf;/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/schulman et al_2017_proximal policy optimization algorithms2.pdf;/home/juan/Zotero/storage/5H666N85/1707.html;/home/juan/Zotero/storage/KP8AUEHU/1707.html},
  journal = {arXiv:1707.06347 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{schulman_trust_2015-1,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2015},
  month = feb,
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archivePrefix = {arXiv},
  eprint = {1502.05477},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/schulman et al_2015_trust region policy optimization.pdf;/home/juan/Zotero/storage/RILLYSPW/1502.html},
  journal = {arXiv:1502.05477 [cs]},
  keywords = {Computer Science - Learning,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{schweighofer_meta-learning_2003,
  title = {Meta-Learning in {{Reinforcement Learning}}},
  author = {Schweighofer, Nicolas and Doya, Kenji},
  year = {2003},
  month = jan,
  volume = {16},
  pages = {5--9},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(02)00228-9},
  abstract = {Meta-parameters in reinforcement learning should be tuned to the environmental dynamics and the animal performance. Here, we propose a biologically plausible meta-reinforcement learning algorithm for tuning these meta-parameters in a dynamic, adaptive manner. We tested our algorithm in both a simulation of a Markov decision task and in a non-linear control task. Our results show that the algorithm robustly finds appropriate meta-parameter values, and controls the meta-parameter time course, in both static and dynamic environments. We suggest that the phasic and tonic components of dopamine neuron firing can encode the signal required for meta-learning of reinforcement learning.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/schweighofer_doya_2003_meta-learning in reinforcement learning.pdf;/home/juan/Zotero/storage/SV6NPAUX/S0893608002002289.html},
  journal = {Neural Networks},
  keywords = {Dopamine,Dynamic environment,Meta-learning,Meta-parameters,Neuromodulation,Reinforcement learning,TD error},
  number = {1}
}

@article{sciuto_evaluating_2019,
  title = {Evaluating the {{Search Phase}} of {{Neural Architecture Search}}},
  author = {Sciuto, Christian and Yu, Kaicheng and Jaggi, Martin and Musat, Claudiu and Salzmann, Mathieu},
  year = {2019},
  month = feb,
  abstract = {Neural Architecture Search (NAS) aims to facilitate the design of deep networks for new tasks. Existing techniques rely on two stages: searching over the architecture space and validating the best architecture. NAS algorithms are currently evaluated solely by comparing their results on the downstream task. While intuitive, this fails to explicitly evaluate the effectiveness of their search strategies. In this paper, we present a NAS evaluation framework that includes the search phase. To this end, we compare the quality of the solutions obtained by NAS search policies with that of random architecture selection. We find that: (i) On average, the random policy outperforms state-of-the-art NAS algorithms; (ii) The results and candidate rankings of NAS algorithms do not reflect the true performance of the candidate architectures; and (iii) The widely used weight sharing strategy negatively impacts the training of good architectures, thus reducing the effectiveness of the search process. We believe that following our evaluation framework will be key to designing NAS strategies that truly discover superior architectures.},
  archivePrefix = {arXiv},
  eprint = {1902.08142},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/sciuto et al_2019_evaluating the search phase of neural architecture search.pdf;/home/juan/Zotero/storage/SAPALVVH/1902.html},
  journal = {arXiv:1902.08142 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{sculley_hidden_nodate,
  title = {Hidden {{Technical Debt}} in {{Machine Learning Systems}}},
  author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Fran{\c c}ois and Dennison, Dan},
  pages = {9},
  abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
  file = {/home/juan/Zotero/storage/UE5G52GV/Sculley et al. - Hidden Technical Debt in Machine Learning Systems.pdf},
  language = {en}
}

@article{sculley_machine_nodate,
  title = {Machine {{Learning}}: {{The High}}-{{Interest Credit Card}} of {{Technical Debt}}},
  author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
  pages = {9},
  abstract = {Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
  file = {/home/juan/Zotero/storage/S2QJZ7T8/Sculley et al. - Machine Learning The High-Interest Credit Card of.pdf},
  language = {en}
}

@misc{seita_assessing_nodate,
  title = {Assessing {{Generalization}} in {{Deep Reinforcement Learning}}},
  author = {Seita, Daniel},
  abstract = {The BAIR Blog},
  file = {/home/juan/Zotero/storage/QJF9CZVQ/rl-generalization.html},
  howpublished = {http://bair.berkeley.edu/blog/2019/03/18/rl-generalization/},
  journal = {The Berkeley Artificial Intelligence Research Blog}
}

@article{shahriari_taking_2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, B. and Swersky, K. and Wang, Ziyu and Adams, R.P. and {de Freitas}, N.},
  year = {2016},
  month = jan,
  volume = {104},
  pages = {148--175},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2015.2494218},
  abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/shahriari et al_2016_taking the human out of the loop.pdf;/home/juan/Zotero/storage/2IWBNB5W/articleDetails.html},
  journal = {Proceedings of the IEEE},
  keywords = {Bayes methods,Bayesian optimization,Big Data,Big data application,Decision making,Design of experiments,Genomes,genomic medicine,human productivity,large-scale heterogeneous computing,Linear programming,massive complex software system,optimisation,Optimization,product quality,response surface methodology,Statistical analysis,statistical learning,storage allocation,storage architecture},
  number = {1}
}

@article{shalev-shwartz_failures_2017,
  title = {Failures of {{Gradient}}-{{Based Deep Learning}}},
  author = {{Shalev-Shwartz}, Shai and Shamir, Ohad and Shammah, Shaked},
  year = {2017},
  month = mar,
  abstract = {In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradient-based algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied1.},
  archivePrefix = {arXiv},
  eprint = {1703.07950},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/YRMN2D56/Shalev-Shwartz et al. - 2017 - Failures of Gradient-Based Deep Learning.pdf},
  journal = {arXiv:1703.07950 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{shalev-shwartz_safe_2016,
  title = {Safe, {{Multi}}-{{Agent}}, {{Reinforcement Learning}} for {{Autonomous Driving}}},
  author = {{Shalev-Shwartz}, Shai and Shammah, Shaked and Shashua, Amnon},
  year = {2016},
  month = oct,
  abstract = {Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an "Option Graph" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further.},
  archivePrefix = {arXiv},
  eprint = {1610.03295},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/shalev-shwartz et al_2016_safe, multi-agent, reinforcement learning for autonomous driving.pdf;/home/juan/Zotero/storage/RHS23ZGN/1610.html},
  journal = {arXiv:1610.03295 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{silver_deterministic_2014,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year = {2014},
  pages = {9},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/silver et al_2014_deterministic policy gradient algorithms.pdf},
  language = {en}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {0028-0836},
  doi = {10.1038/nature16961},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  copyright = {\textcopyright{} 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/silver et al_2016_mastering the game of go with deep neural networks and tree search.pdf;/home/juan/Zotero/storage/C3V4CK95/nature16961.html},
  journal = {Nature},
  keywords = {Computational science,Computer science,Reward},
  language = {en},
  number = {7587}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  volume = {550},
  pages = {354},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
  copyright = {2017 Nature Publishing Group},
  file = {/home/juan/Zotero/storage/72E6QU54/nature24270.html},
  journal = {Nature},
  language = {En},
  number = {7676}
}

@article{silver_mastering_2017-1,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self}}-{{Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  month = dec,
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archivePrefix = {arXiv},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/silver et al_2017_mastering chess and shogi by self-play with a general reinforcement learning.pdf;/home/juan/Zotero/storage/PP7QJ6GB/1712.html},
  journal = {arXiv:1712.01815 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning},
  primaryClass = {cs}
}

@article{smart_practical_2000,
  title = {Practical {{Reinforcement Learning}} in {{Continuous Spaces}}},
  author = {Smart, William D and Kaelbling, L. P.},
  year = {2000},
  month = jun,
  pages = {8},
  doi = {10.1.1.97.9314},
  abstract = {Dynamic control tasks are good candidates for the application of reinforcement learning techniques. However, many of these tasks inherently have continuous state or action variables. This can cause problems for traditional reinforcement learning algorithms which assume discrete states and actions. In this paper, we introduce an algorithm that safely approximates the value function for continuous state control tasks, and that learns quickly from a small amount of data. We give experimental results using this algorithm to learn policies for both a simulated task and also for a real robot, operating in an unaltered environment. The algorithm works well in a traditional learning setting, and demonstrates extremely good learning when bootstrapped with a small amount of human-provided data.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/smart_kaelbling_2000_practical reinforcement learning in continuous spaces.pdf},
  language = {en}
}

@inproceedings{snelson_sparse_2006,
  title = {Sparse {{Gaussian Processes}} Using {{Pseudo}}-Inputs},
  booktitle = {{{ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS}}},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  year = {2006},
  pages = {1257--1264},
  publisher = {{MIT press}},
  abstract = {We present a new Gaussian process (GP) regression model whose covariance  is parameterized by the the locations of M pseudo-input points,  which we learn by a gradient based optimization. We take M    N ,  where N is the number of real data points, and hence obtain a sparse  regression method which has    N) training cost and    ) prediction  cost per test case. We also find hyperparameters of the covariance  function in the same joint optimization. The method can be viewed  as a Bayesian regression model with particular input dependent noise. The method turns},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/snelson_ghahramani_2006_sparse gaussian processes using pseudo-inputs.pdf;/home/juan/Zotero/storage/QPUHM58A/summary.html}
}

@incollection{snoek_practical_2012,
  title = {Practical {{Bayesian Optimization}} of {{Machine Learning Algorithms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {2951--2959},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/snoek et al_2012_practical bayesian optimization of machine learning algorithms.pdf}
}

@article{srinivas_gaussian_2010,
  title = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}: {{No Regret}} and {{Experimental Design}}},
  shorttitle = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
  year = {2010},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/srinivas et al_2010_gaussian process optimization in the bandit setting.pdf;/home/juan/Zotero/storage/PU5UMR5X/161305.html},
  journal = {Proceedings of the 27th International Conference on Machine Learning}
}

@article{stadie_considerations_2018,
  title = {Some {{Considerations}} on {{Learning}} to {{Explore}} via {{Meta}}-{{Reinforcement Learning}}},
  author = {Stadie, Bradly C. and Yang, Ge and Houthooft, Rein and Chen, Xi and Duan, Yan and Wu, Yuhuai and Abbeel, Pieter and Sutskever, Ilya},
  year = {2018},
  month = mar,
  abstract = {We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-\$\textbackslash{}text\{RL\}\^2\$. Results are presented on a novel environment we call `Krazy World' and a set of maze environments. We show E-MAML and E-\$\textbackslash{}text\{RL\}\^2\$ deliver better performance on tasks where exploration is important.},
  archivePrefix = {arXiv},
  eprint = {1803.01118},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/stadie et al_2018_some considerations on learning to explore via meta-reinforcement learning.pdf;/home/juan/Zotero/storage/KSQ9PJ6G/1803.html},
  journal = {arXiv:1803.01118 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@inproceedings{sutton_integrated_1990,
  title = {Integrated {{Architectures}} for {{Learning}}, {{Planning}}, and {{Reacting Based}} on {{Approximating Dynamic Programming}}},
  booktitle = {In {{Proceedings}} of the {{Seventh International Conference}} on {{Machine Learning}}},
  author = {Sutton, Richard S.},
  year = {1990},
  pages = {216--224},
  publisher = {{Morgan Kaufmann}},
  abstract = {This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement a...},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/sutton_1990_integrated architectures for learning, planning, and reacting based on.pdf;/home/juan/Zotero/storage/ZXA7A6IT/summary.html}
}

@inproceedings{sutton_planning_1991,
  title = {Planning by {{Incremental Dynamic Programming}}},
  booktitle = {In {{Proceedings}} of the {{Eighth International Workshop}} on {{Machine Learning}}},
  author = {Sutton, Richard},
  year = {1991},
  pages = {353--357},
  publisher = {{Morgan Kaufmann}}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {2nd},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Computer Science/Reinforcement Learning/Sutton and Barto - 2018 - Reinforcement Learning An Introduction.pdf},
  isbn = {978-0-262-19398-6},
  keywords = {Reinforcement learning},
  language = {English},
  lccn = {Q325.6 .S88 1998},
  series = {Adaptive {{Computation}} and {{Machine Learning}}}
}

@inproceedings{sutton_step-size_1994,
  title = {On {{Step}}-{{Size}} and {{Bias}} in {{Temporal}}-{{Difference Learning}}},
  booktitle = {Proceedings of the {{Eighth Yale Workshop}} on {{Adaptive}} and {{Learning Systems}}},
  author = {Sutton, Richard and Singh, Satinder P.},
  year = {1994},
  pages = {91--96},
  abstract = {We present results for three new algorithms for setting the step-size parameters, ff and , of temporaldifference learning methods such as TD(). The overall task is that of learning to predict the outcome of an unknown Markov chain based on repeated observations of its state trajectories. The new algorithms select step-size parameters online in such a way as to eliminate the bias normally inherent in temporaldifference methods. We compare our algorithms with conventional Monte Carlo methods. Monte Carlo methods have a natural way of setting the step size: for each state s they use a step size of 1=n s , where  n s is the number of times state s has been visited. We seek and come close to achieving comparable stepsize algorithms for TD(). One new algorithm uses a   = 1=n s schedule to achieve the same effect as processing a state backwards with TD(0), but remains completely incremental. Another algorithm uses a   at each time equal to the estimated transition probability of the current t...},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/sutton_singh_1994_on step-size and bias in temporal-difference learning.pdf;/home/juan/Zotero/storage/48XZ8RYD/summary.html}
}

@article{swersky_freeze-thaw_2014,
  title = {Freeze-{{Thaw Bayesian Optimization}}},
  author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  year = {2014},
  month = jun,
  abstract = {In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice.},
  archivePrefix = {arXiv},
  eprint = {1406.3896},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/swersky et al_2014_freeze-thaw bayesian optimization.pdf;/home/juan/Zotero/storage/7DMV2XZG/1406.html},
  journal = {arXiv:1406.3896 [cs, stat]},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{szeliski_computer_2011,
  title = {Computer {{Vision}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Computer {{Vision}}},
  author = {Szeliski, Richard},
  year = {2011},
  publisher = {{Springer-Verlag}},
  address = {{London}},
  abstract = {Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of ``recipes,'' this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features: Structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses Presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects Provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory Suggests additional reading at the end of each chapter, including the latest research in each sub-field, in addition to a full Bibliography at the end of the book Supplies supplementary course material for students at the associated website, http://szeliski.org/Book/ Suitable for an upper-level undergraduate or graduate-level course in computer science or engineering, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries. Its design and exposition also make it eminently suitable as a unique reference to the fundamental techniques and current research literature in computer vision. Dr. Richard Szeliski has more than 25 years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft Research. This text draws on that experience, as well as on computer vision courses he has taught at the University of Washington and Stanford.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/szeliski_2011_computer vision.pdf;/home/juan/Zotero/storage/GSZTHDWK/9781848829343.html},
  isbn = {978-1-84882-934-3},
  language = {en},
  series = {Texts in {{Computer Science}}}
}

@article{tesauro_temporal_1995,
  title = {Temporal {{Difference Learning}} and {{TD}}-{{Gammon}}},
  author = {Tesauro, Gerald},
  year = {1995},
  month = mar,
  volume = {38},
  pages = {58--68},
  issn = {0001-0782},
  doi = {10.1145/203330.203343},
  abstract = {Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.},
  journal = {Commun. ACM},
  number = {3}
}

@inproceedings{tesch_expensive_2013,
  title = {Expensive {{Function Optimization}} with {{Stochastic Binary Outcomes}}},
  booktitle = {Proceedings of {{The}} 30th {{International Conference}} on {{Machine Learning}}},
  author = {Tesch, Matthew and Schneider, Jeff and Choset, Howie},
  year = {2013},
  pages = {1283--1291},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/tesch et al_2013_expensive function optimization with stochastic binary outcomes.pdf;/home/juan/Zotero/storage/3JHU6KJH/tesch13.html}
}

@phdthesis{tesch_improving_2013,
  title = {Improving {{Robot Locomotion Through Learning Methods}} for {{Expensive Black}}-{{Box Systems}}},
  author = {Tesch, Matthew},
  year = {2013},
  month = nov,
  copyright = {Approved for public release; distribution is unlimited.},
  file = {/home/juan/Zotero/storage/T83JMFRS/oai.html},
  keywords = {*LOCOMOTION,*ROBOTICS,Cybernetics,DENSITY FUNCTIONAL THEORY,DYNAMICS,EXPERIMENTAL DESIGN,GAUSSIAN NOISE,INFORMATION PROCESSING,KINEMATICS,LEARNING MACHINES,LOCOMOTIVE TASKS,MAXIMUM LIKELIHOOD ESTIMATION,MULTISENSORS,Navigation and Guidance,NAVIGATIONAL AIDS,Optimization,PATTERN RECOGNITION,PE611102A,PROBABILITY DENSITY FUNCTIONS,SNAKE ROBOTS,Statistics and Probability,STOCHASTIC PROCESSES,TARGET CLASSIFICATION,TEST AND EVALUATION,THESES,TRAJECTORIES},
  language = {en}
}

@inproceedings{thornton_auto-weka_2013,
  title = {Auto-{{WEKA}}: {{Combined Selection}} and {{Hyperparameter Optimization}} of {{Classification Algorithms}}},
  shorttitle = {Auto-{{WEKA}}},
  booktitle = {Proceedings of the 19th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2013},
  pages = {847--855},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2487575.2487629},
  abstract = {Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/thornton et al_2013_auto-weka.pdf},
  isbn = {978-1-4503-2174-7},
  keywords = {hyperparameter optimization,model selection,weka},
  series = {{{KDD}} '13}
}

@incollection{thrun_learning_1998,
  title = {Learning to {{Learn}}: {{Introduction}} and {{Overview}}},
  shorttitle = {Learning to {{Learn}}},
  booktitle = {Learning to {{Learn}}},
  author = {Thrun, Sebastian and Pratt, Lorien},
  year = {1998},
  pages = {3--17},
  publisher = {{Springer, Boston, MA}},
  doi = {10.1007/978-1-4615-5529-2_1},
  abstract = {Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications (see e.g., [Langley, 1992; Widrow et al., 1994]).},
  file = {/home/juan/Zotero/storage/CN4F4S8W/978-1-4615-5529-2_1.html},
  isbn = {978-1-4613-7527-2 978-1-4615-5529-2},
  language = {en}
}

@article{van_hasselt_deep_2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q}}-Learning},
  author = {{van Hasselt}, Hado and Guez, Arthur and Silver, David},
  year = {2015},
  month = sep,
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  archivePrefix = {arXiv},
  eprint = {1509.06461},
  eprinttype = {arxiv},
  file = {/home/juan/Zotero/storage/ZMBXYBM4/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf},
  journal = {arXiv:1509.06461 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{van_hasselt_deep_2018,
  title = {Deep {{Reinforcement Learning}} and the {{Deadly Triad}}},
  author = {{van Hasselt}, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  year = {2018},
  month = dec,
  abstract = {We know from reinforcement learning theory that temporal difference learning can fail in certain cases. Sutton and Barto (2018) identify a deadly triad of function approximation, bootstrapping, and off-policy learning. When these three properties are combined, learning can diverge with the value estimates becoming unbounded. However, several algorithms successfully combine these three properties, which indicates that there is at least a partial gap in our understanding. In this work, we investigate the impact of the deadly triad in practice, in the context of a family of popular deep reinforcement learning models - deep Q-networks trained with experience replay - analysing how the components of this system play a role in the emergence of the deadly triad, and in the agent's performance},
  archivePrefix = {arXiv},
  eprint = {1812.02648},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/van hasselt et al_2018_deep reinforcement learning and the deadly triad.pdf;/home/juan/Zotero/storage/DK2GH94D/1812.html},
  journal = {arXiv:1812.02648 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{van_seijen_true_2016,
  title = {True {{Online Temporal}}-{{Difference Learning}}},
  author = {{van Seijen}, Harm and Mahmood, Ashique Rupam and Pilarski, Patrick and C. Machado, Marlos and Sutton, Richard},
  year = {2016},
  month = sep,
  volume = {17},
  pages = {1},
  abstract = {The temporal-difference methods TD(lambda) and Sarsa(lambda) form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD(lambda) and true online Sarsa(lambda), respectively (van Seijen \& Sutton, 2014). Algorithmically, these true online methods only make two small changes to the update rules of the regular methods, and the extra computational cost is negligible in most cases. However, they follow the ideas underlying the forward view much more closely. In particular, they maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD(lambda)/Sarsa(lambda) with regular TD(lambda)/Sarsa(lambda) on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods.
Besides the empirical results, we provide an in-depth analysis of the theory behind true online temporal-difference learning. In addition, we show that new true online temporal-difference methods can be derived by making changes to the online forward view and then rewriting the update equations.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/van seijen et al_2016_true online temporal-difference learning.pdf},
  journal = {Journal of Machine Learning Research}
}

@article{van_zyl_frequentist_2018,
  title = {Frequentist and {{Bayesian}} Inference: {{A}} Conceptual Primer},
  shorttitle = {Frequentist and {{Bayesian}} Inference},
  author = {Van Zyl, Casper},
  year = {2018},
  month = jul,
  volume = {51},
  doi = {10.1016/j.newideapsych.2018.06.004},
  abstract = {In recent years, there has been a crisis of confidence in many empirical fields including psychology, regarding the reproducibility of scientific findings. Among several causes thought to have contributed to this situation, the inferential basis of traditional, or so-called frequentist statistics, is arguably chief among them. Of particular concern is null hypothesis significance testing (NHST), which inadvertently became the de facto basis of scientific inference in the frequentist paradigm. The objective of this paper is to describe some of the most prominent issues plaguing frequentist inference, including NHST. In addition, some Bayesian benefits are introduced to show that it offers solutions to several problems inherent in frequentist statistics. The overall aim is to provide a non-threatening, conceptual overview of these concerns. The hope is that this will facilitate greater awareness and understanding of the need to address these matters in empirical psychology.},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/van zyl_2018_frequentist and bayesian inference.pdf},
  journal = {New Ideas in Psychology}
}

@book{vanderplas_python_2016,
  title = {Python {{Data Science Handbook}}},
  author = {VanderPlas, Jake},
  year = {2016},
  month = nov,
  abstract = {For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data science stack, but only with the Python Data...},
  file = {/home/juan/Zotero/storage/QZL6JRBB/0636920034919.html},
  isbn = {978-1-4919-1205-8},
  language = {en}
}

@article{venkatesh_high_2017,
  title = {High {{Dimensional Bayesian Optimization}} Using {{Dropout}}},
  author = {Venkatesh, Svetha and Li, Cheng and Gupta, Sunil and Nguyen, Vu and Rana, Santu and Shilton, Alistair},
  year = {2017},
  pages = {2096--2102},
  abstract = {Electronic proceedings of IJCAI 2017},
  file = {/home/juan/Zotero/storage/ANX247EP/291.html}
}

@article{vieira_rescheduling_2003,
  title = {Rescheduling {{Manufacturing Systems}}: {{A Framework}} of {{Strategies}}, {{Policies}}, and {{Methods}}},
  shorttitle = {Rescheduling {{Manufacturing Systems}}},
  author = {Vieira, Guilherme E. and Herrmann, Jeffrey W. and Lin, Edward},
  year = {2003},
  month = jan,
  volume = {6},
  pages = {39--62},
  issn = {1094-6136, 1099-1425},
  doi = {10.1023/A:1022235519958},
  abstract = {Many manufacturing facilities generate and update production schedules, which are plans that state when certain controllable activities (e.g., processing of jobs by resources) should take place. Production schedules help managers and supervisors coordinate activities to increase productivity and reduce operating costs. Because a manufacturing system is dynamic and unexpected events occur, rescheduling is necessary to update a production schedule when the state of the manufacturing system makes it infeasible. Rescheduling updates an existing production schedule in response to disruptions or other changes. Though many studies discuss rescheduling, there are no standard definitions or classification of the strategies, policies, and methods presented in the rescheduling literature. This paper presents definitions appropriate for most applications of rescheduling manufacturing systems and describes a framework for understanding rescheduling strategies, policies, and methods. This framework is based on a wide variety of experimental and practical approaches that have been described in the rescheduling literature. The paper also discusses studies that show how rescheduling affects the performance of a manufacturing system, and it concludes with a discussion of how understanding rescheduling can bring closer some aspects of scheduling theory and practice.},
  file = {/home/juan/Zotero/storage/7ZR3G3Z8/A1022235519958.html},
  journal = {Journal of Scheduling},
  keywords = {Artificial Intelligence (incl. Robotics),Calculus of Variations and Optimal Control,dynamic scheduling,Operation Research/Decision Theory,Optimization,predictive-reactive scheduling,Production/Logistics,Rescheduling},
  language = {en},
  number = {1}
}

@article{vinyals_grandmaster_2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = oct,
  pages = {1--5},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1\textendash{}3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players. AlphaStar uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2\% of human players for the real-time strategy game StarCraft II.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  file = {/home/juan/Zotero/storage/Q25T8K2C/s41586-019-1724-z.html},
  journal = {Nature},
  language = {en}
}

@article{volpp_meta-learning_2019,
  title = {Meta-{{Learning Acquisition Functions}} for {{Bayesian Optimization}}},
  author = {Volpp, Michael and Fr{\"o}hlich, Lukas and Doerr, Andreas and Falkner, Stefan and Hutter, Frank and Daniel, Christian},
  year = {2019},
  month = apr,
  abstract = {Many practical applications of machine learning, such as tuning hyperparameters or process settings, rely on data-efficient black-box function optimization. Readily available algorithms are typically designed to be universal optimizers and, thus, often suboptimal for specific tasks. We propose a method to meta-learn customized optimizers within the well-established framework of Bayesian optimization (BO), allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Indeed, only the acquisition function (AF) is replaced by a learned neural network, while retaining the standard BO optimization loop. The approach is evaluated on several simulated tasks, two hyperparameter search problems, as well as a sim-to-real transfer experiment. The results show that the learned optimizers (1) perform better than or on-par with classical AFs on general function classes, (2) automatically identify structural properties of a function class on available source tasks or cheap simulations and (3) are able to transfer this knowledge to new problems, thereby significantly outperforming existing hand-engineered AFs.},
  archivePrefix = {arXiv},
  eprint = {1904.02642},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/volpp et al_2019_meta-learning acquisition functions for bayesian optimization.pdf;/home/juan/Zotero/storage/QJVHW8B9/1904.html},
  journal = {arXiv:1904.02642 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@phdthesis{wang_bayesian_2017,
  title = {Bayesian {{Optimization}} with {{Parallel Function Evaluations}} and {{Multiple Information Sources}}: {{Methodology}} with {{Applications}} in {{Biochemistry}}, {{Aerospace}}, {{Engineering}}, and {{Machine Learning}}},
  author = {Wang, Jialei},
  year = {2017},
  abstract = {Bayesian optimization, a framework for global optimization of expensive-to-
evaluate functions, has recently gained popularity in machine learning and global optimization because it can find good feasible points with few function evaluations. In this dissertation, we present novel Bayesian optimization algorithms for problems with parallel function evaluations and multiple information sources, for use in machine learning, biochemistry, and aerospace engineering applications.
First, we present a novel algorithm that extends expected improvement, a
widely-used Bayesian optimization algorithm that evaluates one point at a time,
to settings with parallel function evaluations. This algorithm is based on a new efficient solution method for finding the Bayes-optimal set of points to evaluate next in the context of parallel Bayesian optimization. The author implemented this algorithm in an open source software package co-developed with engineers at Yelp, which was used by Yelp and Netflix for automatic tuning of hyperparameters in machine learning algorithms, and for choosing parameters in online content
delivery systems based on evaluations in A/B tests on live traffic.
Second, we present a novel parallel Bayesian optimization algorithm with a worst-case approximation guarantee applied to peptide optimization in biochemistry, where we face a large collection of peptides with unknown fitness prior to
experimentation, and our goal is to identify peptides with a high score using a small number of experiments. High scoring peptides can be used for biolabeling, targeted drug delivery, and self-assembly of metamaterials. This problem has two novelties: first, unlike traditional Bayesian optimization, where the objective function has a continuous domain and real-valued output well-modeled by a Gaussian Process, this problem has a discrete domain, and involves binary output not well-modeled by a Gaussian process; second, it uses hundreds of parallel function evaluations, which is a level of parallelism too large to be approached with other
previously-proposed parallel Bayesian optimization methods.
Third, we present a novel Bayesian optimization algorithm for problems in
which there are multiple methods or ``information sources'' for evaluating the ob-
jective function, each with its own bias, noise and cost of evaluation. For example,
in aerospace engineering, to evaluate an aircraft wing design, different computa-
tional models may simulate performance. Our algorithm explores the correlation
and model discrepancy of each information source, and optimally chooses the in-
formation source to evaluate next and the point at which to evaluate it. We describe how this algorithm can be used in general multi-information source optimization problems, and also how a related algorithm can be used in ``warm start'' problems, where we have results from previous ptimizations of closely related objective functions, and we wish to leverage these results to more quickly optimize a new objective function.},
  language = {English},
  school = {Cornell University}
}

@article{wang_dueling_2015,
  ids = {wang\_dueling\_2015},
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {van Hasselt}, Hado and Lanctot, Marc and {de Freitas}, Nando},
  year = {2015},
  month = nov,
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  archivePrefix = {arXiv},
  eprint = {1511.06581},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/wang et al_2015_dueling network architectures for deep reinforcement learning.pdf;/home/juan/Zotero/storage/TZ3E3TRJ/1511.html},
  journal = {arXiv:1511.06581 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{wang_knowledge_2016,
  title = {The {{Knowledge Gradient}} for {{Sequential Decision Making}} with {{Stochastic Binary Feedbacks}}},
  booktitle = {{{PMLR}}},
  author = {Wang, Yingfei and Wang, Chu and Powell, Warren},
  year = {2016},
  month = jun,
  pages = {1138--1147},
  abstract = {We consider the problem of sequentially making decisions that are rewarded by ``successes'' and ``failures'' which can be predicted through an unknown relationship that depends on a partially controlla...},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/wang et al_2016_the knowledge gradient for sequential decision making with stochastic binary.pdf;/home/juan/Zotero/storage/8VTCC3LV/wangb16.html},
  language = {en}
}

@article{wang_learning_2016,
  title = {Learning to Reinforcement Learn},
  author = {Wang, Jane X. and {Kurth-Nelson}, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  year = {2016},
  month = nov,
  abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
  archivePrefix = {arXiv},
  eprint = {1611.05763},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/wang et al_2016_learning to reinforcement learn.pdf;/home/juan/Zotero/storage/TR6A4P65/1611.html},
  journal = {arXiv:1611.05763 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{wang_parallel_2016,
  title = {Parallel Bayesian Global Optimization of Expensive Functions},
  author = {Wang, Jialei and Clark, Scott C and Liu, Eric and Frazier, Peter I},
  year = {2016},
  journal = {arXiv preprint arXiv:1602.05149}
}

@book{wasserman_all_2004,
  title = {All of Statistics: A Concise Course in Statistical Inference},
  shorttitle = {All of Statistics},
  author = {Wasserman, Larry},
  year = {2004},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-40272-7},
  keywords = {Mathematical statistics},
  lccn = {QA276.12 .W37 2003},
  series = {Springer Texts in Statistics}
}

@article{watkins_q-learning_1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  year = {1992},
  month = may,
  volume = {8},
  pages = {279--292},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00992698},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/watkins_dayan_1992_q-learning.pdf;/home/juan/Zotero/storage/3BIV8U54/BF00992698.html},
  journal = {Machine Learning},
  language = {en},
  number = {3-4}
}

@article{wawrzynski_autonomous_2013,
  title = {Autonomous Reinforcement Learning with Experience Replay},
  author = {Wawrzy{\'n}ski, Pawe{\l} and Tanwani, Ajay Kumar},
  year = {2013},
  month = may,
  volume = {41},
  pages = {156--167},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.11.007},
  abstract = {This paper considers the issues of efficiency and autonomy that are required to make reinforcement learning suitable for real-life control tasks. A real-time reinforcement learning algorithm is presented that repeatedly adjusts the control policy with the use of previously collected samples, and autonomously estimates the appropriate step-sizes for the learning updates. The algorithm is based on the actor\textendash{}critic with experience replay whose step-sizes are determined on-line by an enhanced fixed point algorithm for on-line neural network training. An experimental study with simulated octopus arm and half-cheetah demonstrates the feasibility of the proposed algorithm to solve difficult learning control problems in an autonomous way within reasonably short time.},
  file = {/home/juan/Zotero/storage/9484ZZJB/Wawrzyński and Tanwani - 2013 - Autonomous reinforcement learning with experience .pdf;/home/juan/Zotero/storage/X26DAB2F/S0893608012002936.html},
  journal = {Neural Networks},
  keywords = {Actor–critic,Autonomous learning,Reinforcement learning,Step-size estimation},
  language = {en}
}

@article{white_bananas_2019,
  title = {{{BANANAS}}: {{Bayesian Optimization}} with {{Neural Architectures}} for {{Neural Architecture Search}}},
  shorttitle = {{{BANANAS}}},
  author = {White, Colin and Neiswanger, Willie and Savani, Yash},
  year = {2019},
  month = oct,
  abstract = {Neural Architecture Search (NAS) has seen an explosion of research in the past few years. A variety of methods have been proposed to perform NAS, including reinforcement learning, Bayesian optimization with a Gaussian process model, evolutionary search, and gradient descent. In this work, we design a NAS algorithm that performs Bayesian optimization using a neural network model. We develop a path-based encoding scheme to featurize the neural architectures that are used to train the neural network model. This strategy is particularly effective for encoding architectures in cell-based search spaces. After training on just 200 random neural architectures, we are able to predict the validation accuracy of a new architecture to within one percent of its true accuracy on average, for popular search spaces. This may be of independent interest beyond Bayesian neural architecture search. We test our algorithm on the NASBench (Ying et al. 2019) and DARTS (Liu et al. 2018) search spaces, and we show that our algorithm outperforms other NAS methods including evolutionary search, reinforcement learning, AlphaX, ASHA, and DARTS. Our algorithm is over 100x more efficient than random search, and 3.8x more efficient than the next-best algorithm on the NASBench dataset. As there have been problems with fair and reproducible experimental evauations in the field of NAS, we adhere to the recent NAS research checklist (Lindauer and Hutter 2019) to facilitate NAS research. In particular, our implementation has been made publicly available, including all details needed to fully reproduce our results.},
  archivePrefix = {arXiv},
  eprint = {1910.11858},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/white et al_2019_bananas.pdf;/home/juan/Zotero/storage/GJTSGY3I/1910.html},
  journal = {arXiv:1910.11858 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{white_greedy_2016,
  title = {A {{Greedy Approach}} to {{Adapting}} the {{Trace Parameter}} for {{Temporal Difference Learning}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Autonomous Agents}} \& {{Multiagent Systems}}},
  author = {White, Martha and White, Adam},
  year = {2016},
  month = jul,
  abstract = {One of the main obstacles to broad application of reinforcement learning methods is the parameter sensitivity of our core learning algorithms. In many large-scale applications, online computation and function approximation represent key strategies in scaling up reinforcement learning algorithms. In this setting, we have effective and reasonably well understood algorithms for adapting the learning-rate parameter, online during learning. Such meta-learning approaches can improve robustness of learning and enable specialization to current task, improving learning speed. For temporal-difference learning algorithms which we study here, there is yet another parameter, \$\textbackslash{}lambda\$, that similarly impacts learning speed and stability in practice. Unfortunately, unlike the learning-rate parameter, \$\textbackslash{}lambda\$ parametrizes the objective function that temporal-difference methods optimize. Different choices of \$\textbackslash{}lambda\$ produce different fixed-point solutions, and thus adapting \$\textbackslash{}lambda\$ online and characterizing the optimization is substantially more complex than adapting the learning-rate parameter. There are no meta-learning method for \$\textbackslash{}lambda\$ that can achieve (1) incremental updating, (2) compatibility with function approximation, and (3) maintain stability of learning under both on and off-policy sampling. In this paper we contribute a novel objective function for optimizing \$\textbackslash{}lambda\$ as a function of state rather than time. We derive a new incremental, linear complexity \$\textbackslash{}lambda\$-adaption algorithm that does not require offline batch updating or access to a model of the world, and present a suite of experiments illustrating the practicality of our new algorithm in three different settings. Taken together, our contributions represent a concrete step towards black-box application of temporal-difference learning methods in real world problems.},
  archivePrefix = {arXiv},
  eprint = {1607.00446},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/white_white_2016_a greedy approach to adapting the trace parameter for temporal difference.pdf;/home/juan/Zotero/storage/73TNZZT8/1607.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning}
}

@article{wistuba_survey_2019,
  title = {A {{Survey}} on {{Neural Architecture Search}}},
  author = {Wistuba, Martin and Rawat, Ambrish and Pedapati, Tejaswini},
  year = {2019},
  month = may,
  abstract = {The growing interest in both the automation of machine learning and deep learning has inevitably led to the development of automated methods for neural architecture optimization. The choice of the network architecture has proven to be critical, and many advances in deep learning spring from its immediate improvements. However, deep learning techniques are computationally intensive and their application requires a high level of domain knowledge. Therefore, even partial automation of this process would help make deep learning more accessible to both researchers and practitioners. With this survey, we provide a formalism which unifies and categorizes the landscape of existing methods along with a detailed analysis that compares and contrasts the different approaches. We achieve this via a discussion of common architecture search spaces and architecture optimization algorithms based on principles of reinforcement learning and evolutionary algorithms along with approaches that incorporate surrogate and one-shot models. Additionally, we address the new research directions which include constrained and multi-objective architecture search as well as automated data augmentation, optimizer and activation function search.},
  archivePrefix = {arXiv},
  eprint = {1905.01392},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/wistuba et al_2019_a survey on neural architecture search.pdf;/home/juan/Zotero/storage/63E35N9W/1905.html},
  journal = {arXiv:1905.01392 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{wu_bayesian_2017,
  title = {Bayesian {{Optimization}} with {{Gradients}}},
  author = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew Gordon and Frazier, Peter I.},
  year = {2017},
  month = mar,
  abstract = {In recent years, Bayesian optimization has proven successful for global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, and comes in both sequential and batch forms. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, kernel learning, and k-nearest neighbors.},
  archivePrefix = {arXiv},
  eprint = {1703.04389},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/wu et al_2017_bayesian optimization with gradients.pdf;/home/juan/Zotero/storage/QKPBH2UT/1703.html},
  journal = {arXiv:1703.04389 [cs, math, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@phdthesis{wu_knowledge_2017,
  title = {Knowledge {{Gradient Methods}} for {{Bayesian Optimization}}},
  author = {Wu, Jian},
  year = {2017},
  school = {Cornell University},
  type = {{{PhD Thesis}}}
}

@article{wu_parallel_2016,
  title = {The {{Parallel Knowledge Gradient Method}} for {{Batch Bayesian Optimization}}},
  author = {Wu, Jian and Frazier, Peter I.},
  year = {2016},
  month = jun,
  abstract = {In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes-optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.},
  archivePrefix = {arXiv},
  eprint = {1606.04414},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/wu_frazier_2016_the parallel knowledge gradient method for batch bayesian optimization.pdf;/home/juan/Zotero/storage/U3VA396C/1606.html},
  journal = {arXiv:1606.04414 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{xiao_hyperparameter_2015,
  title = {Hyperparameter {{Selection}} for {{Gaussian Process One}}-{{Class Classification}}},
  author = {Xiao, Y. and Wang, H. and Xu, W.},
  year = {2015},
  month = sep,
  volume = {26},
  pages = {2182--2187},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2014.2363457},
  abstract = {Gaussian processes (GPs) provide predicted outputs with a full conditional statistical description, which can be used to establish confidence intervals and to set hyperparameters. This characteristic provides GPs with competitive or better performance in various applications. However, the specificity of one-class classification (OCC) makes GPs unable to select suitable hyperparameters in their traditional way. This brief proposes to select hyperparameters for GP OCC using the prediction difference between edge and interior positive training samples. Experiments on 2-D artificial and University of California benchmark data sets verify the effectiveness of this method.},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/xiao et al_2015_hyperparameter selection for gaussian process one-class classification.pdf;/home/juan/Zotero/storage/R7S2F9EE/6940303.html},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords = {Benchmark testing,Covariance function,Gaussian distribution,Gaussian process one-class classification,Gaussian processes,Gaussian processes (GPs),GP OCC,Ground penetrating radar,hyperparameter selection,Learning systems,Measurement,one-class classification (OCC),one-class classification (OCC).,parameter estimation,pattern classification,Training,Vectors},
  number = {9}
}

@inproceedings{yoshida_reinforcement_2013,
  title = {Reinforcement Learning with State-Dependent Discount Factor},
  author = {Yoshida, Naoto and Uchibe, Eiji and Doya, Kenji},
  year = {2013},
  month = aug,
  pages = {1--6},
  doi = {10.1109/DevLrn.2013.6652533},
  abstract = {Conventional reinforcement learning algorithms have several parameters which determine the feature of learning process, called meta-parameters. In this study, we focus on the discount factor that influences the time scale of the tradeoff between immediate and delayed rewards. The discount factor is usually considered as a constant value, but we introduce the state-dependent discount function and a new optimization criterion for the reinforcement learning algorithm. We first derive a new algorithm under the criterion, named ExQ-learning and we prove that the algorithm converges to the optimal action-value function in the meaning of new criterion w.p.1. We then present a framework to optimize the discount factor and the discount function by using an evolutionary algorithm. In order to validate the proposed method, we conduct a simple computer simulation and show that the proposed algorithm can find an appropriate state-dependent discount function with which performs better than that with a constant discount factor.}
}

@article{zela_towards_2018,
  title = {Towards {{Automated Deep Learning}}: {{Efficient Joint Neural Architecture}} and {{Hyperparameter Search}}},
  shorttitle = {Towards {{Automated Deep Learning}}},
  author = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
  year = {2018},
  month = jul,
  abstract = {While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.},
  archivePrefix = {arXiv},
  eprint = {1807.06906},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/zela et al_2018_towards automated deep learning.pdf;/home/juan/Zotero/storage/TFVBMZPP/1807.html},
  journal = {arXiv:1807.06906 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{zhang_dissection_2018,
  title = {A {{Dissection}} of {{Overfitting}} and {{Generalization}} in {{Continuous Reinforcement Learning}}},
  author = {Zhang, Amy and Ballas, Nicolas and Pineau, Joelle},
  year = {2018},
  month = jun,
  abstract = {The risks and perils of overfitting in machine learning are well known. However most of the treatment of this, including diagnostic tools and remedies, was developed for the supervised learning case. In this work, we aim to offer new perspectives on the characterization and prevention of overfitting in deep Reinforcement Learning (RL) methods, with a particular focus on continuous domains. We examine several aspects, such as how to define and diagnose overfitting in MDPs, and how to reduce risks by injecting sufficient training diversity. This work complements recent findings on the brittleness of deep RL methods and offers practical observations for RL researchers and practitioners.},
  archivePrefix = {arXiv},
  eprint = {1806.07937},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/zhang et al_2018_a dissection of overfitting and generalization in continuous reinforcement.pdf;/home/juan/Zotero/storage/WTCKXPRT/1806.html},
  journal = {arXiv:1806.07937 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{zhang_study_2018,
  title = {A {{Study}} on {{Overfitting}} in {{Deep Reinforcement Learning}}},
  author = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
  year = {2018},
  month = apr,
  abstract = {Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen ``robustly'': commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.},
  archivePrefix = {arXiv},
  eprint = {1804.06893},
  eprinttype = {arxiv},
  file = {/home/juan/Dropbox/Universidad/Bibliografía/Zotero Library/zhang et al_2018_a study on overfitting in deep reinforcement learning.pdf},
  journal = {arXiv:1804.06893 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zoph_neural_2016,
  title = {Neural {{Architecture Search}} with {{Reinforcement Learning}}},
  author = {Zoph, Barret and Le, Quoc V.},
  year = {2016},
  month = nov,
  abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
  archivePrefix = {arXiv},
  eprint = {1611.01578},
  eprinttype = {arxiv},
  file = {/home/jbarsce/Dropbox/Universidad/Bibliografía/Zotero Library/zoph_le_2016_neural architecture search with reinforcement learning.pdf;/home/juan/Zotero/storage/DAD6GBGN/1611.html},
  journal = {arXiv:1611.01578 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}


